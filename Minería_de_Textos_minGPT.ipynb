{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lTgJSTKuzsM"
      },
      "source": [
        "# minGPT\n",
        "\n",
        "**Note:** The `autoreload` extension allows the interpreter to reload modules every time a cell is executed. This is useful when editing the code in a module. The following cell enables the extension and downloads the minGPT package from Github. You can now double-click on a file like model.py, edit its contents, and press Ctrl+S to save it. If you then re-run the notebook cells, including those that create an object of the corresponding class, you will see the changes reflected. Note that the next cell should *only be executed once*, as running `pip install` again will overwrite the modified contents of the module.\n",
        "\n",
        "Recall that changes in the files (except the notebook itself) are not persistent unless you connect them to your Google Drive account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEghw4-QA6oY",
        "outputId": "cbb70c42-df3d-42dd-8df1-43e663f09c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining mingpt from git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26#egg=mingpt\n",
            "  Cloning https://github.com/karpathy/minGPT.git (to revision 37baab71b9abea1b76ab957409a1cc2fbfba8a26) to ./.venv/src/mingpt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/karpathy/minGPT.git /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/src/mingpt\n",
            "  Running command git rev-parse -q --verify 'sha^37baab71b9abea1b76ab957409a1cc2fbfba8a26'\n",
            "  Running command git fetch -q https://github.com/karpathy/minGPT.git 37baab71b9abea1b76ab957409a1cc2fbfba8a26\n",
            "  Resolved https://github.com/karpathy/minGPT.git to commit 37baab71b9abea1b76ab957409a1cc2fbfba8a26\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from mingpt) (2.9.1)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (4.15.0)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (2025.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch->mingpt) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch->mingpt) (3.0.3)\n",
            "Building wheels for collected packages: mingpt\n",
            "  Building editable for mingpt (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for mingpt: filename=mingpt-0.0.1-0.editable-py3-none-any.whl size=3597 sha256=a1f8c699c4ffb40066f884608efc6214f047b59949bf9470c470f6d6965d4c0c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b75hez0r/wheels/00/63/d9/a8cbf56faebea087e4c2e61b7041cb6af80e18d658b27c351e\n",
            "Successfully built mingpt\n",
            "Installing collected packages: mingpt\n",
            "  Attempting uninstall: mingpt\n",
            "    Found existing installation: minGPT 0.0.1\n",
            "    Uninstalling minGPT-0.0.1:\n",
            "      Successfully uninstalled minGPT-0.0.1\n",
            "Successfully installed mingpt-0.0.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%pip install -e 'git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26#egg=mingpt'\n",
        "\n",
        "# Fix this issue: https://github.com/karpathy/minGPT/issues/120\n",
        "#!sed -i '200s/.*/        assert len(keys) == len([k for k in sd if not k.endswith(\".attn.bias\")])/' /content/src/mingpt/mingpt/model.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkm4BPSu17LG"
      },
      "source": [
        "Add module's location to PYTHONPATH, which tells your Python interpreter where to search modules for. The previous `pip install -e` changes the variable in a subshell and the interpreter is therefore not aware of the updated value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26\n",
            "  Using cached mingpt-0.0.1-py3-none-any.whl\n",
            "Collecting torch (from mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting filelock (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached filelock-3.20.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting setuptools (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting sympy>=1.13.3 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx>=2.5.1 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting jinja2 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec>=0.8.5 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Using cached triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
            "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Using cached filelock-3.20.2-py3-none-any.whl (16 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, triton, sympy, setuptools, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, mingpt\n",
            "Successfully installed MarkupSafe-3.0.3 filelock-3.20.2 fsspec-2025.12.0 jinja2-3.1.6 mingpt-0.0.1 mpmath-1.3.0 networkx-3.6.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 setuptools-80.9.0 sympy-1.14.0 torch-2.9.1 triton-3.5.1 typing-extensions-4.15.0\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install --upgrade --force-reinstall \"mingpt @ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SWZ69BeU1v49"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/src/mingpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMaLovlTyIZs",
        "outputId": "a69bba5f-09f9-465d-dcbf-51b30e68a25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.20.2)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy>=1.17 (from transformers)\n",
            "  Using cached numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Using cached regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting requests (from transformers)\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
            "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
            "  Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers)\n",
            "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
            "  Using cached urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
            "  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "Using cached numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
            "Using cached regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
            "Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
            "Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
            "Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Using cached urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
            "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, numpy, idna, hf-xet, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed certifi-2026.1.4 charset_normalizer-3.4.4 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 numpy-2.4.0 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 tokenizers-0.22.2 tqdm-4.67.1 transformers-4.57.3 urllib3-2.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-otHTa0guzsT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from mingpt.model import GPT\n",
        "from mingpt.utils import set_seed\n",
        "from mingpt.bpe import BPETokenizer\n",
        "set_seed(3407)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yyjR2C7BuzsW"
      },
      "outputs": [],
      "source": [
        "use_mingpt = True # use minGPT or huggingface/transformers model?\n",
        "model_type = 'gpt2'\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311,
          "referenced_widgets": [
            "74d877d849d1498fbf87968a79b436d5",
            "d95517340925450d9544a147d0064f8f",
            "795a4f3580c5469f86d0bcb96edc1f13",
            "648a553be0bf49cba777f0eaaac741b2",
            "37af4bb362b94e87b7c128e6966534f9",
            "1209e810b3fc41808b444108a87afca1",
            "3ce2905b2f8e41adbc9eb724aeb2f62a",
            "5cbfa6b49da84ee781492abacea18884",
            "f9bcaec9f9c5467aa6e35316d3296302",
            "0c789b9007024556915b3f9e3cabe35e",
            "f6620c81dc4e4285a332cc40b7655caf",
            "784c12d3beb34f25a3c3387217b9a80a",
            "20ea57fda15f42d18d60048c165cb019",
            "8f25e6d89124473688b788bf69c1bfba",
            "bc9fe7054cc849e0ae9a0d69393de9d2",
            "b858fc0c1caa4872b91ec7638d6dbf68",
            "d0449079fc7a48a3a941c34c7cf2fdc9",
            "9a67d3d3a8744fd881b8a2d5359e78ab",
            "a08a849c10e24dffb645cfe75c4bb66a",
            "efb3d94f64314da6a0e9b3e4d1e123e4",
            "a9d6ce4b456f4b6b872a832ffe858e06",
            "52f6eb03404a425aabda1a87b86afe15",
            "80c335cb65a5408dae9ecd4ec9eda4ba",
            "3ba0334c76344e0a824177d3572e6dd7",
            "eff3788657d8436e96a328e73cab52cc",
            "260906d3854c4ff78c2cda042bae0a2e",
            "aa21a3379eb44eb28a52028cc5a0590b",
            "16ca502911134178a532cda1cf139ca4",
            "ca37e879895f4707bb2e39d9229c65dd",
            "44854839f29f4de08577daf5c4db4c63",
            "5eec0e26428c4034867633d70b4b3915",
            "048b2b328d174740a7f4eb64fd2f8b7f",
            "c033d8e19c084b90bd8a88a3e5bdb5b8"
          ]
        },
        "id": "7zgx5CsVuzsW",
        "outputId": "d82696bb-980b-427b-8dac-d85c19137a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 124.44M\n"
          ]
        }
      ],
      "source": [
        "if use_mingpt:\n",
        "    model = GPT.from_pretrained(model_type)\n",
        "else:\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "    model.config.pad_token_id = model.config.eos_token_id # suppress a warning\n",
        "\n",
        "# ship model to device and set to eval mode\n",
        "model.to(device)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vizgt_f4uzsY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
        "\n",
        "    # tokenize the input prompt into integer input sequence\n",
        "    if use_mingpt:\n",
        "        tokenizer = BPETokenizer()\n",
        "        if prompt == '':\n",
        "            # to create unconditional samples...\n",
        "            # manually create a tensor with only the special <|endoftext|> token\n",
        "            # similar to what openai's code does here https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\n",
        "            x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
        "        else:\n",
        "            x = tokenizer(prompt).to(device)\n",
        "    else:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
        "        if prompt == '':\n",
        "            # to create unconditional samples...\n",
        "            # huggingface/transformers tokenizer special cases these strings\n",
        "            prompt = '<|endoftext|>'\n",
        "        encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "        x = encoded_input['input_ids']\n",
        "\n",
        "    # we'll process all desired num_samples in a batch, so expand out the batch dim\n",
        "    x = x.expand(num_samples, -1)\n",
        "\n",
        "    # forward the model `steps` times to get samples, in a batch\n",
        "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
        "        print('-'*80)\n",
        "        print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxpoqSOWuzsZ",
        "outputId": "ee342ba7-57a5-48d5-96da-800bffc3b8ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on NASA's Juno mission, will also receive this year's award.\n",
            "\n",
            "While his experience shows that\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on Russia's delegation to the G20 summit, spoke about his work to end climate change in his blog\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the United Nations Security Council who was asked on Monday to take a position on climate change in order to\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the UN climate conference in Paris where he was one of the leading climate advocates, said the decision to\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the panel of the U.N. Climate Change Conference on Nov. 17 (UNCIT),\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on an expedition to the moon,\" Karpathy said in an interview with SPACE.com, referring to\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the committee, said, \"We are not convinced that the planet is as good as it should be\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the NASA Goddard Centre for Astrophysics project and a senior fellow at NASA's Goddard Institute for Space\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the Council of Ministers, has declared that there is no possibility of any major change in the trajectory of\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the Energetic System, agrees with his co-organizer and is hopeful the process will prove\n"
          ]
        }
      ],
      "source": [
        "generate(prompt='Andrej Karpathy, the Earth representative on', num_samples=10, steps=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting repo_orientation.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile repo_orientation.py\n",
        "\"\"\"\n",
        "What this script does:\n",
        "- Prints where mingpt is installed.\n",
        "- Locates mingpt/model.py.\n",
        "- Extracts/prints the key lines of GPT.forward that matter for the assignment:\n",
        "  embeddings -> transformer blocks -> ln_f -> lm_head -> logits\n",
        "- Provides programmatic checks used by unit tests.\n",
        "\n",
        "This does NOT implement activation caching/patching yet. \n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import inspect\n",
        "import pathlib\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ForwardLandmarks:\n",
        "    has_tok_emb: bool\n",
        "    has_pos_emb: bool\n",
        "    has_blocks_loop: bool\n",
        "    has_ln_f: bool\n",
        "    has_lm_head: bool\n",
        "\n",
        "\n",
        "def get_paths() -> Dict[str, str]:\n",
        "    pkg_path = pathlib.Path(mingpt.__file__).resolve()\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return {\n",
        "        \"mingpt.__file__\": str(pkg_path),\n",
        "        \"mingpt.model.__file__\": str(model_path),\n",
        "    }\n",
        "\n",
        "\n",
        "def read_model_source() -> str:\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return model_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "def attn_bias_fix_present(model_source: str) -> bool:\n",
        "    # Required fix: assert len(keys) == len([k for k in sd if not k.endswith(\".attn.bias\")])\n",
        "    return 'len([k for k in sd if not k.endswith(\".attn.bias\")])' in model_source\n",
        "\n",
        "\n",
        "def forward_source() -> str:\n",
        "    return inspect.getsource(GPT.forward)\n",
        "\n",
        "\n",
        "def find_forward_landmarks(src: str) -> ForwardLandmarks:\n",
        "    has_tok_emb = (\"tok_emb\" in src) and (\"wte\" in src)\n",
        "    has_pos_emb = (\"pos_emb\" in src) and (\"wpe\" in src)\n",
        "\n",
        "    has_blocks_loop = (\n",
        "        re.search(r\"\\bfor\\b\\s+.+\\s+\\bin\\b\\s+.*self\\.transformer\\.h\", src) is not None\n",
        "        or re.search(r\"\\bfor\\b\\s+.+\\s+\\bin\\b\\s+.*self\\.transformer\\['h'\\]\", src) is not None\n",
        "    )\n",
        "\n",
        "    has_ln_f = \"ln_f\" in src\n",
        "    has_lm_head = (\"lm_head\" in src) and (\"logits\" in src)\n",
        "\n",
        "    return ForwardLandmarks(\n",
        "        has_tok_emb=has_tok_emb,\n",
        "        has_pos_emb=has_pos_emb,\n",
        "        has_blocks_loop=has_blocks_loop,\n",
        "        has_ln_f=has_ln_f,\n",
        "        has_lm_head=has_lm_head,\n",
        "    )\n",
        "\n",
        "\n",
        "def print_forward_snippet(src: str, max_lines: int = 80) -> None:\n",
        "    lines = src.splitlines()\n",
        "    print(\"=== GPT.forward (snippet) ===\")\n",
        "    for i, line in enumerate(lines[:max_lines], start=1):\n",
        "        print(f\"{i:03d}: {line}\")\n",
        "    if len(lines) > max_lines:\n",
        "        print(f\"... ({len(lines)-max_lines} more lines)\")\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    paths = get_paths()\n",
        "    print(\"=== Installed paths ===\")\n",
        "    for k, v in paths.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    model_src = read_model_source()\n",
        "    print(\"\\n=== .attn.bias fix present? ===\")\n",
        "    print(attn_bias_fix_present(model_src))\n",
        "\n",
        "    fwd_src = forward_source()\n",
        "    landmarks = find_forward_landmarks(fwd_src)\n",
        "    print(\"\\n=== Forward pipeline landmarks ===\")\n",
        "    print(landmarks)\n",
        "\n",
        "    print()\n",
        "    print_forward_snippet(fwd_src)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Installed paths ===\n",
            "mingpt.__file__: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/src/mingpt/mingpt/__init__.py\n",
            "mingpt.model.__file__: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/src/mingpt/mingpt/model.py\n",
            "\n",
            "=== .attn.bias fix present? ===\n",
            "True\n",
            "\n",
            "=== Forward pipeline landmarks ===\n",
            "ForwardLandmarks(has_tok_emb=True, has_pos_emb=True, has_blocks_loop=True, has_ln_f=True, has_lm_head=True)\n",
            "\n",
            "=== GPT.forward (snippet) ===\n",
            "001:     def forward(self, idx, targets=None):\n",
            "002:         device = idx.device\n",
            "003:         b, t = idx.size()\n",
            "004:         assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
            "005:         pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
            "006: \n",
            "007:         # forward the GPT model itself\n",
            "008:         tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
            "009:         pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
            "010:         x = self.transformer.drop(tok_emb + pos_emb)\n",
            "011:         for block in self.transformer.h:\n",
            "012:             x = block(x)\n",
            "013:         x = self.transformer.ln_f(x)\n",
            "014:         logits = self.lm_head(x)\n",
            "015: \n",
            "016:         # if we are given some desired targets also calculate the loss\n",
            "017:         loss = None\n",
            "018:         if targets is not None:\n",
            "019:             loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
            "020: \n",
            "021:         return logits, loss\n"
          ]
        }
      ],
      "source": [
        "!python repo_orientation.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting generate_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile generate_driver.py\n",
        "\"\"\"\n",
        "Right now this only:\n",
        "- loads GPT-2 small via GPT.from_pretrained('gpt2')\n",
        "- tokenizes a prompt with BPETokenizer\n",
        "- runs a single forward pass to confirm logits shape\n",
        "- runs model.generate to confirm decoding loop works\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "\n",
        "def get_device() -> str:\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    set_seed(3407)\n",
        "\n",
        "    device = get_device()\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    model = GPT.from_pretrained(\"gpt2\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    bpe = BPETokenizer()\n",
        "    prompt = \"Andrej Karpathy, the Earth representative on\"\n",
        "    idx = bpe(prompt).to(device)  # shape (1, T)\n",
        "\n",
        "    # forward pass (logits for each position)\n",
        "    logits, loss = model(idx)\n",
        "    print(\"Input shape:\", tuple(idx.shape))\n",
        "    print(\"Logits shape:\", tuple(logits.shape))\n",
        "    assert logits.ndim == 3, \"Expected (B, T, V) logits\"\n",
        "    assert logits.shape[0] == idx.shape[0] and logits.shape[1] == idx.shape[1], \"B,T must match input\"\n",
        "\n",
        "    # generate a short continuation (just to prove decoding loop works)\n",
        "    out_idx = model.generate(idx, max_new_tokens=20, do_sample=True, top_k=40)\n",
        "    out_text = bpe.decode(out_idx[0].cpu())\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(out_text)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "number of parameters: 124.44M\n",
            "Input shape: (1, 10)\n",
            "Logits shape: (1, 10, 50257)\n",
            "\n",
            "=== Generated ===\n",
            "Andrej Karpathy, the Earth representative on NASA's Mars Exploration Rover Curiosity, talks about the success of the science rover Curiosity, which now has\n"
          ]
        }
      ],
      "source": [
        "!python generate_driver.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_section_2.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section_2.py\n",
        "import os\n",
        "import pathlib\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "import repo_orientation as ro\n",
        "\n",
        "\n",
        "def test_mingpt_importable_and_paths_exist():\n",
        "    paths = ro.get_paths()\n",
        "    assert \"mingpt.__file__\" in paths and \"mingpt.model.__file__\" in paths\n",
        "\n",
        "    pkg_path = pathlib.Path(paths[\"mingpt.__file__\"])\n",
        "    model_path = pathlib.Path(paths[\"mingpt.model.__file__\"])\n",
        "    assert pkg_path.exists(), f\"mingpt package file not found: {pkg_path}\"\n",
        "    assert model_path.exists(), f\"mingpt.model file not found: {model_path}\"\n",
        "\n",
        "\n",
        "def test_attn_bias_fix_present_or_applied():\n",
        "    src = ro.read_model_source()\n",
        "    assert ro.attn_bias_fix_present(src), (\n",
        "        \"Required fix not found in mingpt/model.py. \"\n",
        "        \"Expected assert to ignore keys ending with .attn.bias.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def test_forward_pipeline_landmarks_present():\n",
        "    fwd_src = ro.forward_source()\n",
        "    lm = ro.find_forward_landmarks(fwd_src)\n",
        "    assert lm.has_tok_emb, \"Expected token embedding (wte/tok_emb) usage in forward.\"\n",
        "    assert lm.has_pos_emb, \"Expected positional embedding (wpe/pos_emb) usage in forward.\"\n",
        "    assert lm.has_blocks_loop, \"Expected loop over transformer blocks in forward.\"\n",
        "    assert lm.has_ln_f, \"Expected final layer norm ln_f in forward.\"\n",
        "    assert lm.has_lm_head, \"Expected lm_head/logits in forward.\"\n",
        "\n",
        "\n",
        "def test_fast_forward_and_generate_from_scratch():\n",
        "    # Fast test: avoid downloading HF weights.\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"  # tiny\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "    assert logits.shape == (1, 10, cfg.vocab_size)\n",
        "    assert loss is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=5, do_sample=False)\n",
        "    assert out.shape[1] == 15\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_slow_from_pretrained_gpt2_loads_and_runs():\n",
        "    # Slow test: tries to download and load GPT-2 weights.\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    try:\n",
        "        model = GPT.from_pretrained(\"gpt2\")\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping from_pretrained test due to load/download error: {e}\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, 50257, (1, 8), dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "\n",
        "    assert logits.shape == (1, 8, 50257)\n",
        "    assert loss is None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting pytest.ini\n"
          ]
        }
      ],
      "source": [
        "%%writefile pytest.ini\n",
        "[pytest]\n",
        "markers =\n",
        "    slow: marks tests as slow (deselect with '-m \"not slow\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tokenization_protocol.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tokenization_protocol.py\n",
        "\"\"\"\n",
        "Tokenization Protocol and \"Same Number of Tokens\" Guarantee.\n",
        "\n",
        "This module provides:\n",
        "- Tokenization reports (token ids, per-token decoded strings, token count)\n",
        "- Pair comparison (same-length check, diff positions, one-token-diff check)\n",
        "- Report-friendly Markdown export for token-by-token decomposition\n",
        "- Heuristic suggestions to fix token length mismatches\n",
        "\n",
        "Designed for minGPT's BPETokenizer (mingpt/bpe.py).\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Sequence, Tuple, Dict\n",
        "\n",
        "import torch\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "\n",
        "# Data structures\n",
        "@dataclass(frozen=True)\n",
        "class TokenizationReport:\n",
        "    text: str\n",
        "    token_ids: List[int]\n",
        "    token_strs: List[str]  # decoded per-token strings (may include leading spaces)\n",
        "    seq_len: int\n",
        "    decoded_roundtrip: str\n",
        "\n",
        "    def short_preview(self, max_chars: int = 120) -> str:\n",
        "        s = self.text.replace(\"\\n\", \"\\\\n\")\n",
        "        return s if len(s) <= max_chars else s[: max_chars - 3] + \"...\"\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class PairComparison:\n",
        "    clean: TokenizationReport\n",
        "    corrupt: TokenizationReport\n",
        "    same_length: bool\n",
        "    diff_positions: List[int]\n",
        "    diff_count: int\n",
        "\n",
        "    @property\n",
        "    def one_token_diff(self) -> bool:\n",
        "        return self.same_length and self.diff_count == 1\n",
        "\n",
        "\n",
        "# Core tokenization helpers\n",
        "def tokenize_2d(bpe: BPETokenizer, text: str, device: Optional[str] = None) -> torch.LongTensor:\n",
        "    \"\"\"\n",
        "    Returns token ids as a 2D tensor of shape (1, T) as BPETokenizer does.\n",
        "    \"\"\"\n",
        "    ids_2d = bpe(text)  # (1, T)\n",
        "    if device is not None:\n",
        "        ids_2d = ids_2d.to(device)\n",
        "    return ids_2d\n",
        "\n",
        "\n",
        "def tokenize_1d_ids(bpe: BPETokenizer, text: str) -> List[int]:\n",
        "    \"\"\"\n",
        "    Returns token ids as a python list[int] (1D).\n",
        "    \"\"\"\n",
        "    ids = bpe(text)[0].tolist()\n",
        "    return [int(x) for x in ids]\n",
        "\n",
        "\n",
        "def decode_token_id(bpe: BPETokenizer, token_id: int) -> str:\n",
        "    \"\"\"\n",
        "    Decode a single token id into its string form.\n",
        "    \"\"\"\n",
        "    t = torch.tensor([token_id], dtype=torch.long)\n",
        "    return bpe.decode(t)\n",
        "\n",
        "\n",
        "def decode_tokens_1d(bpe: BPETokenizer, token_ids: Sequence[int]) -> str:\n",
        "    \"\"\"\n",
        "    Decode a sequence of token ids back into a string.\n",
        "    \"\"\"\n",
        "    t = torch.tensor(list(token_ids), dtype=torch.long)\n",
        "    return bpe.decode(t)\n",
        "\n",
        "\n",
        "def per_token_strings(bpe: BPETokenizer, token_ids: Sequence[int]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Per-token decoded strings (important for inspecting leading spaces).\n",
        "    \"\"\"\n",
        "    return [decode_token_id(bpe, int(tid)) for tid in token_ids]\n",
        "\n",
        "\n",
        "def build_report(bpe: BPETokenizer, text: str) -> TokenizationReport:\n",
        "    \"\"\"\n",
        "    Build a complete tokenization report for one text.\n",
        "    \"\"\"\n",
        "    token_ids = tokenize_1d_ids(bpe, text)\n",
        "    token_strs = per_token_strings(bpe, token_ids)\n",
        "    decoded = decode_tokens_1d(bpe, token_ids)\n",
        "    return TokenizationReport(\n",
        "        text=text,\n",
        "        token_ids=token_ids,\n",
        "        token_strs=token_strs,\n",
        "        seq_len=len(token_ids),\n",
        "        decoded_roundtrip=decoded,\n",
        "    )\n",
        "\n",
        "\n",
        "# Comparison and validations\n",
        "def diff_positions(a: Sequence[int], b: Sequence[int]) -> List[int]:\n",
        "    \"\"\"\n",
        "    Returns a list of positions where sequences differ.\n",
        "    If lengths differ, extra positions beyond min length are included as diffs.\n",
        "    \"\"\"\n",
        "    la, lb = len(a), len(b)\n",
        "    m = min(la, lb)\n",
        "    diffs = [i for i in range(m) if int(a[i]) != int(b[i])]\n",
        "    if la != lb:\n",
        "        diffs.extend(list(range(m, max(la, lb))))\n",
        "    return diffs\n",
        "\n",
        "\n",
        "def compare_clean_corrupt(clean: TokenizationReport, corrupt: TokenizationReport) -> PairComparison:\n",
        "    diffs = diff_positions(clean.token_ids, corrupt.token_ids)\n",
        "    same_len = (clean.seq_len == corrupt.seq_len)\n",
        "    return PairComparison(\n",
        "        clean=clean,\n",
        "        corrupt=corrupt,\n",
        "        same_length=same_len,\n",
        "        diff_positions=diffs,\n",
        "        diff_count=len(diffs),\n",
        "    )\n",
        "\n",
        "\n",
        "def assert_same_length(clean: TokenizationReport, corrupt: TokenizationReport) -> None:\n",
        "    if clean.seq_len != corrupt.seq_len:\n",
        "        raise ValueError(\n",
        "            f\"Token length mismatch: clean={clean.seq_len}, corrupt={corrupt.seq_len}.\\n\"\n",
        "            f\"Clean preview: {clean.short_preview()}\\n\"\n",
        "            f\"Corrupt preview: {corrupt.short_preview()}\"\n",
        "        )\n",
        "\n",
        "\n",
        "def assert_one_token_difference(comp: PairComparison) -> None:\n",
        "    if not comp.same_length:\n",
        "        raise ValueError(\n",
        "            f\"Cannot check one-token-diff: lengths differ (clean={comp.clean.seq_len}, corrupt={comp.corrupt.seq_len}).\"\n",
        "        )\n",
        "    if comp.diff_count != 1:\n",
        "        raise ValueError(\n",
        "            f\"Expected exactly 1 differing token position, found {comp.diff_count}: {comp.diff_positions}\\n\"\n",
        "            f\"Tip: inspect the per-token strings and adjust the text until only one BPE token changes.\"\n",
        "        )\n",
        "\n",
        "\n",
        "def validate_pair(\n",
        "    bpe: BPETokenizer,\n",
        "    clean_text: str,\n",
        "    corrupt_text: str,\n",
        "    require_same_length: bool = True,\n",
        "    require_one_token_diff: bool = True,\n",
        ") -> PairComparison:\n",
        "    \"\"\"\n",
        "    Tokenize both texts, compare, and (optionally) enforce constraints by raising errors.\n",
        "    \"\"\"\n",
        "    clean = build_report(bpe, clean_text)\n",
        "    corrupt = build_report(bpe, corrupt_text)\n",
        "    comp = compare_clean_corrupt(clean, corrupt)\n",
        "\n",
        "    if require_same_length:\n",
        "        assert_same_length(clean, corrupt)\n",
        "    if require_one_token_diff:\n",
        "        assert_one_token_difference(comp)\n",
        "    return comp\n",
        "\n",
        "\n",
        "# Printing / report exports\n",
        "def format_token_list_for_console(rep: TokenizationReport) -> str:\n",
        "    \"\"\"\n",
        "    Console-friendly token list.\n",
        "    Shows position, token_id, and repr(token_str) to make spaces visible.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for i, (tid, s) in enumerate(zip(rep.token_ids, rep.token_strs)):\n",
        "        lines.append(f\"{i:02d} | {tid:5d} | {repr(s)}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def format_pair_diff_markdown(comp: PairComparison) -> str:\n",
        "    \"\"\"\n",
        "    Markdown table: position-wise clean vs corrupt tokens.\n",
        "    Great for pasting into the report.\n",
        "    \"\"\"\n",
        "    clean = comp.clean\n",
        "    corrupt = comp.corrupt\n",
        "    max_len = max(clean.seq_len, corrupt.seq_len)\n",
        "\n",
        "    header = \"| pos | clean_id | clean_tok | corrupt_id | corrupt_tok | diff? |\\n|---:|---:|---|---:|---|:---:|\\n\"\n",
        "    rows = []\n",
        "    for i in range(max_len):\n",
        "        c_id = clean.token_ids[i] if i < clean.seq_len else None\n",
        "        k_id = corrupt.token_ids[i] if i < corrupt.seq_len else None\n",
        "        c_tok = clean.token_strs[i] if i < clean.seq_len else \"\"\n",
        "        k_tok = corrupt.token_strs[i] if i < corrupt.seq_len else \"\"\n",
        "        diff = \"\" if i in comp.diff_positions else \"\"\n",
        "        rows.append(\n",
        "            f\"| {i} | {'' if c_id is None else c_id} | {repr(c_tok)} | {'' if k_id is None else k_id} | {repr(k_tok)} | {diff} |\"\n",
        "        )\n",
        "    return header + \"\\n\".join(rows) + \"\\n\"\n",
        "\n",
        "\n",
        "def describe_pair(comp: PairComparison) -> str:\n",
        "    \"\"\"\n",
        "    Human-readable summary.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        \"=== Pair summary ===\\n\"\n",
        "        f\"Clean tokens:   {comp.clean.seq_len}\\n\"\n",
        "        f\"Corrupt tokens: {comp.corrupt.seq_len}\\n\"\n",
        "        f\"Same length?    {comp.same_length}\\n\"\n",
        "        f\"Diff count:     {comp.diff_count}\\n\"\n",
        "        f\"Diff positions: {comp.diff_positions}\\n\"\n",
        "        f\"One-token diff? {comp.one_token_diff}\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Heuristic suggestions (for mismatch debugging)\n",
        "def suggest_fixes(clean: TokenizationReport, corrupt: TokenizationReport) -> List[str]:\n",
        "    \"\"\"\n",
        "    Heuristics to help the user fix length mismatches / multi-token mismatches.\n",
        "    Not an automatic fixer; it gives actionable suggestions.\n",
        "    \"\"\"\n",
        "    suggestions: List[str] = []\n",
        "\n",
        "    # Length mismatch guidance\n",
        "    if clean.seq_len != corrupt.seq_len:\n",
        "        suggestions.append(\n",
        "            \"Token length mismatch detected. Common causes: whitespace differences, punctuation attachment, \"\n",
        "            \"or swapping a word that tokenizes into a different number of BPE tokens.\"\n",
        "        )\n",
        "        suggestions.append(\n",
        "            \"Try keeping punctuation identical (e.g., 'student.' vs 'student .') and keep spaces consistent around the changed word.\"\n",
        "        )\n",
        "        suggestions.append(\n",
        "            \"Proper nouns are often unstable: try swapping to a more common single-token alternative and re-check.\"\n",
        "        )\n",
        "\n",
        "    # Multi-token difference guidance\n",
        "    diffs = diff_positions(clean.token_ids, corrupt.token_ids)\n",
        "    if clean.seq_len == corrupt.seq_len and len(diffs) != 1:\n",
        "        suggestions.append(\n",
        "            f\"More than one token differs ({len(diffs)}). You want exactly 1 differing BPE token position.\"\n",
        "        )\n",
        "        suggestions.append(\n",
        "            \"Inspect per-token strings around the diff positions; often a punctuation or whitespace token is also changing.\"\n",
        "        )\n",
        "\n",
        "    # Space-specific hint\n",
        "    suggestions.append(\n",
        "        \"Remember GPT-2 BPE: tokens in the middle often include a leading space. \"\n",
        "        \"If you care about the token 'Jones', the actual token is usually ' Jones'.\"\n",
        "    )\n",
        "\n",
        "    return suggestions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tokenization_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tokenization_driver.py\n",
        "\"\"\"\n",
        "Tokenize clean/corrupt prompts, enforce same-length and one-token-diff,\n",
        "print per-token decomposition, and export a Markdown token table for the report.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "import tokenization_protocol as tp\n",
        "\n",
        "CLEAN_TEXT = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "CORRUPT_TEXT = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--clean\", type=str, default=CLEAN_TEXT, help=\"Clean prompt text\")\n",
        "    p.add_argument(\"--corrupt\", type=str, default=CORRUPT_TEXT, help=\"Corrupted prompt text\")\n",
        "    p.add_argument(\"--no-require-one-diff\", action=\"store_true\", help=\"Do not require exactly 1 token difference\")\n",
        "    p.add_argument(\"--out_md\", type=str, default=\"token_table.md\", help=\"Output markdown file for token table\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    clean_rep = tp.build_report(bpe, args.clean)\n",
        "    corrupt_rep = tp.build_report(bpe, args.corrupt)\n",
        "    comp = tp.compare_clean_corrupt(clean_rep, corrupt_rep)\n",
        "\n",
        "    print(tp.describe_pair(comp))\n",
        "\n",
        "    print(\"=== Clean prompt ===\")\n",
        "    print(clean_rep.text)\n",
        "    print(\"\\n=== Clean tokens (pos | id | repr(token)) ===\")\n",
        "    print(tp.format_token_list_for_console(clean_rep))\n",
        "\n",
        "    print(\"\\n=== Corrupt prompt ===\")\n",
        "    print(corrupt_rep.text)\n",
        "    print(\"\\n=== Corrupt tokens (pos | id | repr(token)) ===\")\n",
        "    print(tp.format_token_list_for_console(corrupt_rep))\n",
        "\n",
        "    # Enforce constraints as requested by the assignment\n",
        "    require_one = not args.no_require_one_diff\n",
        "    try:\n",
        "        _ = tp.validate_pair(\n",
        "            bpe=bpe,\n",
        "            clean_text=args.clean,\n",
        "            corrupt_text=args.corrupt,\n",
        "            require_same_length=True,\n",
        "            require_one_token_diff=require_one,\n",
        "        )\n",
        "        print(\"\\n Validation passed.\")\n",
        "    except Exception as e:\n",
        "        print(\"\\n Validation failed:\")\n",
        "        print(e)\n",
        "        print(\"\\nSuggestions:\")\n",
        "        for s in tp.suggest_fixes(clean_rep, corrupt_rep):\n",
        "            print(\"-\", s)\n",
        "\n",
        "    # Export markdown table for report\n",
        "    md = tp.format_pair_diff_markdown(comp)\n",
        "    out_path = Path(args.out_md)\n",
        "    out_path.write_text(md, encoding=\"utf-8\")\n",
        "    print(f\"\\nWrote Markdown token table to: {out_path.resolve()}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Pair summary ===\n",
            "Clean tokens:   11\n",
            "Corrupt tokens: 11\n",
            "Same length?    True\n",
            "Diff count:     1\n",
            "Diff positions: [1]\n",
            "One-token diff? True\n",
            "\n",
            "=== Clean prompt ===\n",
            "Michelle Jones was a top-notch student. Michelle\n",
            "\n",
            "=== Clean tokens (pos | id | repr(token)) ===\n",
            "00 | 48736 | 'Michelle'\n",
            "01 |  5437 | ' Jones'\n",
            "02 |   373 | ' was'\n",
            "03 |   257 | ' a'\n",
            "04 |  1353 | ' top'\n",
            "05 |    12 | '-'\n",
            "06 |  1662 | 'not'\n",
            "07 |   354 | 'ch'\n",
            "08 |  3710 | ' student'\n",
            "09 |    13 | '.'\n",
            "10 | 16738 | ' Michelle'\n",
            "\n",
            "=== Corrupt prompt ===\n",
            "Michelle Smith was a top-notch student. Michelle\n",
            "\n",
            "=== Corrupt tokens (pos | id | repr(token)) ===\n",
            "00 | 48736 | 'Michelle'\n",
            "01 |  4176 | ' Smith'\n",
            "02 |   373 | ' was'\n",
            "03 |   257 | ' a'\n",
            "04 |  1353 | ' top'\n",
            "05 |    12 | '-'\n",
            "06 |  1662 | 'not'\n",
            "07 |   354 | 'ch'\n",
            "08 |  3710 | ' student'\n",
            "09 |    13 | '.'\n",
            "10 | 16738 | ' Michelle'\n",
            "\n",
            " Validation passed.\n",
            "\n",
            "Wrote Markdown token table to: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/token_table.md\n"
          ]
        }
      ],
      "source": [
        "!python tokenization_driver.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| pos | clean_id | clean_tok | corrupt_id | corrupt_tok | diff? |\n",
            "|---:|---:|---|---:|---|:---:|\n",
            "| 0 | 48736 | 'Michelle' | 48736 | 'Michelle' |  |\n",
            "| 1 | 5437 | ' Jones' | 4176 | ' Smith' |  |\n",
            "| 2 | 373 | ' was' | 373 | ' was' |  |\n",
            "| 3 | 257 | ' a' | 257 | ' a' |  |\n",
            "| 4 | 1353 | ' top' | 1353 | ' top' |  |\n",
            "| 5 | 12 | '-' | 12 | '-' |  |\n",
            "| 6 | 1662 | 'not' | 1662 | 'not' |  |\n",
            "| 7 | 354 | 'ch' | 354 | 'ch' |  |\n",
            "| 8 | 3710 | ' student' | 3710 | ' student' |  |\n",
            "| 9 | 13 | '.' | 13 | '.' |  |\n",
            "| 10 | 16738 | ' Michelle' | 16738 | ' Michelle' |  |\n"
          ]
        }
      ],
      "source": [
        "!sed -n '1,120p' token_table.md\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_section_3.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section_3.py\n",
        "import pathlib\n",
        "import sys\n",
        "\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "COLAB_MINGPT_PATH = pathlib.Path(\"/content/src/mingpt\")\n",
        "if COLAB_MINGPT_PATH.exists():\n",
        "    sys.path.append(str(COLAB_MINGPT_PATH))\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "import repo_orientation as ro\n",
        "import tokenization_protocol as tp\n",
        "\n",
        "def test_mingpt_importable_and_paths_exist():\n",
        "    paths = ro.get_paths()\n",
        "    assert \"mingpt.__file__\" in paths and \"mingpt.model.__file__\" in paths\n",
        "\n",
        "    pkg_path = pathlib.Path(paths[\"mingpt.__file__\"])\n",
        "    model_path = pathlib.Path(paths[\"mingpt.model.__file__\"])\n",
        "    assert pkg_path.exists(), f\"mingpt package file not found: {pkg_path}\"\n",
        "    assert model_path.exists(), f\"mingpt.model file not found: {model_path}\"\n",
        "\n",
        "\n",
        "def test_attn_bias_fix_present_or_applied():\n",
        "    src = ro.read_model_source()\n",
        "    assert ro.attn_bias_fix_present(src), (\n",
        "        \"Required fix not found in mingpt/model.py. \"\n",
        "        \"Expected assert to ignore keys ending with .attn.bias.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def test_forward_pipeline_landmarks_present():\n",
        "    fwd_src = ro.forward_source()\n",
        "    lm = ro.find_forward_landmarks(fwd_src)\n",
        "    assert lm.has_tok_emb, \"Expected token embedding (wte/tok_emb) usage in forward.\"\n",
        "    assert lm.has_pos_emb, \"Expected positional embedding (wpe/pos_emb) usage in forward.\"\n",
        "    assert lm.has_blocks_loop, \"Expected loop over transformer blocks in forward.\"\n",
        "    assert lm.has_ln_f, \"Expected final layer norm ln_f in forward.\"\n",
        "    assert lm.has_lm_head, \"Expected lm_head/logits in forward.\"\n",
        "\n",
        "\n",
        "def test_fast_forward_and_generate_from_scratch():\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"  # tiny\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "    assert logits.shape == (1, 10, cfg.vocab_size)\n",
        "    assert loss is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=5, do_sample=False)\n",
        "    assert out.shape[1] == 15\n",
        "\n",
        "\n",
        "def test_diff_positions_length_mismatch_includes_tail():\n",
        "    a = [1, 2, 3]\n",
        "    b = [1, 2, 3, 4, 5]\n",
        "    diffs = tp.diff_positions(a, b)\n",
        "    assert diffs == [3, 4]\n",
        "\n",
        "\n",
        "def test_compare_reports_detects_one_token_diff_synthetic():\n",
        "    clean = tp.TokenizationReport(\n",
        "        text=\"clean\",\n",
        "        token_ids=[10, 20, 30],\n",
        "        token_strs=[\"a\", \"b\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"abc\",\n",
        "    )\n",
        "    corrupt = tp.TokenizationReport(\n",
        "        text=\"corrupt\",\n",
        "        token_ids=[10, 99, 30],\n",
        "        token_strs=[\"a\", \"X\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"aXc\",\n",
        "    )\n",
        "    comp = tp.compare_clean_corrupt(clean, corrupt)\n",
        "    assert comp.same_length is True\n",
        "    assert comp.diff_positions == [1]\n",
        "    assert comp.diff_count == 1\n",
        "    assert comp.one_token_diff is True\n",
        "\n",
        "\n",
        "def test_assert_one_token_difference_raises_when_multi_diff():\n",
        "    clean = tp.TokenizationReport(\n",
        "        text=\"clean\",\n",
        "        token_ids=[1, 2, 3],\n",
        "        token_strs=[\"a\", \"b\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"abc\",\n",
        "    )\n",
        "    corrupt = tp.TokenizationReport(\n",
        "        text=\"corrupt\",\n",
        "        token_ids=[9, 2, 8],\n",
        "        token_strs=[\"X\", \"b\", \"Y\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"XbY\",\n",
        "    )\n",
        "    comp = tp.compare_clean_corrupt(clean, corrupt)\n",
        "    assert comp.diff_count == 2\n",
        "    with pytest.raises(ValueError):\n",
        "        tp.assert_one_token_difference(comp)\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_bpe_tokenization_roundtrip_and_lengths():\n",
        "    \"\"\"\n",
        "    Slow-ish test because BPETokenizer may download merges/vocab on first use in a fresh runtime.\n",
        "    \"\"\"\n",
        "    from mingpt.bpe import BPETokenizer\n",
        "\n",
        "    try:\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping BPETokenizer test due to tokenizer init/download error: {e}\")\n",
        "\n",
        "    text = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    rep = tp.build_report(bpe, text)\n",
        "\n",
        "    # Basic sanity\n",
        "    assert rep.seq_len > 0\n",
        "    assert len(rep.token_ids) == rep.seq_len\n",
        "    assert len(rep.token_strs) == rep.seq_len\n",
        "\n",
        "    # Roundtrip should contain the key content (exact equality may vary by whitespace normalization)\n",
        "    assert \"Michelle\" in rep.decoded_roundtrip\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_bpe_pair_validation_example_michelle_jones_smith():\n",
        "    \"\"\"\n",
        "    Uses the assignment's canonical-style example to ensure:\n",
        "    - same token length\n",
        "    - ideally a one-token difference (it usually is, but tokenizer quirks can vary)\n",
        "    \"\"\"\n",
        "    from mingpt.bpe import BPETokenizer\n",
        "\n",
        "    try:\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping BPETokenizer test due to tokenizer init/download error: {e}\")\n",
        "\n",
        "    clean = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    corrupt = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "\n",
        "    clean_rep = tp.build_report(bpe, clean)\n",
        "    corrupt_rep = tp.build_report(bpe, corrupt)\n",
        "    comp = tp.compare_clean_corrupt(clean_rep, corrupt_rep)\n",
        "\n",
        "    assert comp.same_length is True, f\"Expected same token length; got {clean_rep.seq_len} vs {corrupt_rep.seq_len}\"\n",
        "\n",
        "    assert comp.diff_count >= 1\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_slow_from_pretrained_gpt2_loads_and_runs():\n",
        "    \"\"\"\n",
        "    Slow test: downloads and loads GPT-2 weights.\n",
        "    If network/cache issues happen in Colab, we skip rather than fail hard.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    try:\n",
        "        model = GPT.from_pretrained(\"gpt2\")\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping from_pretrained test due to load/download error: {e}\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, 50257, (1, 8), dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "\n",
        "    assert logits.shape == (1, 8, 50257)\n",
        "    assert loss is None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                               [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m10 passed\u001b[0m\u001b[32m in 12.91s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!{sys.executable} -m pytest -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiment_design.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiment_design.py\n",
        "\"\"\"\n",
        "Experimental Design (clean/corrupted prompts + target tokens + hypothesis)\n",
        "\n",
        "This module provides:\n",
        "- A structured ExperimentSpec (clean/corrupt prompts, target tokens A/B, hypothesis)\n",
        "- Validation utilities:\n",
        "  - clean/corrupt differ by EXACTLY one BPE token\n",
        "  - same number of tokens\n",
        "  - target tokens A/B are SINGLE BPE tokens (usually with leading space)\n",
        "- Convenience: candidate specs + \"pick the first valid one\" to avoid tokenization surprises.\n",
        "\n",
        "It depends on tokenization_protocol.py (Section 3), which you already implemented.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "import tokenization_protocol as tp\n",
        "\n",
        "\n",
        "# Data structures\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ExperimentSpec:\n",
        "    \"\"\"\n",
        "    Contract\" for the experiment.\n",
        "\n",
        "    clean_text and corrupt_text:\n",
        "      - must have same BPE token length\n",
        "      - must differ by exactly one BPE token\n",
        "\n",
        "    token_a_str and token_b_str:\n",
        "      - intended to be single BPE tokens for the next-token prediction\n",
        "      - recommended to include leading space (e.g., \" Paris\", \" def\")\n",
        "    \"\"\"\n",
        "    clean_text: str\n",
        "    corrupt_text: str\n",
        "    token_a_str: str\n",
        "    token_b_str: str\n",
        "    hypothesis: str\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ValidatedExperiment:\n",
        "    spec: ExperimentSpec\n",
        "    comparison: tp.PairComparison\n",
        "    changed_position: int\n",
        "    token_a_id: int\n",
        "    token_b_id: int\n",
        "\n",
        "\n",
        "# Token helpers (Token A / Token B)\n",
        "def ensure_leading_space(token_str: str) -> str:\n",
        "    \"\"\"\n",
        "    GPT-2 BPE typically encodes mid-sequence words with a leading space.\n",
        "    This helper makes it harder to forget that, but does NOT guarantee single-token.\n",
        "    \"\"\"\n",
        "    if token_str.startswith(\" \"):\n",
        "        return token_str\n",
        "    return \" \" + token_str\n",
        "\n",
        "\n",
        "def single_token_id(bpe: BPETokenizer, token_str: str) -> int:\n",
        "    \"\"\"\n",
        "    Convert a string (e.g., \" def\") into a SINGLE BPE token id.\n",
        "    Raises ValueError if the string tokenizes into multiple tokens.\n",
        "    \"\"\"\n",
        "    ids_2d = bpe(token_str)  # (1, T)\n",
        "    ids = ids_2d[0].tolist()\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(\n",
        "            f\"Target token string must be a single BPE token, but got {len(ids)} tokens for {repr(token_str)}: {ids}\"\n",
        "        )\n",
        "    return int(ids[0])\n",
        "\n",
        "\n",
        "# Core validation utilities\n",
        "def changed_token_position(comp: tp.PairComparison) -> int:\n",
        "    \"\"\"\n",
        "    Returns the unique position where clean vs corrupt differ.\n",
        "    Raises if not exactly one differing token.\n",
        "    \"\"\"\n",
        "    tp.assert_one_token_difference(comp)\n",
        "    return int(comp.diff_positions[0])\n",
        "\n",
        "\n",
        "def default_hypothesis(changed_pos: int) -> str:\n",
        "    \"\"\"\n",
        "    A report-ready hypothesis for GPT-2 small activation patching heatmaps.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        f\"Hypothesis: The changed token position (position {changed_pos}) should matter most, \"\n",
        "        \"so patching activations at this position across early-to-mid layers should strongly restore the clean-consistent continuation. \"\n",
        "        \"Middle layers are expected to dominate because they often integrate and route the key conditioning fact/entity forward through the residual stream. \"\n",
        "        \"Late layers may also show a secondary effect near the final token position because they directly refine the next-token logits.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def validate_experiment(bpe: BPETokenizer, spec: ExperimentSpec) -> ValidatedExperiment:\n",
        "    \"\"\"\n",
        "    Full validation for this section:\n",
        "    - clean/corrupt must be same length AND differ by exactly one BPE token\n",
        "    - token A and token B must each be single BPE tokens (usually with leading space)\n",
        "    \"\"\"\n",
        "    comp = tp.validate_pair(\n",
        "        bpe=bpe,\n",
        "        clean_text=spec.clean_text,\n",
        "        corrupt_text=spec.corrupt_text,\n",
        "        require_same_length=True,\n",
        "        require_one_token_diff=True,\n",
        "    )\n",
        "    pos = changed_token_position(comp)\n",
        "\n",
        "    token_a_id = single_token_id(bpe, spec.token_a_str)\n",
        "    token_b_id = single_token_id(bpe, spec.token_b_str)\n",
        "\n",
        "    return ValidatedExperiment(\n",
        "        spec=spec,\n",
        "        comparison=comp,\n",
        "        changed_position=pos,\n",
        "        token_a_id=token_a_id,\n",
        "        token_b_id=token_b_id,\n",
        "    )\n",
        "\n",
        "\n",
        "# Candidate specs + auto-pick\n",
        "def candidate_experiments() -> List[ExperimentSpec]:\n",
        "    \"\"\"\n",
        "    A small pool of \"creative but simple\" candidates.\n",
        "    We DO NOT assume these always pass one-token-diff constraints;\n",
        "    that's why pick_first_valid_experiment() exists.\n",
        "    \"\"\"\n",
        "    # Note: token strings here include leading spaces on purpose.\n",
        "    return [\n",
        "        ExperimentSpec(\n",
        "            clean_text=\"In Python, the keyword to define a function is\",\n",
        "            corrupt_text=\"In JavaScript, the keyword to define a function is\",\n",
        "            token_a_str=\" def\",\n",
        "            token_b_str=\" function\",\n",
        "            hypothesis=\"(auto)\",\n",
        "        ),\n",
        "        ExperimentSpec(\n",
        "            clean_text=\"The capital of France is\",\n",
        "            corrupt_text=\"The capital of Germany is\",\n",
        "            token_a_str=\" Paris\",\n",
        "            token_b_str=\" Berlin\",\n",
        "            hypothesis=\"(auto)\",\n",
        "        ),\n",
        "        ExperimentSpec(\n",
        "            clean_text=\"The chemical symbol for water is\",\n",
        "            corrupt_text=\"The chemical symbol for salt is\",\n",
        "            token_a_str=\" H\",\n",
        "            token_b_str=\" Na\",\n",
        "            hypothesis=\"(auto)\",\n",
        "        ),\n",
        "        ExperimentSpec(\n",
        "            clean_text=\"A triangle has three sides. A\",\n",
        "            corrupt_text=\"A square has three sides. A\",\n",
        "            token_a_str=\" triangle\",\n",
        "            token_b_str=\" square\",\n",
        "            hypothesis=\"(auto)\",\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "\n",
        "def pick_first_valid_experiment(bpe: BPETokenizer, specs: Optional[List[ExperimentSpec]] = None) -> ValidatedExperiment:\n",
        "    \"\"\"\n",
        "    Tries a list of candidate ExperimentSpec and returns the first one that:\n",
        "    - differs by exactly one BPE token\n",
        "    - has equal BPE length\n",
        "    - has single-token target tokens A/B\n",
        "\n",
        "    If none works, raises ValueError with a helpful message.\n",
        "    \"\"\"\n",
        "    specs = specs or candidate_experiments()\n",
        "    errors: List[str] = []\n",
        "\n",
        "    for i, s in enumerate(specs):\n",
        "        # If hypothesis was left as \"(auto)\", fill it after we know the changed position\n",
        "        try:\n",
        "            tmp = validate_experiment(bpe, s if s.hypothesis != \"(auto)\" else s)\n",
        "            if tmp.spec.hypothesis == \"(auto)\":\n",
        "                auto_h = default_hypothesis(tmp.changed_position)\n",
        "                s2 = ExperimentSpec(\n",
        "                    clean_text=s.clean_text,\n",
        "                    corrupt_text=s.corrupt_text,\n",
        "                    token_a_str=s.token_a_str,\n",
        "                    token_b_str=s.token_b_str,\n",
        "                    hypothesis=auto_h,\n",
        "                )\n",
        "                tmp = validate_experiment(bpe, s2)\n",
        "            return tmp\n",
        "        except Exception as e:\n",
        "            errors.append(f\"[Candidate {i}] {e}\")\n",
        "\n",
        "    raise ValueError(\n",
        "        \"None of the candidate experiments passed the strict Section 4 constraints.\\n\"\n",
        "        \"This is normal: GPT-2 BPE tokenization can be surprising.\\n\\n\"\n",
        "        \"What to do:\\n\"\n",
        "        \"1) Provide your own clean/corrupt prompts and re-run the driver with --clean/--corrupt.\\n\"\n",
        "        \"2) Ensure the two prompts differ by only one BPE token (see token tables).\\n\"\n",
        "        \"3) Ensure token A/B are single BPE tokens (often with leading spaces).\\n\\n\"\n",
        "        \"Errors from candidates:\\n\" + \"\\n\".join(errors)\n",
        "    )\n",
        "\n",
        "\n",
        "# Report-oriented formatting\n",
        "def section4_markdown(valid: ValidatedExperiment) -> str:\n",
        "    \"\"\"\n",
        "    Produces a compact Markdown block you can paste into the PDF report (Section 4).\n",
        "    Includes prompts, token stats, target tokens, and the hypothesis.\n",
        "    \"\"\"\n",
        "    comp = valid.comparison\n",
        "    md_table = tp.format_pair_diff_markdown(comp)\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\"## 4) Experimental Design: Clean/Corrupted Pair + Hypothesis\\n\")\n",
        "    lines.append(\"**Clean prompt:**\")\n",
        "    lines.append(f\"`{valid.spec.clean_text}`\\n\")\n",
        "    lines.append(\"**Corrupted prompt:**\")\n",
        "    lines.append(f\"`{valid.spec.corrupt_text}`\\n\")\n",
        "\n",
        "    lines.append(f\"**Tokenization constraint:** both prompts have **{comp.clean.seq_len}** BPE tokens and differ at exactly **one** token position: **{valid.changed_position}**.\\n\")\n",
        "    lines.append(\"**Token-by-token comparison (diff highlighted):**\\n\")\n",
        "    lines.append(md_table)\n",
        "\n",
        "    lines.append(\"**Target competing tokens (next-token prediction at the last position):**\")\n",
        "    lines.append(f\"- Token A (clean-consistent): `{valid.spec.token_a_str}`  (token id: {valid.token_a_id})\")\n",
        "    lines.append(f\"- Token B (corrupted-consistent): `{valid.spec.token_b_str}`  (token id: {valid.token_b_id})\\n\")\n",
        "\n",
        "    lines.append(\"**Metric used later (matches the handout):**\")\n",
        "    lines.append(\"`logit(Token B)  logit(Token A)` from the last-position logits.\\n\")\n",
        "\n",
        "    lines.append(\"**Hypothesis:**\")\n",
        "    lines.append(valid.spec.hypothesis + \"\\n\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiment_design_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiment_design_driver.py\n",
        "\"\"\"\n",
        "What this does:\n",
        "- Builds/validates an ExperimentSpec:\n",
        "  - clean/corrupt prompts: same length and exactly 1 token difference\n",
        "  - target tokens A/B: each must be a single BPE token id\n",
        "- Prints all evidence needed for the report\n",
        "- Writes a report-ready Markdown file section4.md\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "import tokenization_protocol as tp\n",
        "import experiment_design as ed\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--clean\", type=str, default=None, help=\"Clean prompt text (optional)\")\n",
        "    p.add_argument(\"--corrupt\", type=str, default=None, help=\"Corrupted prompt text (optional)\")\n",
        "    p.add_argument(\"--token_a\", type=str, default=None, help=\"Token A string (optional, usually with leading space)\")\n",
        "    p.add_argument(\"--token_b\", type=str, default=None, help=\"Token B string (optional, usually with leading space)\")\n",
        "    p.add_argument(\"--out_md\", type=str, default=\"section4.md\", help=\"Output markdown file for Section 4\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    if args.clean and args.corrupt and args.token_a and args.token_b:\n",
        "        spec = ed.ExperimentSpec(\n",
        "            clean_text=args.clean,\n",
        "            corrupt_text=args.corrupt,\n",
        "            token_a_str=args.token_a,\n",
        "            token_b_str=args.token_b,\n",
        "            hypothesis=\"(auto)\",  # we will fill after validation\n",
        "        )\n",
        "        valid = ed.validate_experiment(bpe, spec)\n",
        "        # Fill automatic hypothesis (now that we know changed position)\n",
        "        spec2 = ed.ExperimentSpec(\n",
        "            clean_text=spec.clean_text,\n",
        "            corrupt_text=spec.corrupt_text,\n",
        "            token_a_str=spec.token_a_str,\n",
        "            token_b_str=spec.token_b_str,\n",
        "            hypothesis=ed.default_hypothesis(valid.changed_position),\n",
        "        )\n",
        "        valid = ed.validate_experiment(bpe, spec2)\n",
        "    else:\n",
        "        # Auto-pick the first candidate that satisfies strict constraints\n",
        "        valid = ed.pick_first_valid_experiment(bpe)\n",
        "\n",
        "    comp = valid.comparison\n",
        "\n",
        "    print(tp.describe_pair(comp))\n",
        "    print(f\"Changed token position: {valid.changed_position}\")\n",
        "\n",
        "    print(\"\\n=== Clean prompt ===\")\n",
        "    print(valid.spec.clean_text)\n",
        "    print(\"\\n=== Corrupted prompt ===\")\n",
        "    print(valid.spec.corrupt_text)\n",
        "\n",
        "    print(\"\\n=== Token-by-token (clean) ===\")\n",
        "    print(tp.format_token_list_for_console(comp.clean))\n",
        "\n",
        "    print(\"\\n=== Token-by-token (corrupt) ===\")\n",
        "    print(tp.format_token_list_for_console(comp.corrupt))\n",
        "\n",
        "    print(\"\\n=== Target tokens ===\")\n",
        "    print(f\"Token A (clean-consistent): {repr(valid.spec.token_a_str)} -> id {valid.token_a_id}\")\n",
        "    print(f\"Token B (corrupted-consistent): {repr(valid.spec.token_b_str)} -> id {valid.token_b_id}\")\n",
        "\n",
        "    print(\"\\n=== Hypothesis ===\")\n",
        "    print(valid.spec.hypothesis)\n",
        "\n",
        "    md = ed.section4_markdown(valid)\n",
        "    out_path = Path(args.out_md)\n",
        "    out_path.write_text(md, encoding=\"utf-8\")\n",
        "    print(f\"\\nWrote Section 4 Markdown to: {out_path.resolve()}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_section_4.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section_4.py\n",
        "import pathlib\n",
        "import sys\n",
        "\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "# Colab-friendly: ensure mingpt editable install path is visible during pytest subprocess\n",
        "COLAB_MINGPT_PATH = pathlib.Path(\"/content/src/mingpt\")\n",
        "if COLAB_MINGPT_PATH.exists():\n",
        "    sys.path.append(str(COLAB_MINGPT_PATH))\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "import repo_orientation as ro\n",
        "import tokenization_protocol as tp\n",
        "import experiment_design as ed\n",
        "\n",
        "\n",
        "# Section 2 tests (repo orientation)\n",
        "\n",
        "def test_mingpt_importable_and_paths_exist():\n",
        "    paths = ro.get_paths()\n",
        "    assert \"mingpt.__file__\" in paths and \"mingpt.model.__file__\" in paths\n",
        "\n",
        "    pkg_path = pathlib.Path(paths[\"mingpt.__file__\"])\n",
        "    model_path = pathlib.Path(paths[\"mingpt.model.__file__\"])\n",
        "    assert pkg_path.exists(), f\"mingpt package file not found: {pkg_path}\"\n",
        "    assert model_path.exists(), f\"mingpt.model file not found: {model_path}\"\n",
        "\n",
        "\n",
        "def test_attn_bias_fix_present_or_applied():\n",
        "    src = ro.read_model_source()\n",
        "    assert ro.attn_bias_fix_present(src), (\n",
        "        \"Required fix not found in mingpt/model.py. \"\n",
        "        \"Expected assert to ignore keys ending with .attn.bias.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def test_forward_pipeline_landmarks_present():\n",
        "    fwd_src = ro.forward_source()\n",
        "    lm = ro.find_forward_landmarks(fwd_src)\n",
        "    assert lm.has_tok_emb, \"Expected token embedding (wte/tok_emb) usage in forward.\"\n",
        "    assert lm.has_pos_emb, \"Expected positional embedding (wpe/pos_emb) usage in forward.\"\n",
        "    assert lm.has_blocks_loop, \"Expected loop over transformer blocks in forward.\"\n",
        "    assert lm.has_ln_f, \"Expected final layer norm ln_f in forward.\"\n",
        "    assert lm.has_lm_head, \"Expected lm_head/logits in forward.\"\n",
        "\n",
        "\n",
        "def test_fast_forward_and_generate_from_scratch():\n",
        "    # Fast test: avoid downloading HF weights.\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"  # tiny\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "    assert logits.shape == (1, 10, cfg.vocab_size)\n",
        "    assert loss is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=5, do_sample=False)\n",
        "    assert out.shape[1] == 15\n",
        "\n",
        "\n",
        "# Section 3 tests (tokenization protocol)\n",
        "\n",
        "def test_diff_positions_length_mismatch_includes_tail():\n",
        "    a = [1, 2, 3]\n",
        "    b = [1, 2, 3, 4, 5]\n",
        "    diffs = tp.diff_positions(a, b)\n",
        "    assert diffs == [3, 4]\n",
        "\n",
        "\n",
        "def test_compare_reports_detects_one_token_diff_synthetic():\n",
        "    clean = tp.TokenizationReport(\n",
        "        text=\"clean\",\n",
        "        token_ids=[10, 20, 30],\n",
        "        token_strs=[\"a\", \"b\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"abc\",\n",
        "    )\n",
        "    corrupt = tp.TokenizationReport(\n",
        "        text=\"corrupt\",\n",
        "        token_ids=[10, 99, 30],\n",
        "        token_strs=[\"a\", \"X\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"aXc\",\n",
        "    )\n",
        "    comp = tp.compare_clean_corrupt(clean, corrupt)\n",
        "    assert comp.same_length is True\n",
        "    assert comp.diff_positions == [1]\n",
        "    assert comp.diff_count == 1\n",
        "    assert comp.one_token_diff is True\n",
        "\n",
        "\n",
        "def test_assert_one_token_difference_raises_when_multi_diff():\n",
        "    clean = tp.TokenizationReport(\n",
        "        text=\"clean\",\n",
        "        token_ids=[1, 2, 3],\n",
        "        token_strs=[\"a\", \"b\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"abc\",\n",
        "    )\n",
        "    corrupt = tp.TokenizationReport(\n",
        "        text=\"corrupt\",\n",
        "        token_ids=[9, 2, 8],\n",
        "        token_strs=[\"X\", \"b\", \"Y\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"XbY\",\n",
        "    )\n",
        "    comp = tp.compare_clean_corrupt(clean, corrupt)\n",
        "    assert comp.diff_count == 2\n",
        "    with pytest.raises(ValueError):\n",
        "        tp.assert_one_token_difference(comp)\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_bpe_tokenization_roundtrip_and_lengths():\n",
        "    \"\"\"\n",
        "    Slow-ish test because BPETokenizer may download merges/vocab on first use in a fresh runtime.\n",
        "    \"\"\"\n",
        "    from mingpt.bpe import BPETokenizer\n",
        "\n",
        "    try:\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping BPETokenizer test due to tokenizer init/download error: {e}\")\n",
        "\n",
        "    text = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    rep = tp.build_report(bpe, text)\n",
        "\n",
        "    # Basic sanity\n",
        "    assert rep.seq_len > 0\n",
        "    assert len(rep.token_ids) == rep.seq_len\n",
        "    assert len(rep.token_strs) == rep.seq_len\n",
        "\n",
        "    # Roundtrip should contain the key content (exact equality may vary by whitespace normalization)\n",
        "    assert \"Michelle\" in rep.decoded_roundtrip\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_bpe_pair_validation_example_michelle_jones_smith():\n",
        "    \"\"\"\n",
        "    Uses the assignment's canonical-style example to ensure:\n",
        "    - same token length\n",
        "    - at least one differing token (ideally one)\n",
        "    \"\"\"\n",
        "    from mingpt.bpe import BPETokenizer\n",
        "\n",
        "    try:\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping BPETokenizer test due to tokenizer init/download error: {e}\")\n",
        "\n",
        "    clean = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    corrupt = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "\n",
        "    clean_rep = tp.build_report(bpe, clean)\n",
        "    corrupt_rep = tp.build_report(bpe, corrupt)\n",
        "    comp = tp.compare_clean_corrupt(clean_rep, corrupt_rep)\n",
        "\n",
        "    assert comp.same_length is True, f\"Expected same token length; got {clean_rep.seq_len} vs {corrupt_rep.seq_len}\"\n",
        "    assert comp.diff_count >= 1\n",
        "\n",
        "\n",
        "# Section 4 tests (experiment design)\n",
        "def test_changed_token_position_returns_correct_pos_synthetic():\n",
        "    clean = tp.TokenizationReport(\n",
        "        text=\"clean\",\n",
        "        token_ids=[1, 2, 3, 4],\n",
        "        token_strs=[\"a\", \"b\", \"c\", \"d\"],\n",
        "        seq_len=4,\n",
        "        decoded_roundtrip=\"abcd\",\n",
        "    )\n",
        "    corrupt = tp.TokenizationReport(\n",
        "        text=\"corrupt\",\n",
        "        token_ids=[1, 99, 3, 4],\n",
        "        token_strs=[\"a\", \"X\", \"c\", \"d\"],\n",
        "        seq_len=4,\n",
        "        decoded_roundtrip=\"aXcd\",\n",
        "    )\n",
        "    comp = tp.compare_clean_corrupt(clean, corrupt)\n",
        "    assert comp.one_token_diff is True\n",
        "    assert ed.changed_token_position(comp) == 1\n",
        "\n",
        "\n",
        "def test_default_hypothesis_mentions_changed_position():\n",
        "    h = ed.default_hypothesis(7)\n",
        "    assert \"position 7\" in h\n",
        "\n",
        "\n",
        "def test_ensure_leading_space_adds_space_when_missing():\n",
        "    assert ed.ensure_leading_space(\"Paris\") == \" Paris\"\n",
        "    assert ed.ensure_leading_space(\" Paris\") == \" Paris\"\n",
        "\n",
        "\n",
        "def test_single_token_id_raises_for_multi_token_string_with_dummy_tokenizer():\n",
        "    class DummyBPE:\n",
        "        def __call__(self, s: str):\n",
        "            # Return (1, T) tensor\n",
        "            if s == \" def\":\n",
        "                return torch.tensor([[10]], dtype=torch.long)\n",
        "            if s == \" function\":\n",
        "                return torch.tensor([[20]], dtype=torch.long)\n",
        "            if s == \" JavaScript\":\n",
        "                return torch.tensor([[1, 2]], dtype=torch.long)  # multi-token\n",
        "            return torch.tensor([[999]], dtype=torch.long)\n",
        "\n",
        "    bpe = DummyBPE()\n",
        "    assert ed.single_token_id(bpe, \" def\") == 10\n",
        "    assert ed.single_token_id(bpe, \" function\") == 20\n",
        "    with pytest.raises(ValueError):\n",
        "        _ = ed.single_token_id(bpe, \" JavaScript\")\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_pick_first_valid_experiment_runs_or_skips():\n",
        "    \"\"\"\n",
        "    This validates the *real* Section 4 pipeline using BPETokenizer.\n",
        "    It may skip if tokenizer initialization/download fails OR if none of the candidates satisfy\n",
        "    the strict one-token-diff constraint in this environment.\n",
        "    \"\"\"\n",
        "    from mingpt.bpe import BPETokenizer\n",
        "\n",
        "    try:\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping Section 4 BPETokenizer test due to tokenizer init/download error: {e}\")\n",
        "\n",
        "    try:\n",
        "        valid = ed.pick_first_valid_experiment(bpe)\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping because no candidate spec validated under strict constraints: {e}\")\n",
        "\n",
        "    assert valid.comparison.one_token_diff is True\n",
        "    assert isinstance(valid.changed_position, int)\n",
        "    assert isinstance(valid.token_a_id, int)\n",
        "    assert isinstance(valid.token_b_id, int)\n",
        "\n",
        "\n",
        "# Slow model-weight test (optional)\n",
        "@pytest.mark.slow\n",
        "def test_slow_from_pretrained_gpt2_loads_and_runs():\n",
        "    \"\"\"\n",
        "    Slow test: downloads and loads GPT-2 weights.\n",
        "    If network/cache issues happen in Colab, we skip rather than fail hard.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    try:\n",
        "        model = GPT.from_pretrained(\"gpt2\")\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping from_pretrained test due to load/download error: {e}\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, 50257, (1, 8), dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "\n",
        "    assert logits.shape == (1, 8, 50257)\n",
        "    assert loss is None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Pair summary ===\n",
            "Clean tokens:   10\n",
            "Corrupt tokens: 10\n",
            "Same length?    True\n",
            "Diff count:     1\n",
            "Diff positions: [1]\n",
            "One-token diff? True\n",
            "\n",
            "Changed token position: 1\n",
            "\n",
            "=== Clean prompt ===\n",
            "In Python, the keyword to define a function is\n",
            "\n",
            "=== Corrupted prompt ===\n",
            "In JavaScript, the keyword to define a function is\n",
            "\n",
            "=== Token-by-token (clean) ===\n",
            "00 |   818 | 'In'\n",
            "01 | 11361 | ' Python'\n",
            "02 |    11 | ','\n",
            "03 |   262 | ' the'\n",
            "04 | 21179 | ' keyword'\n",
            "05 |   284 | ' to'\n",
            "06 |  8160 | ' define'\n",
            "07 |   257 | ' a'\n",
            "08 |  2163 | ' function'\n",
            "09 |   318 | ' is'\n",
            "\n",
            "=== Token-by-token (corrupt) ===\n",
            "00 |   818 | 'In'\n",
            "01 | 11933 | ' JavaScript'\n",
            "02 |    11 | ','\n",
            "03 |   262 | ' the'\n",
            "04 | 21179 | ' keyword'\n",
            "05 |   284 | ' to'\n",
            "06 |  8160 | ' define'\n",
            "07 |   257 | ' a'\n",
            "08 |  2163 | ' function'\n",
            "09 |   318 | ' is'\n",
            "\n",
            "=== Target tokens ===\n",
            "Token A (clean-consistent): ' def' -> id 825\n",
            "Token B (corrupted-consistent): ' function' -> id 2163\n",
            "\n",
            "=== Hypothesis ===\n",
            "Hypothesis: The changed token position (position 1) should matter most, so patching activations at this position across early-to-mid layers should strongly restore the clean-consistent continuation. Middle layers are expected to dominate because they often integrate and route the key conditioning fact/entity forward through the residual stream. Late layers may also show a secondary effect near the final token position because they directly refine the next-token logits.\n",
            "\n",
            "Wrote Section 4 Markdown to: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/section4.md\n"
          ]
        }
      ],
      "source": [
        "!python experiment_design_driver.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Pair summary ===\n",
            "Clean tokens:   10\n",
            "Corrupt tokens: 10\n",
            "Same length?    True\n",
            "Diff count:     1\n",
            "Diff positions: [1]\n",
            "One-token diff? True\n",
            "\n",
            "Changed token position: 1\n",
            "\n",
            "=== Clean prompt ===\n",
            "In Python, the keyword to define a function is\n",
            "\n",
            "=== Corrupted prompt ===\n",
            "In JavaScript, the keyword to define a function is\n",
            "\n",
            "=== Token-by-token (clean) ===\n",
            "00 |   818 | 'In'\n",
            "01 | 11361 | ' Python'\n",
            "02 |    11 | ','\n",
            "03 |   262 | ' the'\n",
            "04 | 21179 | ' keyword'\n",
            "05 |   284 | ' to'\n",
            "06 |  8160 | ' define'\n",
            "07 |   257 | ' a'\n",
            "08 |  2163 | ' function'\n",
            "09 |   318 | ' is'\n",
            "\n",
            "=== Token-by-token (corrupt) ===\n",
            "00 |   818 | 'In'\n",
            "01 | 11933 | ' JavaScript'\n",
            "02 |    11 | ','\n",
            "03 |   262 | ' the'\n",
            "04 | 21179 | ' keyword'\n",
            "05 |   284 | ' to'\n",
            "06 |  8160 | ' define'\n",
            "07 |   257 | ' a'\n",
            "08 |  2163 | ' function'\n",
            "09 |   318 | ' is'\n",
            "\n",
            "=== Target tokens ===\n",
            "Token A (clean-consistent): ' def' -> id 825\n",
            "Token B (corrupted-consistent): ' function' -> id 2163\n",
            "\n",
            "=== Hypothesis ===\n",
            "Hypothesis: The changed token position (position 1) should matter most, so patching activations at this position across early-to-mid layers should strongly restore the clean-consistent continuation. Middle layers are expected to dominate because they often integrate and route the key conditioning fact/entity forward through the residual stream. Late layers may also show a secondary effect near the final token position because they directly refine the next-token logits.\n",
            "\n",
            "Wrote Section 4 Markdown to: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/section4.md\n"
          ]
        }
      ],
      "source": [
        "!python experiment_design_driver.py \\\n",
        "  --clean \"In Python, the keyword to define a function is\" \\\n",
        "  --corrupt \"In JavaScript, the keyword to define a function is\" \\\n",
        "  --token_a \" def\" \\\n",
        "  --token_b \" function\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                          [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m15 passed\u001b[0m\u001b[32m in 13.23s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m pytest -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting apply_section5_patch.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile apply_section5_patch.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import pathlib\n",
        "import re\n",
        "\n",
        "import mingpt.model\n",
        "\n",
        "\n",
        "def ensure_typing_import(src: str) -> str:\n",
        "    \"\"\"\n",
        "    Ensure we have: from typing import Any, Dict, List, Optional\n",
        "    We insert it right after the torch imports if not present.\n",
        "    \"\"\"\n",
        "    need_line = \"from typing import Any, Dict, List, Optional\"\n",
        "    if need_line in src:\n",
        "        return src\n",
        "\n",
        "    # Try inserting after torch imports (stable in minGPT)\n",
        "    pattern = r\"(import torch\\s*\\nimport torch\\.nn as nn\\s*\\n)\"\n",
        "    m = re.search(pattern, src)\n",
        "    if not m:\n",
        "        # Fallback: insert after \"import math\"\n",
        "        pattern2 = r\"(import math\\s*\\n)\"\n",
        "        m2 = re.search(pattern2, src)\n",
        "        if not m2:\n",
        "            raise RuntimeError(\"Could not find a safe place to insert typing imports.\")\n",
        "        insert_at = m2.end(1)\n",
        "        return src[:insert_at] + \"\\n\" + need_line + \"\\n\" + src[insert_at:]\n",
        "\n",
        "    insert_at = m.end(1)\n",
        "    return src[:insert_at] + need_line + \"\\n\" + src[insert_at:]\n",
        "\n",
        "\n",
        "def insert_instrumentation_attributes(src: str) -> str:\n",
        "    \"\"\"\n",
        "    Insert instrumentation attributes into GPT.__init__ (only once).\n",
        "    We place them right after the parameter count print.\n",
        "    \"\"\"\n",
        "    if \"self.clean_activations\" in src:\n",
        "        return src\n",
        "\n",
        "    marker = 'print(\"number of parameters: %.2fM\" % (n_params/1e6,))'\n",
        "    if marker not in src:\n",
        "        raise RuntimeError(\"Could not find the parameter-count print marker in GPT.__init__.\")\n",
        "\n",
        "    inject = (\n",
        "        marker\n",
        "        + \"\\n\\n\"\n",
        "        + \"        # --- Mechanistic interpretability instrumentation (Section 5) ---\\n\"\n",
        "        + \"        # clean cache: list[layer][position] -> Tensor(d_model) for batch element 0\\n\"\n",
        "        + \"        self.clean_activations: Optional[List[List[torch.Tensor]]] = None\\n\"\n",
        "        + \"        self.clean_activation_meta: Optional[Dict[str, int]] = None\\n\"\n",
        "        + \"        # last recorded activations (debug/inspection; does NOT overwrite clean cache)\\n\"\n",
        "        + \"        self.last_activations: Optional[List[List[torch.Tensor]]] = None\\n\"\n",
        "    )\n",
        "    return src.replace(marker, inject)\n",
        "\n",
        "\n",
        "def insert_clear_method_if_missing(src: str) -> str:\n",
        "    \"\"\"\n",
        "    Add a small helper method to clear clean cache (only once).\n",
        "    We insert it right before forward() definition.\n",
        "    \"\"\"\n",
        "    if \"def clear_clean_activations\" in src:\n",
        "        return src\n",
        "\n",
        "    # Insert before the forward definition (original minGPT has `def forward(self, idx, targets=None):`)\n",
        "    anchor = \"    def forward(self, idx, targets=None):\"\n",
        "    if anchor not in src:\n",
        "        # Maybe forward already patched; insert before `def forward(` anyway\n",
        "        m = re.search(r\"\\n\\s*def forward\\(\", src)\n",
        "        if not m:\n",
        "            raise RuntimeError(\"Could not find forward() to insert clear_clean_activations() before.\")\n",
        "        insert_at = m.start()\n",
        "        helper = (\n",
        "            \"\\n\"\n",
        "            \"    def clear_clean_activations(self) -> None:\\n\"\n",
        "            \"        \\\"\\\"\\\"Clear the stored clean activation cache (Section 5).\\\"\\\"\\\"\\n\"\n",
        "            \"        self.clean_activations = None\\n\"\n",
        "            \"        self.clean_activation_meta = None\\n\"\n",
        "            \"\\n\"\n",
        "        )\n",
        "        return src[:insert_at] + helper + src[insert_at:]\n",
        "\n",
        "    helper = (\n",
        "        \"\\n\"\n",
        "        \"    def clear_clean_activations(self) -> None:\\n\"\n",
        "        \"        \\\"\\\"\\\"Clear the stored clean activation cache (Section 5).\\\"\\\"\\\"\\n\"\n",
        "        \"        self.clean_activations = None\\n\"\n",
        "        \"        self.clean_activation_meta = None\\n\"\n",
        "        \"\\n\"\n",
        "    )\n",
        "    return src.replace(anchor, helper + anchor)\n",
        "\n",
        "\n",
        "def replace_forward_with_instrumented(src: str) -> str:\n",
        "    \"\"\"\n",
        "    Replace GPT.forward with an instrumented version that can record activations:\n",
        "      - after each transformer block\n",
        "      - for each token position\n",
        "    Stored as: list[layer][position] -> Tensor(d_model), for batch element 0\n",
        "    \"\"\"\n",
        "    new_forward = r'''\n",
        "    def forward(\n",
        "        self,\n",
        "        idx,\n",
        "        targets=None,\n",
        "        *,\n",
        "        record_activations: bool = False,\n",
        "        cache_activations: bool = False,\n",
        "        overwrite_cache: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Forward pass with optional activation recording (Section 5).\n",
        "\n",
        "        Activation definition (standardized for this assignment):\n",
        "          - residual stream output AFTER each transformer block\n",
        "          - recorded for each token position\n",
        "          - stored for batch element 0 only\n",
        "          - deep-copied via detach().clone()\n",
        "\n",
        "        Storage:\n",
        "          - self.clean_activations: persistent \"clean run cache\" (read later for patching)\n",
        "          - self.last_activations: last recorded activations (debug/inspection)\n",
        "        \"\"\"\n",
        "        # If we're caching, we must record\n",
        "        record_activations = bool(record_activations or cache_activations)\n",
        "\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)  # (1, t)\n",
        "\n",
        "        # Clear last_activations to avoid stale reads\n",
        "        self.last_activations = None\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx)  # (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos)  # (1, t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "\n",
        "        acts = None\n",
        "        if record_activations:\n",
        "            acts = []  # list[layer][pos] -> Tensor(d_model)\n",
        "\n",
        "        for layer_idx, block in enumerate(self.transformer.h):\n",
        "            x = block(x)\n",
        "\n",
        "            if record_activations:\n",
        "                # store ONLY batch element 0\n",
        "                layer_acts = []\n",
        "                for p in range(t):\n",
        "                    # defensive copy: detach + clone\n",
        "                    layer_acts.append(x[0, p, :].detach().clone())\n",
        "                acts.append(layer_acts)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        # finalize activation storage\n",
        "        if record_activations:\n",
        "            self.last_activations = acts\n",
        "\n",
        "        if cache_activations:\n",
        "            if (self.clean_activations is not None) and (not overwrite_cache):\n",
        "                raise RuntimeError(\n",
        "                    \"Clean activation cache already exists. \"\n",
        "                    \"Pass overwrite_cache=True (or call model.clear_clean_activations()) \"\n",
        "                    \"to replace it for a new clean prompt.\"\n",
        "                )\n",
        "\n",
        "            self.clean_activations = acts\n",
        "            self.clean_activation_meta = {\n",
        "                \"seq_len\": int(t),\n",
        "                \"n_layer\": int(len(self.transformer.h)),\n",
        "                \"d_model\": int(logits.shape[-1] if False else x.shape[-1]),  # x is (b,t,d_model) here pre-ln_f? ln_f preserves size\n",
        "            }\n",
        "\n",
        "        return logits, loss\n",
        "'''.strip(\"\\n\")\n",
        "\n",
        "    # Replace ONLY the forward() definition inside GPT, stopping before generate()\n",
        "    # We match from `def forward(self, idx, targets=None):` up to just before `@torch.no_grad()` of generate.\n",
        "    pattern = r\"\\n\\s*def forward\\(self, idx, targets=None\\):\\n(?:.|\\n)*?(?=\\n\\s*@torch\\.no_grad\\(\\)\\n\\s*def generate)\"\n",
        "    if not re.search(pattern, src):\n",
        "        # If forward signature already changed, match more generally:\n",
        "        pattern2 = r\"\\n\\s*def forward\\([^\\)]*\\):\\n(?:.|\\n)*?(?=\\n\\s*@torch\\.no_grad\\(\\)\\n\\s*def generate)\"\n",
        "        if not re.search(pattern2, src):\n",
        "            raise RuntimeError(\"Could not find GPT.forward block to replace (before generate).\")\n",
        "        src, n = re.subn(pattern2, \"\\n\" + new_forward + \"\\n\", src, count=1)\n",
        "        if n != 1:\n",
        "            raise RuntimeError(\"Unexpected number of replacements for forward().\")\n",
        "        return src\n",
        "\n",
        "    src, n = re.subn(pattern, \"\\n\" + new_forward + \"\\n\", src, count=1)\n",
        "    if n != 1:\n",
        "        raise RuntimeError(\"Unexpected number of replacements for forward().\")\n",
        "    return src\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    src = path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "    src = ensure_typing_import(src)\n",
        "    src = insert_instrumentation_attributes(src)\n",
        "    src = insert_clear_method_if_missing(src)\n",
        "    src = replace_forward_with_instrumented(src)\n",
        "\n",
        "    path.write_text(src, encoding=\"utf-8\")\n",
        "    print(f\" Section 5 patch applied to: {path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Section 5 patch applied to: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/src/mingpt/mingpt/model.py\n"
          ]
        }
      ],
      "source": [
        "!python apply_section5_patch.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 0.14M\n",
            "layers cached: 3\n",
            "positions cached (layer 0): 10\n",
            "one activation shape: (48,)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from mingpt.model import GPT\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "set_seed(123)\n",
        "\n",
        "cfg = GPT.get_default_config()\n",
        "cfg.model_type = \"gpt-nano\"\n",
        "cfg.vocab_size = 1000\n",
        "cfg.block_size = 64\n",
        "\n",
        "m = GPT(cfg).eval()\n",
        "idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits, _ = m(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "print(\"layers cached:\", len(m.clean_activations))\n",
        "print(\"positions cached (layer 0):\", len(m.clean_activations[0]))\n",
        "print(\"one activation shape:\", tuple(m.clean_activations[0][0].shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_section_5.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section_5.py\n",
        "import pathlib\n",
        "import sys\n",
        "\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "COLAB_MINGPT_PATH = pathlib.Path(\"/content/src/mingpt\")\n",
        "if COLAB_MINGPT_PATH.exists():\n",
        "    sys.path.append(str(COLAB_MINGPT_PATH))\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "# Section 2 tests (repo orientation)\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ForwardLandmarks:\n",
        "    has_tok_emb: bool\n",
        "    has_pos_emb: bool\n",
        "    has_blocks_loop: bool\n",
        "    has_ln_f: bool\n",
        "    has_lm_head: bool\n",
        "\n",
        "\n",
        "def get_paths() -> Dict[str, str]:\n",
        "    pkg_path = pathlib.Path(mingpt.__file__).resolve()\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return {\n",
        "        \"mingpt.__file__\": str(pkg_path),\n",
        "        \"mingpt.model.__file__\": str(model_path),\n",
        "    }\n",
        "\n",
        "\n",
        "def read_model_source() -> str:\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return model_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "def attn_bias_fix_present(model_source: str) -> bool:\n",
        "    return 'len([k for k in sd if not k.endswith(\".attn.bias\")])' in model_source\n",
        "\n",
        "\n",
        "def forward_source() -> str:\n",
        "    return inspect.getsource(GPT.forward)\n",
        "\n",
        "\n",
        "def find_forward_landmarks(src: str) -> ForwardLandmarks:\n",
        "    has_tok_emb = \"tok_emb\" in src and \"wte\" in src\n",
        "    has_pos_emb = \"pos_emb\" in src and \"wpe\" in src\n",
        "    has_blocks_loop = (\"for block in self.transformer.h\" in src) or (\"for layer_idx, block in enumerate(self.transformer.h\" in src)\n",
        "    has_ln_f = \"ln_f\" in src\n",
        "    has_lm_head = \"lm_head\" in src and \"logits\" in src\n",
        "    return ForwardLandmarks(\n",
        "        has_tok_emb=has_tok_emb,\n",
        "        has_pos_emb=has_pos_emb,\n",
        "        has_blocks_loop=has_blocks_loop,\n",
        "        has_ln_f=has_ln_f,\n",
        "        has_lm_head=has_lm_head,\n",
        "    )\n",
        "\n",
        "\n",
        "def test_mingpt_importable_and_paths_exist():\n",
        "    paths = get_paths()\n",
        "    assert \"mingpt.__file__\" in paths and \"mingpt.model.__file__\" in paths\n",
        "\n",
        "    pkg_path = pathlib.Path(paths[\"mingpt.__file__\"])\n",
        "    model_path = pathlib.Path(paths[\"mingpt.model.__file__\"])\n",
        "    assert pkg_path.exists(), f\"mingpt package file not found: {pkg_path}\"\n",
        "    assert model_path.exists(), f\"mingpt.model file not found: {model_path}\"\n",
        "\n",
        "\n",
        "def test_attn_bias_fix_present_or_applied():\n",
        "    src = read_model_source()\n",
        "    assert attn_bias_fix_present(src), (\n",
        "        \"Required fix not found in mingpt/model.py. \"\n",
        "        \"Expected assert to ignore keys ending with .attn.bias.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def test_forward_pipeline_landmarks_present():\n",
        "    fwd_src = forward_source()\n",
        "    lm = find_forward_landmarks(fwd_src)\n",
        "    assert lm.has_tok_emb, \"Expected token embedding (wte/tok_emb) usage in forward.\"\n",
        "    assert lm.has_pos_emb, \"Expected positional embedding (wpe/pos_emb) usage in forward.\"\n",
        "    assert lm.has_blocks_loop, \"Expected loop over transformer blocks in forward.\"\n",
        "    assert lm.has_ln_f, \"Expected final layer norm ln_f in forward.\"\n",
        "    assert lm.has_lm_head, \"Expected lm_head/logits in forward.\"\n",
        "\n",
        "\n",
        "def test_fast_forward_and_generate_from_scratch():\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "    assert logits.shape == (1, 10, cfg.vocab_size)\n",
        "    assert loss is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=5, do_sample=False)\n",
        "    assert out.shape[1] == 15\n",
        "\n",
        "\n",
        "# Section 5 tests (activation recording / clean cache)\n",
        "\n",
        "def _make_tiny_gpt():\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg).eval()\n",
        "    return model, cfg\n",
        "\n",
        "\n",
        "def test_section5_cache_structure_and_shapes():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 12\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    assert model.clean_activations is not None\n",
        "    assert isinstance(model.clean_activations, list)\n",
        "    assert len(model.clean_activations) == len(model.transformer.h)  # n_layer\n",
        "    assert len(model.clean_activations[0]) == T\n",
        "\n",
        "    # each [layer][pos] must be (d_model,)\n",
        "    d_model = model.transformer.wte.weight.shape[1]\n",
        "    a00 = model.clean_activations[0][0]\n",
        "    assert tuple(a00.shape) == (d_model,)\n",
        "    assert a00.requires_grad is False\n",
        "\n",
        "\n",
        "def test_section5_cache_uses_detach_clone_not_views():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 6\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    # If we had stored views into the same underlying tensor, data_ptr() would often match.\n",
        "    a0 = model.clean_activations[0][0]\n",
        "    a1 = model.clean_activations[0][1]\n",
        "    assert a0.data_ptr() != a1.data_ptr(), \"Expected clone()d per-position tensors with distinct storage.\"\n",
        "\n",
        "\n",
        "def test_section5_logits_identical_with_and_without_recording():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 10\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits1, _ = model(idx)  # normal\n",
        "        logits2, _ = model(idx, record_activations=True, cache_activations=False)  # recording only\n",
        "\n",
        "    assert torch.allclose(logits1, logits2), \"Activation recording must not change logits.\"\n",
        "\n",
        "\n",
        "def test_section5_clean_cache_not_overwritten_unless_requested():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    idx1 = torch.randint(0, cfg.vocab_size, (1, 8), dtype=torch.long)\n",
        "    idx2 = torch.randint(0, cfg.vocab_size, (1, 8), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx1, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    # snapshot values\n",
        "    snap = [[t.clone() for t in layer] for layer in model.clean_activations]\n",
        "\n",
        "    # normal forward must not change cache\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx2)\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(len(snap[L])):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p])\n",
        "\n",
        "    # recording-only must not change clean cache\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx2, record_activations=True, cache_activations=False)\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(len(snap[L])):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p])\n",
        "\n",
        "    # caching again WITHOUT overwrite must raise\n",
        "    with pytest.raises(RuntimeError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(idx2, cache_activations=True, overwrite_cache=False)\n",
        "\n",
        "\n",
        "def test_section5_batch_behavior_records_only_first_element():\n",
        "    model1, cfg = _make_tiny_gpt()\n",
        "    model2, _ = _make_tiny_gpt()\n",
        "    model2.load_state_dict(model1.state_dict())  # identical weights\n",
        "\n",
        "    T = 9\n",
        "    idx_batch = torch.randint(0, cfg.vocab_size, (2, T), dtype=torch.long)\n",
        "    idx_first = idx_batch[:1, :]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model1(idx_batch, cache_activations=True, overwrite_cache=True)\n",
        "        _ = model2(idx_first, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    # caches must match up to floating-point tolerance\n",
        "    for L in range(len(model1.clean_activations)):\n",
        "        for p in range(T):\n",
        "            assert torch.allclose(\n",
        "                model1.clean_activations[L][p],\n",
        "                model2.clean_activations[L][p],\n",
        "                rtol=1e-5,\n",
        "                atol=1e-6,\n",
        "            )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m9 passed\u001b[0m\u001b[32m in 3.03s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m pytest -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 5 EXTRA COMPROBATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 0.14M\n",
            "Cached layers: 3\n",
            "Seq len in cache (layer 0): 12\n",
            "One activation shape: (48,)\n",
            "Activation requires_grad: False\n",
            "Logits identical (cache ON vs OFF)?: True\n",
            "Cache unchanged after normal forward?: True\n",
            "Re-cache without overwrite correctly raised RuntimeError:\n",
            "  Clean activation cache already exists. Pass overwrite_cache=True (or call model.clear_clean_activations()) to replace it for a new clean pro ...\n",
            "\n",
            " Section 5 smoke test PASSED under your cache-protection semantics.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from mingpt.model import GPT\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "set_seed(3407)\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# ---- Build a tiny model (fast, no downloads) ----\n",
        "cfg = GPT.get_default_config()\n",
        "cfg.model_type = \"gpt-nano\"\n",
        "cfg.vocab_size = 1000\n",
        "cfg.block_size = 64\n",
        "\n",
        "model = GPT(cfg)\n",
        "model.eval()\n",
        "\n",
        "B, T = 1, 12\n",
        "idx = torch.randint(0, cfg.vocab_size, (B, T), dtype=torch.long)\n",
        "\n",
        "# ---- 1) Cache activations (CLEAN RUN) ----\n",
        "logits_cache, _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "print(\"Cached layers:\", len(model.clean_activations))\n",
        "print(\"Seq len in cache (layer 0):\", len(model.clean_activations[0]))\n",
        "print(\"One activation shape:\", tuple(model.clean_activations[0][0].shape))\n",
        "print(\"Activation requires_grad:\", model.clean_activations[0][0].requires_grad)\n",
        "\n",
        "# Structural checks\n",
        "assert len(model.clean_activations) == cfg.n_layer\n",
        "assert all(len(layer) == T for layer in model.clean_activations)\n",
        "assert model.clean_activations[0][0].shape == (cfg.n_embd,)\n",
        "assert model.clean_activations[0][0].requires_grad is False\n",
        "\n",
        "# ---- 2) Caching must NOT change logits ----\n",
        "logits_no_cache, _ = model(idx, cache_activations=False)\n",
        "same = torch.allclose(logits_cache, logits_no_cache, rtol=1e-5, atol=1e-6)\n",
        "print(\"Logits identical (cache ON vs OFF)?:\", same)\n",
        "assert same\n",
        "\n",
        "# ---- 3A) Normal forward runs must NOT modify the existing clean cache ----\n",
        "fp_before = (\n",
        "    model.clean_activations[0][0].clone(),\n",
        "    model.clean_activations[-1][-1].clone(),\n",
        ")\n",
        "\n",
        "idx2 = torch.randint(0, cfg.vocab_size, (B, T), dtype=torch.long)\n",
        "_ = model(idx2, cache_activations=False)   # <-- key change: do NOT try to cache again\n",
        "\n",
        "fp_after = (\n",
        "    model.clean_activations[0][0].clone(),\n",
        "    model.clean_activations[-1][-1].clone(),\n",
        ")\n",
        "\n",
        "unchanged = torch.allclose(fp_before[0], fp_after[0]) and torch.allclose(fp_before[1], fp_after[1])\n",
        "print(\"Cache unchanged after normal forward?:\", unchanged)\n",
        "assert unchanged\n",
        "\n",
        "# ---- 3B) Attempting to re-cache without overwrite MUST raise (your current behavior) ----\n",
        "raised = False\n",
        "try:\n",
        "    _ = model(idx2, cache_activations=True, overwrite_cache=False)\n",
        "except RuntimeError as e:\n",
        "    raised = True\n",
        "    print(\"Re-cache without overwrite correctly raised RuntimeError:\")\n",
        "    print(\" \", str(e)[:140], \"...\")\n",
        "assert raised\n",
        "\n",
        "print(\"\\n Section 5 smoke test PASSED under your cache-protection semantics.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 124.44M\n",
            "gpt2 layers cached: 12\n",
            "seq_len cached: 11\n",
            "d_model shape: (768,)\n",
            "last logits shape: (1, 50257)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "model = GPT.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "bpe = BPETokenizer()\n",
        "text = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "idx = bpe(text)  # (1, T)\n",
        "\n",
        "logits, _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "print(\"gpt2 layers cached:\", len(model.clean_activations))         # EXPECT: 12\n",
        "print(\"seq_len cached:\", len(model.clean_activations[0]))          # EXPECT: T\n",
        "print(\"d_model shape:\", tuple(model.clean_activations[0][0].shape))# EXPECT: (768,)\n",
        "print(\"last logits shape:\", tuple(logits[:, -1, :].shape))         # EXPECT: (1, 50257)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting apply_section6_patch.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile apply_section6_patch.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import pathlib\n",
        "import re\n",
        "\n",
        "import mingpt.model\n",
        "\n",
        "\n",
        "def ensure_typing_import(src: str) -> str:\n",
        "    \"\"\"\n",
        "    Ensure we have: from typing import Any, Dict, List, Optional\n",
        "    Insert it in a stable spot if missing.\n",
        "    \"\"\"\n",
        "    need_line = \"from typing import Any, Dict, List, Optional\"\n",
        "    if need_line in src:\n",
        "        return src\n",
        "\n",
        "    # Prefer inserting after torch imports\n",
        "    pattern = r\"(import torch\\s*\\nimport torch\\.nn as nn\\s*\\n)\"\n",
        "    m = re.search(pattern, src)\n",
        "    if m:\n",
        "        insert_at = m.end(1)\n",
        "        return src[:insert_at] + need_line + \"\\n\" + src[insert_at:]\n",
        "\n",
        "    # Fallback: after import math\n",
        "    pattern2 = r\"(import math\\s*\\n)\"\n",
        "    m2 = re.search(pattern2, src)\n",
        "    if not m2:\n",
        "        raise RuntimeError(\"Could not find a safe place to insert typing imports.\")\n",
        "    insert_at = m2.end(1)\n",
        "    return src[:insert_at] + \"\\n\" + need_line + \"\\n\" + src[insert_at:]\n",
        "\n",
        "\n",
        "def insert_last_logits_attribute(src: str) -> str:\n",
        "    \"\"\"\n",
        "    Add: self.last_logits: Optional[torch.Tensor] = None\n",
        "    Prefer inserting next to Section 5 instrumentation attributes if present.\n",
        "    Idempotent.\n",
        "    \"\"\"\n",
        "    if \"self.last_logits\" in src:\n",
        "        return src\n",
        "\n",
        "    # If Section 5 instrumentation exists, insert after last_activations\n",
        "    pattern = r\"(self\\.last_activations:\\s*Optional\\[List\\[List\\[torch\\.Tensor\\]\\]\\]\\s*=\\s*None\\s*\\n)\"\n",
        "    m = re.search(pattern, src)\n",
        "    if m:\n",
        "        inject = (\n",
        "            m.group(1)\n",
        "            + \"        # last-token logits (Section 6): logits at final prompt position (next-token distribution)\\n\"\n",
        "            + \"        self.last_logits: Optional[torch.Tensor] = None\\n\"\n",
        "        )\n",
        "        return src[:m.start(1)] + inject + src[m.end(1):]\n",
        "\n",
        "    # Fallback: insert after parameter-count print in __init__\n",
        "    marker = 'print(\"number of parameters: %.2fM\" % (n_params/1e6,))'\n",
        "    if marker not in src:\n",
        "        raise RuntimeError(\"Could not find a safe marker in GPT.__init__ to insert last_logits attribute.\")\n",
        "\n",
        "    inject = (\n",
        "        marker\n",
        "        + \"\\n\\n\"\n",
        "        + \"        # --- Mechanistic interpretability instrumentation (Section 6) ---\\n\"\n",
        "        + \"        # logits at final prompt position: shape (B, vocab_size)\\n\"\n",
        "        + \"        self.last_logits: Optional[torch.Tensor] = None\\n\"\n",
        "    )\n",
        "    return src.replace(marker, inject)\n",
        "\n",
        "\n",
        "def insert_last_logits_assignment_in_forward(src: str) -> str:\n",
        "    \"\"\"\n",
        "    After logits = self.lm_head(x), insert:\n",
        "        self.last_logits = logits[:, -1, :].detach().clone()\n",
        "    Idempotent.\n",
        "    \"\"\"\n",
        "    if re.search(r\"self\\.last_logits\\s*=\\s*logits\\[:,\\s*-1,\\s*:\\]\\.detach\\(\\)\\.clone\\(\\)\", src):\n",
        "        return src\n",
        "\n",
        "    # Find logits computation line inside forward\n",
        "    pattern = r\"(\\n(\\s*)logits\\s*=\\s*self\\.lm_head\\(x\\)\\s*\\n)\"\n",
        "    m = re.search(pattern, src)\n",
        "    if not m:\n",
        "        raise RuntimeError(\"Could not find the line `logits = self.lm_head(x)` in GPT.forward.\")\n",
        "\n",
        "    full_match = m.group(1)\n",
        "    indent = m.group(2)\n",
        "\n",
        "    insertion = (\n",
        "        full_match\n",
        "        + f\"{indent}# --- Section 6: store last-position logits (next-token distribution after the prompt) ---\\n\"\n",
        "        + f\"{indent}# Shape: (B, vocab_size). We detach+clone to avoid accidental mutation across runs.\\n\"\n",
        "        + f\"{indent}self.last_logits = logits[:, -1, :].detach().clone()\\n\"\n",
        "    )\n",
        "\n",
        "    return src[:m.start(1)] + insertion + src[m.end(1):]\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    src = path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "    src = ensure_typing_import(src)\n",
        "    src = insert_last_logits_attribute(src)\n",
        "    src = insert_last_logits_assignment_in_forward(src)\n",
        "\n",
        "    path.write_text(src, encoding=\"utf-8\")\n",
        "    print(f\" Section 6 patch applied to: {path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Section 6 patch applied to: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/src/mingpt/mingpt/model.py\n"
          ]
        }
      ],
      "source": [
        "!python apply_section6_patch.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting last_logits_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile last_logits_driver.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "\n",
        "def single_token_id(bpe: BPETokenizer, token_str: str) -> int:\n",
        "    ids = bpe(token_str)[0].tolist()\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{repr(token_str)} is not a single BPE token. Got {len(ids)} ids: {ids}\")\n",
        "    return int(ids[0])\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    clean = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    idx = bpe(clean).to(device)  # (1, T)\n",
        "\n",
        "    logits, _ = model(idx)  # forward ONCE\n",
        "    assert model.last_logits is not None, \"model.last_logits was not set!\"\n",
        "    print(\"logits shape:\", tuple(logits.shape))\n",
        "    print(\"last_logits shape:\", tuple(model.last_logits.shape))  # (1, vocab)\n",
        "\n",
        "    # Top-k next tokens from last_logits\n",
        "    k = 10\n",
        "    last = model.last_logits[0]  # (vocab,)\n",
        "    probs = F.softmax(last, dim=-1)\n",
        "    top_p, top_i = torch.topk(probs, k)\n",
        "\n",
        "    print(\"\\n=== Top-k next tokens (from model.last_logits) ===\")\n",
        "    for rank in range(k):\n",
        "        tid = int(top_i[rank])\n",
        "        tok = bpe.decode(torch.tensor([tid]))\n",
        "        print(f\"{rank+1:02d}. id={tid:5d} tok={repr(tok):>12} prob={float(top_p[rank]):.4f}\")\n",
        "\n",
        "    # Metric: logit(TokenB) - logit(TokenA)\n",
        "    token_a = \" Jones\"\n",
        "    token_b = \" Smith\"\n",
        "    id_a = single_token_id(bpe, token_a)\n",
        "    id_b = single_token_id(bpe, token_b)\n",
        "\n",
        "    logit_a = float(model.last_logits[0, id_a])\n",
        "    logit_b = float(model.last_logits[0, id_b])\n",
        "    score = logit_b - logit_a\n",
        "\n",
        "    print(\"\\n=== Logit-diff metric ===\")\n",
        "    print(f\"Token A: {repr(token_a)} id={id_a} logit={logit_a:.4f}\")\n",
        "    print(f\"Token B: {repr(token_b)} id={id_b} logit={logit_b:.4f}\")\n",
        "    print(f\"score = logit(B) - logit(A) = {score:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "number of parameters: 124.44M\n",
            "logits shape: (1, 11, 50257)\n",
            "last_logits shape: (1, 50257)\n",
            "\n",
            "=== Top-k next tokens (from model.last_logits) ===\n",
            "01. id=  373 tok=      ' was' prob=0.1634\n",
            "02. id= 5437 tok=    ' Jones' prob=0.1396\n",
            "03. id=  338 tok=        \"'s\" prob=0.0806\n",
            "04. id=  550 tok=      ' had' prob=0.0491\n",
            "05. id=  318 tok=       ' is' prob=0.0229\n",
            "06. id=  290 tok=      ' and' prob=0.0227\n",
            "07. id=   11 tok=         ',' prob=0.0222\n",
            "08. id=  531 tok=     ' said' prob=0.0134\n",
            "09. id=  468 tok=      ' has' prob=0.0120\n",
            "10. id=  635 tok=     ' also' prob=0.0117\n",
            "\n",
            "=== Logit-diff metric ===\n",
            "Token A: ' Jones' id=5437 logit=-79.6386\n",
            "Token B: ' Smith' id=4176 logit=-83.7627\n",
            "score = logit(B) - logit(A) = -4.1241\n"
          ]
        }
      ],
      "source": [
        "!python last_logits_driver.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_section_6.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section_6.py\n",
        "import inspect\n",
        "import pathlib\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict\n",
        "\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "# If you happen to be in a Colab-like layout, keep this harmless path add.\n",
        "COLAB_MINGPT_PATH = pathlib.Path(\"/content/src/mingpt\")\n",
        "if COLAB_MINGPT_PATH.exists():\n",
        "    sys.path.append(str(COLAB_MINGPT_PATH))\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "\n",
        "# Section 2: repo orientation sanity\n",
        "@dataclass(frozen=True)\n",
        "class ForwardLandmarks:\n",
        "    has_tok_emb: bool\n",
        "    has_pos_emb: bool\n",
        "    has_blocks_loop: bool\n",
        "    has_ln_f: bool\n",
        "    has_lm_head: bool\n",
        "\n",
        "\n",
        "def get_paths() -> Dict[str, str]:\n",
        "    pkg_path = pathlib.Path(mingpt.__file__).resolve()\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return {\n",
        "        \"mingpt.__file__\": str(pkg_path),\n",
        "        \"mingpt.model.__file__\": str(model_path),\n",
        "    }\n",
        "\n",
        "\n",
        "def read_model_source() -> str:\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return model_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "def attn_bias_fix_present(model_source: str) -> bool:\n",
        "    return 'len([k for k in sd if not k.endswith(\".attn.bias\")])' in model_source\n",
        "\n",
        "\n",
        "def forward_source() -> str:\n",
        "    return inspect.getsource(GPT.forward)\n",
        "\n",
        "\n",
        "def find_forward_landmarks(src: str) -> ForwardLandmarks:\n",
        "    has_tok_emb = (\"tok_emb\" in src) and (\"wte\" in src)\n",
        "    has_pos_emb = (\"pos_emb\" in src) and (\"wpe\" in src)\n",
        "    has_blocks_loop = (\"for block in self.transformer.h\" in src) or (\"enumerate(self.transformer.h\" in src)\n",
        "    has_ln_f = \"ln_f\" in src\n",
        "    has_lm_head = (\"lm_head\" in src) and (\"logits\" in src)\n",
        "    return ForwardLandmarks(\n",
        "        has_tok_emb=has_tok_emb,\n",
        "        has_pos_emb=has_pos_emb,\n",
        "        has_blocks_loop=has_blocks_loop,\n",
        "        has_ln_f=has_ln_f,\n",
        "        has_lm_head=has_lm_head,\n",
        "    )\n",
        "\n",
        "\n",
        "def test_mingpt_importable_and_paths_exist():\n",
        "    paths = get_paths()\n",
        "    assert \"mingpt.__file__\" in paths and \"mingpt.model.__file__\" in paths\n",
        "\n",
        "    pkg_path = pathlib.Path(paths[\"mingpt.__file__\"])\n",
        "    model_path = pathlib.Path(paths[\"mingpt.model.__file__\"])\n",
        "    assert pkg_path.exists(), f\"mingpt package file not found: {pkg_path}\"\n",
        "    assert model_path.exists(), f\"mingpt.model file not found: {model_path}\"\n",
        "\n",
        "\n",
        "def test_attn_bias_fix_present_or_applied():\n",
        "    src = read_model_source()\n",
        "    assert attn_bias_fix_present(src), (\n",
        "        \"Required fix not found in mingpt/model.py. \"\n",
        "        \"Expected assert to ignore keys ending with .attn.bias.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def test_forward_pipeline_landmarks_present():\n",
        "    fwd_src = forward_source()\n",
        "    lm = find_forward_landmarks(fwd_src)\n",
        "    assert lm.has_tok_emb, \"Expected token embedding (wte/tok_emb) usage in forward.\"\n",
        "    assert lm.has_pos_emb, \"Expected positional embedding (wpe/pos_emb) usage in forward.\"\n",
        "    assert lm.has_blocks_loop, \"Expected loop over transformer blocks in forward.\"\n",
        "    assert lm.has_ln_f, \"Expected final layer norm ln_f in forward.\"\n",
        "    assert lm.has_lm_head, \"Expected lm_head/logits in forward.\"\n",
        "\n",
        "\n",
        "def test_fast_forward_and_generate_from_scratch():\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg).eval()\n",
        "\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "    assert logits.shape == (1, 10, cfg.vocab_size)\n",
        "    assert loss is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=5, do_sample=False)\n",
        "    assert out.shape[1] == 15\n",
        "\n",
        "\n",
        "# Shared helper for Section 5/6 tests\n",
        "def _make_tiny_gpt():\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg).eval()\n",
        "    return model, cfg\n",
        "\n",
        "\n",
        "# Section 5: activation recording / clean cache\n",
        "def test_section5_cache_structure_and_shapes():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 12\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    assert model.clean_activations is not None\n",
        "    assert isinstance(model.clean_activations, list)\n",
        "    assert len(model.clean_activations) == len(model.transformer.h)  # n_layer\n",
        "    assert len(model.clean_activations[0]) == T\n",
        "\n",
        "    d_model = model.transformer.wte.weight.shape[1]\n",
        "    a00 = model.clean_activations[0][0]\n",
        "    assert tuple(a00.shape) == (d_model,)\n",
        "    assert a00.requires_grad is False\n",
        "\n",
        "\n",
        "def test_section5_cache_uses_detach_clone_not_views():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 6\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    a0 = model.clean_activations[0][0]\n",
        "    a1 = model.clean_activations[0][1]\n",
        "    assert a0.data_ptr() != a1.data_ptr(), \"Expected clone()d per-position tensors with distinct storage.\"\n",
        "\n",
        "\n",
        "def test_section5_logits_identical_with_and_without_recording():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 10\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits1, _ = model(idx)  # normal\n",
        "        logits2, _ = model(idx, record_activations=True, cache_activations=False)  # recording only\n",
        "\n",
        "    assert torch.allclose(logits1, logits2), \"Activation recording must not change logits.\"\n",
        "\n",
        "\n",
        "def test_section5_clean_cache_not_overwritten_unless_requested():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    idx1 = torch.randint(0, cfg.vocab_size, (1, 8), dtype=torch.long)\n",
        "    idx2 = torch.randint(0, cfg.vocab_size, (1, 8), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx1, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    snap = [[t.clone() for t in layer] for layer in model.clean_activations]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx2)\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(len(snap[L])):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx2, record_activations=True, cache_activations=False)\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(len(snap[L])):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p])\n",
        "\n",
        "    with pytest.raises(RuntimeError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(idx2, cache_activations=True, overwrite_cache=False)\n",
        "\n",
        "\n",
        "def test_section5_batch_behavior_records_only_first_element():\n",
        "    model1, cfg = _make_tiny_gpt()\n",
        "    model2, _ = _make_tiny_gpt()\n",
        "    model2.load_state_dict(model1.state_dict())  # identical weights\n",
        "\n",
        "    T = 9\n",
        "    idx_batch = torch.randint(0, cfg.vocab_size, (2, T), dtype=torch.long)\n",
        "    idx_first = idx_batch[:1, :]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model1(idx_batch, cache_activations=True, overwrite_cache=True)\n",
        "        _ = model2(idx_first, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    for L in range(len(model1.clean_activations)):\n",
        "        for p in range(T):\n",
        "            assert torch.allclose(\n",
        "                model1.clean_activations[L][p],\n",
        "                model2.clean_activations[L][p],\n",
        "                rtol=1e-5,\n",
        "                atol=1e-6,\n",
        "            )\n",
        "\n",
        "# Section 6: last-token logits extraction (NEW)\n",
        "def test_section6_last_logits_exists_and_matches_last_position_logits():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 11\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(idx)\n",
        "\n",
        "    assert hasattr(model, \"last_logits\"), \"Expected GPT to expose model.last_logits\"\n",
        "    assert model.last_logits is not None, \"model.last_logits was not set by forward()\"\n",
        "    assert tuple(model.last_logits.shape) == (1, cfg.vocab_size)\n",
        "\n",
        "    expected = logits[:, -1, :]\n",
        "    assert torch.allclose(model.last_logits, expected), \"model.last_logits must equal logits[:, -1, :] for the same run\"\n",
        "\n",
        "\n",
        "def test_section6_last_logits_is_detached_and_cloned():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 7\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(idx)\n",
        "\n",
        "    view = logits[:, -1, :]  # view into logits storage\n",
        "    assert model.last_logits.requires_grad is False\n",
        "    # clone must not share underlying storage with the view\n",
        "    assert model.last_logits.data_ptr() != view.data_ptr(), \"Expected last_logits to be a clone(), not a view\"\n",
        "\n",
        "\n",
        "def test_section6_last_logits_computed_even_when_recording_or_caching():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 9\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits_a, _ = model(idx, record_activations=True, cache_activations=False)\n",
        "    assert model.last_logits is not None\n",
        "    assert torch.allclose(model.last_logits, logits_a[:, -1, :])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits_b, _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "    assert model.last_logits is not None\n",
        "    assert torch.allclose(model.last_logits, logits_b[:, -1, :])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                             [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m12 passed\u001b[0m\u001b[32m in 2.86s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m pytest -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EXTRA COMPROBATIONS FOR SECTION 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 124.44M\n",
            "number of parameters: 124.44M\n",
            "score(clean)  = -4.124076843261719\n",
            "score(corrupt)= 5.656242370605469\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "def single_token_id(bpe, s):\n",
        "    ids = bpe(s)[0].tolist()\n",
        "    assert len(ids) == 1, (s, ids)\n",
        "    return int(ids[0])\n",
        "\n",
        "@torch.no_grad()\n",
        "def score_for(prompt, token_a=\" Jones\", token_b=\" Smith\"):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    bpe = BPETokenizer()\n",
        "    idx = bpe(prompt).to(device)\n",
        "    _ = model(idx)\n",
        "    id_a = single_token_id(bpe, token_a)\n",
        "    id_b = single_token_id(bpe, token_b)\n",
        "    return float(model.last_logits[0, id_b] - model.last_logits[0, id_a])\n",
        "\n",
        "clean = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "corrupt = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "\n",
        "s_clean = score_for(clean)\n",
        "s_corrupt = score_for(corrupt)\n",
        "\n",
        "print(\"score(clean)  =\", s_clean)\n",
        "print(\"score(corrupt)=\", s_corrupt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m  [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m71 passed\u001b[0m\u001b[32m in 24.29s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting patching_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile patching_driver.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from mingpt.model import GPT\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "@torch.no_grad()\n",
        "def main():\n",
        "    set_seed(123)\n",
        "\n",
        "    # tiny model (fast, no downloads)\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = 200\n",
        "    cfg.block_size = 32\n",
        "\n",
        "    model = GPT(cfg).eval()\n",
        "\n",
        "    T = 12\n",
        "    clean = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "    corrupt = clean.clone()\n",
        "    corrupt[0, 3] = (corrupt[0, 3] + 1) % cfg.vocab_size  # minimal corruption\n",
        "\n",
        "    # 1) cache clean activations\n",
        "    _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "    print(\"Cached clean activations:\",\n",
        "          len(model.clean_activations), \"layers x\", len(model.clean_activations[0]), \"positions\")\n",
        "\n",
        "    # 2) corrupted baseline (no patch)\n",
        "    _ = model(corrupt)\n",
        "    base_last = model.last_logits.clone()\n",
        "\n",
        "    # 3) one patched run (layer=0, pos=3)\n",
        "    _ = model(corrupt, layer_to_patch=0, position_to_patch=3)\n",
        "    patched_last = model.last_logits.clone()\n",
        "\n",
        "    print(\"Patch applied:\", model.last_patch)\n",
        "    print(\"Logits changed vs baseline? \", (not torch.allclose(base_last, patched_last)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_section_7_all.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section_7_all.py\n",
        "import pytest\n",
        "import torch\n",
        "from mingpt.model import GPT\n",
        "\n",
        "def _make_tiny():\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = 200\n",
        "    cfg.block_size = 32\n",
        "    model = GPT(cfg).eval()\n",
        "    return model, cfg\n",
        "\n",
        "def _make_clean_corrupt(cfg, T=12):\n",
        "    clean = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "    corrupt = clean.clone()\n",
        "    # change exactly one token id so activations differ\n",
        "    corrupt[0, 3] = (corrupt[0, 3] + 1) % cfg.vocab_size\n",
        "    return clean, corrupt\n",
        "\n",
        "# Section 5/6 sanity (minimal, to ensure Section 7 didn't break them)\n",
        "\n",
        "def test_last_logits_exists_and_shape():\n",
        "    model, cfg = _make_tiny()\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(idx)\n",
        "    assert model.last_logits is not None\n",
        "    assert tuple(model.last_logits.shape) == (1, cfg.vocab_size)\n",
        "    assert torch.allclose(model.last_logits, logits[:, -1, :])\n",
        "\n",
        "def test_clean_cache_written_only_when_requested():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt = _make_clean_corrupt(cfg, T=10)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    snap = [[t.clone() for t in layer] for layer in model.clean_activations]\n",
        "\n",
        "    # normal run must not mutate clean cache\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt)\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(len(snap[L])):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p])\n",
        "\n",
        "# Section 7 patching tests\n",
        "\n",
        "def test_patch_requires_existing_clean_cache():\n",
        "    model, cfg = _make_tiny()\n",
        "    _, corrupt = _make_clean_corrupt(cfg, T=10)\n",
        "\n",
        "    with pytest.raises(RuntimeError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(corrupt, layer_to_patch=0, position_to_patch=3)\n",
        "\n",
        "def test_patch_argument_pairing_rules():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt = _make_clean_corrupt(cfg, T=10)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    with pytest.raises(ValueError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(corrupt, layer_to_patch=0, position_to_patch=None)\n",
        "\n",
        "    with pytest.raises(ValueError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(corrupt, layer_to_patch=None, position_to_patch=3)\n",
        "\n",
        "def test_patch_disallows_cache_write_flags():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt = _make_clean_corrupt(cfg, T=10)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    with pytest.raises(RuntimeError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(corrupt, layer_to_patch=0, position_to_patch=3, cache_activations=True)\n",
        "\n",
        "    with pytest.raises(RuntimeError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(corrupt, layer_to_patch=0, position_to_patch=3, overwrite_cache=True)\n",
        "\n",
        "def test_patch_applies_at_exact_layer_and_position_and_only_there():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt = _make_clean_corrupt(cfg, T=12)\n",
        "\n",
        "    # Cache clean activations\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    # Corrupted baseline with recording (to compare)\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt, record_activations=True, cache_activations=False)\n",
        "    baseline_acts = model.last_activations\n",
        "    assert baseline_acts is not None\n",
        "\n",
        "    # Patched run with recording\n",
        "    L = 0\n",
        "    P = 3\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt, record_activations=True, layer_to_patch=L, position_to_patch=P)\n",
        "    patched_acts = model.last_activations\n",
        "    assert patched_acts is not None\n",
        "\n",
        "    # 1) patched location equals clean cache at that (layer, pos)\n",
        "    assert torch.allclose(\n",
        "        patched_acts[L][P],\n",
        "        model.clean_activations[L][P],\n",
        "        rtol=1e-5,\n",
        "        atol=1e-6,\n",
        "    )\n",
        "\n",
        "    # 2) baseline at that location differs from clean cache (should, because corrupt differs)\n",
        "    assert not torch.allclose(\n",
        "        baseline_acts[L][P],\n",
        "        model.clean_activations[L][P],\n",
        "        rtol=1e-5,\n",
        "        atol=1e-6,\n",
        "    )\n",
        "\n",
        "    # 3) same layer, other positions are unchanged by patch at that layer output\n",
        "    other_pos = 0 if P != 0 else 1\n",
        "    assert torch.allclose(\n",
        "        patched_acts[L][other_pos],\n",
        "        baseline_acts[L][other_pos],\n",
        "        rtol=1e-5,\n",
        "        atol=1e-6,\n",
        "    )\n",
        "\n",
        "    # 4) bookkeeping says exactly one patch\n",
        "    assert model.last_patch == (L, P)\n",
        "\n",
        "def test_patch_changes_last_logits_vs_corrupted_baseline():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt = _make_clean_corrupt(cfg, T=12)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt)\n",
        "    base_last = model.last_logits.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt, layer_to_patch=0, position_to_patch=3)\n",
        "    patched_last = model.last_logits.clone()\n",
        "\n",
        "    # Almost surely different if patch actually applied\n",
        "    assert not torch.allclose(base_last, patched_last)\n",
        "\n",
        "def test_clean_cache_not_mutated_by_patched_runs():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt = _make_clean_corrupt(cfg, T=12)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    snap = [[t.clone() for t in layer] for layer in model.clean_activations]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt, layer_to_patch=0, position_to_patch=3, record_activations=True)\n",
        "\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(len(snap[L])):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m  [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m71 passed\u001b[0m\u001b[32m in 30.49s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing baseline_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile baseline_utils.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Dict, Any\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TopKEntry:\n",
        "    rank: int\n",
        "    token_id: int\n",
        "    token_str: str\n",
        "    prob: float\n",
        "    logit: float\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class BaselineResult:\n",
        "    prompt: str\n",
        "    seq_len: int\n",
        "    topk: List[TopKEntry]\n",
        "    token_a: str\n",
        "    token_b: str\n",
        "    token_a_id: int\n",
        "    token_b_id: int\n",
        "    logit_a: float\n",
        "    logit_b: float\n",
        "    prob_a: float\n",
        "    prob_b: float\n",
        "    score_logit_diff: float  # logit(B) - logit(A)\n",
        "\n",
        "\n",
        "def get_device() -> str:\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def single_token_id(bpe, token_str: str) -> int:\n",
        "    \"\"\"\n",
        "    Convert token_str into a SINGLE BPE token id.\n",
        "    This matches the assignment warning: mid-sequence words usually need a leading space, e.g. ' Jones'.\n",
        "    \"\"\"\n",
        "    ids = bpe(token_str)[0].tolist()\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(\n",
        "            f\"Target token string must map to exactly 1 BPE token. Got {len(ids)} tokens for {repr(token_str)}: {ids}\"\n",
        "        )\n",
        "    return int(ids[0])\n",
        "\n",
        "\n",
        "def topk_from_last_logits(bpe, last_logits_1d: torch.Tensor, k: int = 20) -> List[TopKEntry]:\n",
        "    \"\"\"\n",
        "    last_logits_1d: shape (vocab_size,)\n",
        "    Returns top-k tokens by probability (softmax over logits).\n",
        "    \"\"\"\n",
        "    probs = F.softmax(last_logits_1d, dim=-1)\n",
        "    top_p, top_i = torch.topk(probs, k)\n",
        "\n",
        "    out: List[TopKEntry] = []\n",
        "    for r in range(k):\n",
        "        tid = int(top_i[r])\n",
        "        tok = bpe.decode(torch.tensor([tid], dtype=torch.long))\n",
        "        out.append(\n",
        "            TopKEntry(\n",
        "                rank=r + 1,\n",
        "                token_id=tid,\n",
        "                token_str=tok,\n",
        "                prob=float(top_p[r]),\n",
        "                logit=float(last_logits_1d[tid]),\n",
        "            )\n",
        "        )\n",
        "    return out\n",
        "\n",
        "\n",
        "def compute_logit_diff(last_logits_1d: torch.Tensor, token_b_id: int, token_a_id: int) -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Returns (logit_a, logit_b, score = logit_b - logit_a)\n",
        "    \"\"\"\n",
        "    logit_a = float(last_logits_1d[token_a_id])\n",
        "    logit_b = float(last_logits_1d[token_b_id])\n",
        "    return logit_a, logit_b, (logit_b - logit_a)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_clean_baseline(\n",
        "    model,\n",
        "    bpe,\n",
        "    clean_text: str,\n",
        "    token_a_str: str,\n",
        "    token_b_str: str,\n",
        "    *,\n",
        "    device: Optional[str] = None,\n",
        "    top_k: int = 20,\n",
        "    overwrite_cache: bool = True,\n",
        ") -> BaselineResult:\n",
        "    \"\"\"\n",
        "    CLEAN baseline:\n",
        "    - caches clean activations (for later patching)\n",
        "    - stores model.last_logits\n",
        "    - prints/returns top-k continuation distribution\n",
        "    \"\"\"\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    idx = bpe(clean_text).to(device)  # (1, T)\n",
        "    seq_len = int(idx.shape[1])\n",
        "\n",
        "    # Cache clean activations + set last_logits\n",
        "    _logits, _loss = model(idx, cache_activations=True, overwrite_cache=overwrite_cache)\n",
        "    if model.last_logits is None:\n",
        "        raise RuntimeError(\"model.last_logits was not set by forward() during clean baseline.\")\n",
        "    last = model.last_logits[0]  # (vocab,)\n",
        "\n",
        "    token_a_id = single_token_id(bpe, token_a_str)\n",
        "    token_b_id = single_token_id(bpe, token_b_str)\n",
        "    logit_a, logit_b, score = compute_logit_diff(last, token_b_id=token_b_id, token_a_id=token_a_id)\n",
        "\n",
        "    probs = F.softmax(last, dim=-1)\n",
        "    prob_a = float(probs[token_a_id])\n",
        "    prob_b = float(probs[token_b_id])\n",
        "\n",
        "    topk = topk_from_last_logits(bpe, last, k=top_k)\n",
        "\n",
        "    return BaselineResult(\n",
        "        prompt=clean_text,\n",
        "        seq_len=seq_len,\n",
        "        topk=topk,\n",
        "        token_a=token_a_str,\n",
        "        token_b=token_b_str,\n",
        "        token_a_id=token_a_id,\n",
        "        token_b_id=token_b_id,\n",
        "        logit_a=logit_a,\n",
        "        logit_b=logit_b,\n",
        "        score_logit_diff=score,\n",
        "    )\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_corrupt_baseline(\n",
        "    model,\n",
        "    bpe,\n",
        "    corrupt_text: str,\n",
        "    token_a_str: str,\n",
        "    token_b_str: str,\n",
        "    *,\n",
        "    device: Optional[str] = None,\n",
        "    top_k: int = 20,\n",
        ") -> BaselineResult:\n",
        "    \"\"\"\n",
        "    CORRUPTED baseline (NO patching):\n",
        "    - must NOT overwrite clean cache\n",
        "    - stores model.last_logits\n",
        "    - prints/returns top-k continuation distribution\n",
        "    \"\"\"\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    idx = bpe(corrupt_text).to(device)  # (1, T)\n",
        "    seq_len = int(idx.shape[1])\n",
        "\n",
        "    # No cache flags here => clean cache remains intact\n",
        "    _logits, _loss = model(idx)\n",
        "    if model.last_logits is None:\n",
        "        raise RuntimeError(\"model.last_logits was not set by forward() during corrupt baseline.\")\n",
        "    last = model.last_logits[0]  # (vocab,)\n",
        "\n",
        "    token_a_id = single_token_id(bpe, token_a_str)\n",
        "    token_b_id = single_token_id(bpe, token_b_str)\n",
        "    logit_a, logit_b, score = compute_logit_diff(last, token_b_id=token_b_id, token_a_id=token_a_id)\n",
        "\n",
        "    probs = F.softmax(last, dim=-1)\n",
        "    prob_a = float(probs[token_a_id])\n",
        "    prob_b = float(probs[token_b_id])\n",
        "\n",
        "    topk = topk_from_last_logits(bpe, last, k=top_k)\n",
        "\n",
        "    return BaselineResult(\n",
        "        prompt=clean_text,\n",
        "        seq_len=seq_len,\n",
        "        topk=topk,\n",
        "        token_a=token_a_str,\n",
        "        token_b=token_b_str,\n",
        "        token_a_id=token_a_id,\n",
        "        token_b_id=token_b_id,\n",
        "        logit_a=logit_a,\n",
        "        logit_b=logit_b,\n",
        "        prob_a=prob_a,\n",
        "        prob_b=prob_b,\n",
        "        score_logit_diff=score,\n",
        "    )\n",
        "\n",
        "\n",
        "def format_topk_table(res: BaselineResult, *, max_rows: int = 20) -> str:\n",
        "    lines = []\n",
        "    lines.append(f\"Prompt (seq_len={res.seq_len}): {res.prompt}\")\n",
        "    lines.append(\"\")\n",
        "    lines.append(f\"Metric tokens:\")\n",
        "    lines.append(f\"  Token A (clean-consistent):   {repr(res.token_a)}  id={res.token_a_id}  logit={res.logit_a:.4f}\")\n",
        "    lines.append(f\"  Token B (corrupt-consistent): {repr(res.token_b)}  id={res.token_b_id}  logit={res.logit_b:.4f}\")\n",
        "    lines.append(f\"  score = logit(B) - logit(A) = {res.score_logit_diff:.4f}\")\n",
        "    lines.append(f\"  P(Token A) = {res.prob_a:.4f}\")\n",
        "    lines.append(f\"  P(Token B) = {res.prob_b:.4f}\")\n",
        "    lines.append(\"\")\n",
        "    lines.append(f\"Top-{min(max_rows, len(res.topk))} next-token continuations (by probability):\")\n",
        "    for e in res.topk[:max_rows]:\n",
        "        lines.append(\n",
        "            f\"{e.rank:02d}. id={e.token_id:5d} tok={repr(e.token_str):>14}  prob={e.prob:.4f}  logit={e.logit:.4f}\"\n",
        "        )\n",
        "    return \"\\n\".join(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CLEAN baseline ===\n",
            "Prompt (seq_len=11): Michelle Jones was a top-notch student. Michelle\n",
            "\n",
            "Metric tokens:\n",
            "  Token A (clean-consistent):   ' Jones'  id=5437  logit=-79.6386\n",
            "  Token B (corrupt-consistent): ' Smith'  id=4176  logit=-83.7627\n",
            "  score = logit(B) - logit(A) = -4.1241\n",
            "\n",
            "Top-20 next-token continuations (by probability):\n",
            "01. id=  373 tok=        ' was'  prob=0.1634  logit=-79.4807\n",
            "02. id= 5437 tok=      ' Jones'  prob=0.1396  logit=-79.6386\n",
            "03. id=  338 tok=          \"'s\"  prob=0.0806  logit=-80.1876\n",
            "04. id=  550 tok=        ' had'  prob=0.0491  logit=-80.6838\n",
            "05. id=  318 tok=         ' is'  prob=0.0229  logit=-81.4471\n",
            "06. id=  290 tok=        ' and'  prob=0.0227  logit=-81.4569\n",
            "07. id=   11 tok=           ','  prob=0.0222  logit=-81.4781\n",
            "08. id=  531 tok=       ' said'  prob=0.0134  logit=-81.9811\n",
            "09. id=  468 tok=        ' has'  prob=0.0120  logit=-82.0913\n",
            "10. id=  635 tok=       ' also'  prob=0.0117  logit=-82.1161\n",
            "11. id= 1625 tok=       ' came'  prob=0.0091  logit=-82.3733\n",
            "12. id= 1297 tok=       ' told'  prob=0.0084  logit=-82.4499\n",
            "13. id= 1422 tok=       ' didn'  prob=0.0070  logit=-82.6320\n",
            "14. id= 2993 tok=       ' knew'  prob=0.0067  logit=-82.6713\n",
            "15. id= 1816 tok=       ' went'  prob=0.0061  logit=-82.7684\n",
            "16. id=  561 tok=      ' would'  prob=0.0061  logit=-82.7738\n",
            "17. id= 3111 tok=     ' worked'  prob=0.0055  logit=-82.8806\n",
            "18. id=  750 tok=        ' did'  prob=0.0054  logit=-82.8953\n",
            "19. id= 2486 tok=      ' Obama'  prob=0.0053  logit=-82.9094\n",
            "20. id= 2492 tok=       ' wasn'  prob=0.0050  logit=-82.9685\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "=== CORRUPTED baseline (NO patch) ===\n",
            "Prompt (seq_len=11): Michelle Smith was a top-notch student. Michelle\n",
            "\n",
            "Metric tokens:\n",
            "  Token A (clean-consistent):   ' Jones'  id=5437  logit=-88.4841\n",
            "  Token B (corrupt-consistent): ' Smith'  id=4176  logit=-82.8279\n",
            "  score = logit(B) - logit(A) = 5.6562\n",
            "\n",
            "Top-20 next-token continuations (by probability):\n",
            "01. id=  373 tok=        ' was'  prob=0.1630  logit=-82.6385\n",
            "02. id= 4176 tok=      ' Smith'  prob=0.1349  logit=-82.8279\n",
            "03. id=  338 tok=          \"'s\"  prob=0.0858  logit=-83.2802\n",
            "04. id=  550 tok=        ' had'  prob=0.0627  logit=-83.5946\n",
            "05. id=  318 tok=         ' is'  prob=0.0256  logit=-84.4906\n",
            "06. id=   11 tok=           ','  prob=0.0255  logit=-84.4927\n",
            "07. id=  290 tok=        ' and'  prob=0.0227  logit=-84.6085\n",
            "08. id=  531 tok=       ' said'  prob=0.0160  logit=-84.9604\n",
            "09. id=  468 tok=        ' has'  prob=0.0134  logit=-85.1377\n",
            "10. id= 7817 tok=     ' taught'  prob=0.0111  logit=-85.3298\n",
            "11. id=  635 tok=       ' also'  prob=0.0093  logit=-85.5020\n",
            "12. id= 3111 tok=     ' worked'  prob=0.0092  logit=-85.5162\n",
            "13. id= 1625 tok=       ' came'  prob=0.0079  logit=-85.6695\n",
            "14. id=  561 tok=      ' would'  prob=0.0074  logit=-85.7352\n",
            "15. id= 1297 tok=       ' told'  prob=0.0073  logit=-85.7472\n",
            "16. id= 2993 tok=       ' knew'  prob=0.0066  logit=-85.8450\n",
            "17. id= 1718 tok=       ' took'  prob=0.0063  logit=-85.8929\n",
            "18. id= 1422 tok=       ' didn'  prob=0.0059  logit=-85.9512\n",
            "19. id= 9141 tok=   ' attended'  prob=0.0056  logit=-86.0030\n",
            "20. id=  750 tok=        ' did'  prob=0.0051  logit=-86.0983\n",
            "\n",
            "==========================================================================================\n",
            "=== Metric sanity check (expected shift toward corrupted) ===\n",
            "clean score   = -4.1241\n",
            "corrupt score = 5.6562\n",
            "delta (corrupt - clean) = 9.7803\n"
          ]
        }
      ],
      "source": [
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "from baseline_utils import (\n",
        "    run_clean_baseline,\n",
        "    run_corrupt_baseline,\n",
        "    format_topk_table,\n",
        ")\n",
        "\n",
        "CLEAN_TEXT   = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "CORRUPT_TEXT = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "\n",
        "TOKEN_A = \" Jones\"  # clean-consistent continuation\n",
        "TOKEN_B = \" Smith\"  # corrupt-consistent continuation\n",
        "\n",
        "bpe = BPETokenizer()\n",
        "\n",
        "clean_res = run_clean_baseline(model, bpe, CLEAN_TEXT, TOKEN_A, TOKEN_B, device=device, top_k=20, overwrite_cache=True)\n",
        "corrupt_res = run_corrupt_baseline(model, bpe, CORRUPT_TEXT, TOKEN_A, TOKEN_B, device=device, top_k=20)\n",
        "\n",
        "print(\"=== CLEAN baseline ===\")\n",
        "print(format_topk_table(clean_res, max_rows=20))\n",
        "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
        "print(\"=== CORRUPTED baseline (NO patch) ===\")\n",
        "print(format_topk_table(corrupt_res, max_rows=20))\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"=== Metric sanity check (expected shift toward corrupted) ===\")\n",
        "print(f\"clean score   = {clean_res.score_logit_diff:.4f}\")\n",
        "print(f\"corrupt score = {corrupt_res.score_logit_diff:.4f}\")\n",
        "print(f\"delta (corrupt - clean) = {corrupt_res.score_logit_diff - clean_res.score_logit_diff:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_section_8.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section_8.py\n",
        "import pytest\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from mingpt.model import GPT\n",
        "\n",
        "import baseline_utils as bu\n",
        "\n",
        "\n",
        "# Dummy tokenizer (FAST tests, no downloads)\n",
        "class DummyTokenizer:\n",
        "    \"\"\"\n",
        "    Minimal stand-in for BPETokenizer that supports:\n",
        "    - __call__(text) -> tensor(1,T)\n",
        "    - decode(tensor([id])) -> string\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # ids for prompt chars\n",
        "        self.map = {\n",
        "            \"a\": 1, \"b\": 2, \"c\": 3, \"x\": 4, \"y\": 5, \"z\": 6,\n",
        "            # ids for \"tokens\" A/B (single-token strings)\n",
        "            \" A\": 7,\n",
        "            \" B\": 8,\n",
        "        }\n",
        "        self.inv = {v: k for k, v in self.map.items()}\n",
        "\n",
        "    def __call__(self, s: str):\n",
        "        if s in self.map:\n",
        "            ids = [self.map[s]]\n",
        "        else:\n",
        "            ids = [self.map[ch] for ch in s]  # tokenize per char for prompt\n",
        "        return torch.tensor([ids], dtype=torch.long)\n",
        "\n",
        "    def decode(self, t: torch.Tensor) -> str:\n",
        "        # expects shape (1,) or (n,)\n",
        "        ids = t.flatten().tolist()\n",
        "        return \"\".join(self.inv.get(int(i), f\"<{int(i)}>\") for i in ids)\n",
        "\n",
        "\n",
        "def _make_tiny_gpt(vocab_size=50, block_size=32):\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = vocab_size\n",
        "    cfg.block_size = block_size\n",
        "    return GPT(cfg).eval()\n",
        "\n",
        "\n",
        "# Section 8: baseline utils tests (FAST)\n",
        "\n",
        "def test_single_token_id_accepts_single_and_rejects_multi():\n",
        "    bpe = DummyTokenizer()\n",
        "    assert bu.single_token_id(bpe, \" A\") == 7\n",
        "    assert bu.single_token_id(bpe, \" B\") == 8\n",
        "\n",
        "    # Multi-token string under DummyTokenizer (tokenizes as chars)\n",
        "    with pytest.raises(ValueError):\n",
        "        _ = bu.single_token_id(bpe, \"ab\")\n",
        "\n",
        "\n",
        "def test_compute_logit_diff_correctness():\n",
        "    last = torch.tensor([0.0, 1.0, 2.0, -3.0], dtype=torch.float32)\n",
        "    logit_a, logit_b, score = bu.compute_logit_diff(last, token_b_id=2, token_a_id=1)\n",
        "    assert logit_a == 1.0\n",
        "    assert logit_b == 2.0\n",
        "    assert score == 1.0\n",
        "\n",
        "\n",
        "def test_topk_from_last_logits_sorted_and_probabilities_valid():\n",
        "    bpe = DummyTokenizer()\n",
        "    last = torch.tensor([0.0, 1.0, 2.0, 3.0, -1.0, -2.0, 0.5, 0.25, 0.1], dtype=torch.float32)\n",
        "    k = 5\n",
        "    topk = bu.topk_from_last_logits(bpe, last, k=k)\n",
        "\n",
        "    assert len(topk) == k\n",
        "    # probabilities in [0,1] and sorted desc\n",
        "    probs = [e.prob for e in topk]\n",
        "    assert all(0.0 <= p <= 1.0 for p in probs)\n",
        "    assert probs == sorted(probs, reverse=True)\n",
        "\n",
        "\n",
        "def test_clean_baseline_caches_activations_and_sets_last_logits():\n",
        "    model = _make_tiny_gpt(vocab_size=60, block_size=16)\n",
        "    bpe = DummyTokenizer()\n",
        "\n",
        "    clean_text = \"abc\"      # -> ids [1,2,3]\n",
        "    token_a = \" A\"          # -> id 7\n",
        "    token_b = \" B\"          # -> id 8\n",
        "\n",
        "    res = bu.run_clean_baseline(model, bpe, clean_text, token_a, token_b, device=\"cpu\", top_k=5, overwrite_cache=True)\n",
        "\n",
        "    assert model.clean_activations is not None, \"Clean baseline must cache activations.\"\n",
        "    assert model.last_logits is not None, \"Clean baseline must set last_logits.\"\n",
        "    assert isinstance(res.score_logit_diff, float)\n",
        "\n",
        "\n",
        "def test_corrupt_baseline_does_not_overwrite_clean_cache():\n",
        "    model = _make_tiny_gpt(vocab_size=60, block_size=16)\n",
        "    bpe = DummyTokenizer()\n",
        "\n",
        "    clean_text = \"abc\"\n",
        "    corrupt_text = \"abx\"  # differs in one char token id\n",
        "\n",
        "    token_a = \" A\"\n",
        "    token_b = \" B\"\n",
        "\n",
        "    _ = bu.run_clean_baseline(model, bpe, clean_text, token_a, token_b, device=\"cpu\", top_k=5, overwrite_cache=True)\n",
        "    snap = [[t.clone() for t in layer] for layer in model.clean_activations]\n",
        "\n",
        "    _ = bu.run_corrupt_baseline(model, bpe, corrupt_text, token_a, token_b, device=\"cpu\", top_k=5)\n",
        "\n",
        "    # verify cache unchanged\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(len(snap[L])):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p])\n",
        "\n",
        "\n",
        "# Optional SLOW integration test with real GPT-2 (skips if download/cache missing)\n",
        "@pytest.mark.slow\n",
        "def test_section8_metric_shifts_toward_corrupted_on_gpt2_if_available():\n",
        "    \"\"\"\n",
        "    Expected behavior for the canonical example:\n",
        "      score = logit(' Smith') - logit(' Jones')\n",
        "      clean prompt  -> score tends to be smaller (often negative)\n",
        "      corrupt prompt -> score tends to be larger (often positive)\n",
        "    We only assert corrupt_score > clean_score.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from mingpt.bpe import BPETokenizer\n",
        "        from mingpt.model import GPT\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping due to import error: {e}\")\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    try:\n",
        "        model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping GPT-2 integration (weights/tokenizer unavailable): {e}\")\n",
        "\n",
        "    CLEAN = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    CORR  = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "    A = \" Jones\"\n",
        "    B = \" Smith\"\n",
        "\n",
        "    clean_res = bu.run_clean_baseline(model, bpe, CLEAN, A, B, device=device, top_k=10, overwrite_cache=True)\n",
        "    corr_res  = bu.run_corrupt_baseline(model, bpe, CORR, A, B, device=device, top_k=10)\n",
        "\n",
        "    assert corr_res.score_logit_diff > clean_res.score_logit_diff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                   [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m6 passed\u001b[0m\u001b[32m in 16.32s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -q test_section_8.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 8 RUN SECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "number of parameters: 124.44M\n",
            "Clean cache meta: {'seq_len': 11, 'n_layer': 12, 'd_model': 768}\n",
            "\n",
            "=== CLEAN baseline ===\n",
            "Prompt (seq_len=11): Michelle Jones was a top-notch student. Michelle\n",
            "\n",
            "Metric tokens:\n",
            "  Token A (clean-consistent):   ' Jones'  id=5437  logit=-79.6386\n",
            "  Token B (corrupt-consistent): ' Smith'  id=4176  logit=-83.7627\n",
            "  score = logit(B) - logit(A) = -4.1241\n",
            "\n",
            "Top-20 next-token continuations (by probability):\n",
            "01. id=  373 tok=        ' was'  prob=0.1634  logit=-79.4807\n",
            "02. id= 5437 tok=      ' Jones'  prob=0.1396  logit=-79.6386\n",
            "03. id=  338 tok=          \"'s\"  prob=0.0806  logit=-80.1876\n",
            "04. id=  550 tok=        ' had'  prob=0.0491  logit=-80.6838\n",
            "05. id=  318 tok=         ' is'  prob=0.0229  logit=-81.4471\n",
            "06. id=  290 tok=        ' and'  prob=0.0227  logit=-81.4569\n",
            "07. id=   11 tok=           ','  prob=0.0222  logit=-81.4781\n",
            "08. id=  531 tok=       ' said'  prob=0.0134  logit=-81.9811\n",
            "09. id=  468 tok=        ' has'  prob=0.0120  logit=-82.0913\n",
            "10. id=  635 tok=       ' also'  prob=0.0117  logit=-82.1161\n",
            "11. id= 1625 tok=       ' came'  prob=0.0091  logit=-82.3733\n",
            "12. id= 1297 tok=       ' told'  prob=0.0084  logit=-82.4499\n",
            "13. id= 1422 tok=       ' didn'  prob=0.0070  logit=-82.6320\n",
            "14. id= 2993 tok=       ' knew'  prob=0.0067  logit=-82.6713\n",
            "15. id= 1816 tok=       ' went'  prob=0.0061  logit=-82.7684\n",
            "16. id=  561 tok=      ' would'  prob=0.0061  logit=-82.7738\n",
            "17. id= 3111 tok=     ' worked'  prob=0.0055  logit=-82.8806\n",
            "18. id=  750 tok=        ' did'  prob=0.0054  logit=-82.8953\n",
            "19. id= 2486 tok=      ' Obama'  prob=0.0053  logit=-82.9094\n",
            "20. id= 2492 tok=       ' wasn'  prob=0.0050  logit=-82.9685\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "=== CORRUPTED baseline (NO patch) ===\n",
            "Prompt (seq_len=11): Michelle Smith was a top-notch student. Michelle\n",
            "\n",
            "Metric tokens:\n",
            "  Token A (clean-consistent):   ' Jones'  id=5437  logit=-88.4841\n",
            "  Token B (corrupt-consistent): ' Smith'  id=4176  logit=-82.8279\n",
            "  score = logit(B) - logit(A) = 5.6562\n",
            "\n",
            "Top-20 next-token continuations (by probability):\n",
            "01. id=  373 tok=        ' was'  prob=0.1630  logit=-82.6385\n",
            "02. id= 4176 tok=      ' Smith'  prob=0.1349  logit=-82.8279\n",
            "03. id=  338 tok=          \"'s\"  prob=0.0858  logit=-83.2802\n",
            "04. id=  550 tok=        ' had'  prob=0.0627  logit=-83.5946\n",
            "05. id=  318 tok=         ' is'  prob=0.0256  logit=-84.4906\n",
            "06. id=   11 tok=           ','  prob=0.0255  logit=-84.4927\n",
            "07. id=  290 tok=        ' and'  prob=0.0227  logit=-84.6085\n",
            "08. id=  531 tok=       ' said'  prob=0.0160  logit=-84.9604\n",
            "09. id=  468 tok=        ' has'  prob=0.0134  logit=-85.1377\n",
            "10. id= 7817 tok=     ' taught'  prob=0.0111  logit=-85.3298\n",
            "11. id=  635 tok=       ' also'  prob=0.0093  logit=-85.5020\n",
            "12. id= 3111 tok=     ' worked'  prob=0.0092  logit=-85.5162\n",
            "13. id= 1625 tok=       ' came'  prob=0.0079  logit=-85.6695\n",
            "14. id=  561 tok=      ' would'  prob=0.0074  logit=-85.7352\n",
            "15. id= 1297 tok=       ' told'  prob=0.0073  logit=-85.7472\n",
            "16. id= 2993 tok=       ' knew'  prob=0.0066  logit=-85.8450\n",
            "17. id= 1718 tok=       ' took'  prob=0.0063  logit=-85.8929\n",
            "18. id= 1422 tok=       ' didn'  prob=0.0059  logit=-85.9512\n",
            "19. id= 9141 tok=   ' attended'  prob=0.0056  logit=-86.0030\n",
            "20. id=  750 tok=        ' did'  prob=0.0051  logit=-86.0983\n",
            "\n",
            "==========================================================================================\n",
            "=== Metric sanity check (expected shift toward corrupted) ===\n",
            "clean score   = -4.1241\n",
            "corrupt score = 5.6562\n",
            "delta (corrupt - clean) = 9.7803\n",
            "\n",
            " Section 8 baseline behavior looks correct.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "from baseline_utils import (\n",
        "    run_clean_baseline,\n",
        "    run_corrupt_baseline,\n",
        "    format_topk_table,\n",
        ")\n",
        "\n",
        "def main() -> None:\n",
        "    set_seed(3407)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    CLEAN_TEXT   = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    CORRUPT_TEXT = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "\n",
        "    TOKEN_A = \" Jones\"   # clean-consistent continuation\n",
        "    TOKEN_B = \" Smith\"   # corrupt-consistent continuation\n",
        "\n",
        "    clean_res = run_clean_baseline(\n",
        "        model, bpe, CLEAN_TEXT, TOKEN_A, TOKEN_B,\n",
        "        device=device, top_k=20, overwrite_cache=True\n",
        "    )\n",
        "\n",
        "    assert model.clean_activations is not None, \"Clean cache was not created.\"\n",
        "    assert model.clean_activation_meta is not None, \"Clean cache meta was not created.\"\n",
        "    print(\"Clean cache meta:\", model.clean_activation_meta)\n",
        "\n",
        "    # Snapshot cache to verify it doesn't get overwritten by corrupted baseline\n",
        "    snap = [[t.clone() for t in layer] for layer in model.clean_activations]\n",
        "\n",
        "    corrupt_res = run_corrupt_baseline(\n",
        "        model, bpe, CORRUPT_TEXT, TOKEN_A, TOKEN_B,\n",
        "        device=device, top_k=20\n",
        "    )\n",
        "\n",
        "    # Verify cache unchanged\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(len(snap[L])):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p]), \"Clean cache was mutated by corrupt baseline!\"\n",
        "\n",
        "    print(\"\\n=== CLEAN baseline ===\")\n",
        "    print(format_topk_table(clean_res, max_rows=20))\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90 + \"\\n\")\n",
        "\n",
        "    print(\"=== CORRUPTED baseline (NO patch) ===\")\n",
        "    print(format_topk_table(corrupt_res, max_rows=20))\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(\"=== Metric sanity check (expected shift toward corrupted) ===\")\n",
        "    print(f\"clean score   = {clean_res.score_logit_diff:.4f}\")\n",
        "    print(f\"corrupt score = {corrupt_res.score_logit_diff:.4f}\")\n",
        "    print(f\"delta (corrupt - clean) = {corrupt_res.score_logit_diff - clean_res.score_logit_diff:.4f}\")\n",
        "\n",
        "    assert corrupt_res.score_logit_diff > clean_res.score_logit_diff, (\n",
        "        \"Expected corrupted score to be larger than clean score for logit(B)-logit(A).\"\n",
        "    )\n",
        "    print(\"\\n Section 8 baseline behavior looks correct.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 93%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                    [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m77 passed\u001b[0m\u001b[32m in 30.64s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing patching_sweep.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile patching_sweep.py\n",
        "\"\"\"\n",
        "Full Activation Patching Sweep (Layer  Position Difference Matrix)\n",
        "\n",
        "Goal:\n",
        "- For each layer L and token position P:\n",
        "  run corrupted input with a patch at (L,P),\n",
        "  compute the scalar metric: logit(token_B) - logit(token_A)\n",
        "  from last-position logits,\n",
        "  store it in matrix[L, P].\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Sequence, Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "# Small helpers\n",
        "def _infer_device(model: torch.nn.Module) -> torch.device:\n",
        "    try:\n",
        "        return next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def single_token_id(bpe, token_str: str) -> int:\n",
        "    \"\"\"\n",
        "    Convert token_str into EXACTLY one BPE token id.\n",
        "    Raises ValueError if it tokenizes into multiple tokens.\n",
        "\n",
        "    Important: For GPT-2 BPE, mid-sequence tokens often need a leading space, e.g. \" Jones\".\n",
        "    \"\"\"\n",
        "    ids = bpe(token_str)[0].tolist()\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(\n",
        "            f\"Target token string must map to exactly 1 BPE token. \"\n",
        "            f\"Got {len(ids)} tokens for {repr(token_str)}: {ids}\"\n",
        "        )\n",
        "    return int(ids[0])\n",
        "\n",
        "\n",
        "def logit_diff_from_last_logits(last_logits_1d: torch.Tensor, *, token_a_id: int, token_b_id: int) -> float:\n",
        "    \"\"\"\n",
        "    last_logits_1d: shape (vocab_size,)\n",
        "    returns score = logit(B) - logit(A)\n",
        "    \"\"\"\n",
        "    a = float(last_logits_1d[token_a_id])\n",
        "    b = float(last_logits_1d[token_b_id])\n",
        "    return b - a\n",
        "\n",
        "\n",
        "# Outputs\n",
        "@dataclass(frozen=True)\n",
        "class SweepResult:\n",
        "    \"\"\"\n",
        "    matrix shape: (n_layers, seq_len) on CPU (float32)\n",
        "    \"\"\"\n",
        "    matrix: torch.Tensor\n",
        "    n_layers: int\n",
        "    seq_len: int\n",
        "    token_a_str: str\n",
        "    token_b_str: str\n",
        "    token_a_id: int\n",
        "    token_b_id: int\n",
        "    clean_score: float\n",
        "    corrupt_score: float\n",
        "    clean_text: str\n",
        "    corrupt_text: str\n",
        "\n",
        "\n",
        "# Core sweep (tensor-level) - best for tests\n",
        "@torch.no_grad()\n",
        "def sweep_from_ids(\n",
        "    model,\n",
        "    idx_corrupt: torch.LongTensor,\n",
        "    *,\n",
        "    token_a_id: int,\n",
        "    token_b_id: int,\n",
        "    layers: Optional[Sequence[int]] = None,\n",
        "    positions: Optional[Sequence[int]] = None,\n",
        "    progress: bool = False,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute the full patching matrix given:\n",
        "    - model already has clean activations cached (model.clean_activations != None)\n",
        "    - idx_corrupt is token ids tensor shape (1, T)\n",
        "\n",
        "    Returns:\n",
        "    - matrix: torch.Tensor on CPU, shape (n_layers, T), dtype float32\n",
        "    \"\"\"\n",
        "    if getattr(model, \"clean_activations\", None) is None:\n",
        "        raise RuntimeError(\"No clean cache found. Run a clean pass with cache_activations=True first.\")\n",
        "\n",
        "    if idx_corrupt.ndim != 2 or idx_corrupt.shape[0] != 1:\n",
        "        raise ValueError(f\"Expected idx_corrupt shape (1,T). Got {tuple(idx_corrupt.shape)}\")\n",
        "\n",
        "    device = _infer_device(model)\n",
        "    idx_corrupt = idx_corrupt.to(device)\n",
        "\n",
        "    n_layers = len(model.transformer.h)\n",
        "    T = int(idx_corrupt.shape[1])\n",
        "\n",
        "    layers = list(range(n_layers)) if layers is None else list(layers)\n",
        "    positions = list(range(T)) if positions is None else list(positions)\n",
        "\n",
        "    it = [(L, P) for L in layers for P in positions]\n",
        "    if progress:\n",
        "        try:\n",
        "            from tqdm import tqdm  # type: ignore\n",
        "            it = tqdm(it, desc=\"patching sweep\", total=len(it))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    mat = torch.empty((len(layers), len(positions)), dtype=torch.float32, device=\"cpu\")\n",
        "\n",
        "    layer_index = {L: i for i, L in enumerate(layers)}\n",
        "    pos_index = {P: j for j, P in enumerate(positions)}\n",
        "\n",
        "    for L, P in it:\n",
        "        _logits, _loss = model(idx_corrupt, layer_to_patch=int(L), position_to_patch=int(P))\n",
        "        if model.last_logits is None:\n",
        "            raise RuntimeError(\"model.last_logits was not set. Ensure forward() stores last_logits.\")\n",
        "        last = model.last_logits[0].detach()  # (vocab,)\n",
        "        score = logit_diff_from_last_logits(last, token_a_id=token_a_id, token_b_id=token_b_id)\n",
        "        mat[layer_index[L], pos_index[P]] = float(score)\n",
        "\n",
        "    return mat\n",
        "\n",
        "@torch.no_grad()\n",
        "def build_patching_sweep(\n",
        "    model,\n",
        "    bpe,\n",
        "    *,\n",
        "    clean_text: str,\n",
        "    corrupt_text: str,\n",
        "    token_a_str: str,\n",
        "    token_b_str: str,\n",
        "    overwrite_cache: bool = True,\n",
        "    progress: bool = True,\n",
        ") -> SweepResult:\n",
        "    \"\"\"\n",
        "    Full pipeline:\n",
        "    1) tokenize clean/corrupt and enforce equal seq_len\n",
        "    2) cache clean activations (clean run)\n",
        "    3) compute baseline clean score and corrupted score\n",
        "    4) sweep all (layer, position) patches on corrupted prompt\n",
        "    5) return SweepResult with matrix shape (n_layers, seq_len)\n",
        "\n",
        "    Metric:\n",
        "      score = logit(Token B) - logit(Token A), using last-position logits.\n",
        "    \"\"\"\n",
        "    device = _infer_device(model)\n",
        "\n",
        "    idx_clean = bpe(clean_text).to(device)      # (1, T)\n",
        "    idx_corrupt = bpe(corrupt_text).to(device)  # (1, T)\n",
        "\n",
        "    if idx_clean.shape != idx_corrupt.shape:\n",
        "        raise ValueError(\n",
        "            f\"Clean/Corrupt token length mismatch: clean={tuple(idx_clean.shape)}, corrupt={tuple(idx_corrupt.shape)}. \"\n",
        "            \"You MUST make both prompts have the same number of BPE tokens.\"\n",
        "        )\n",
        "\n",
        "    T = int(idx_clean.shape[1])\n",
        "    n_layers = len(model.transformer.h)\n",
        "\n",
        "    # Token ids for metric\n",
        "    token_a_id = single_token_id(bpe, token_a_str)\n",
        "    token_b_id = single_token_id(bpe, token_b_str)\n",
        "\n",
        "    # 1) CLEAN run: cache activations + baseline score\n",
        "    _logits, _loss = model(idx_clean, cache_activations=True, overwrite_cache=overwrite_cache)\n",
        "    if model.last_logits is None:\n",
        "        raise RuntimeError(\"model.last_logits missing after clean run.\")\n",
        "    clean_last = model.last_logits[0].detach()\n",
        "    clean_score = logit_diff_from_last_logits(clean_last, token_a_id=token_a_id, token_b_id=token_b_id)\n",
        "\n",
        "    # 2) CORRUPTED baseline (no patch)\n",
        "    _logits, _loss = model(idx_corrupt)\n",
        "    if model.last_logits is None:\n",
        "        raise RuntimeError(\"model.last_logits missing after corrupt baseline run.\")\n",
        "    corrupt_last = model.last_logits[0].detach()\n",
        "    corrupt_score = logit_diff_from_last_logits(corrupt_last, token_a_id=token_a_id, token_b_id=token_b_id)\n",
        "\n",
        "    # 3) FULL sweep on corrupted ids (requires clean cache)\n",
        "    matrix = sweep_from_ids(\n",
        "        model,\n",
        "        idx_corrupt,\n",
        "        token_a_id=token_a_id,\n",
        "        token_b_id=token_b_id,\n",
        "        layers=list(range(n_layers)),\n",
        "        positions=list(range(T)),\n",
        "        progress=progress,\n",
        "    )\n",
        "\n",
        "    return SweepResult(\n",
        "        matrix=matrix,\n",
        "        n_layers=n_layers,\n",
        "        seq_len=T,\n",
        "        token_a_str=token_a_str,\n",
        "        token_b_str=token_b_str,\n",
        "        token_a_id=token_a_id,\n",
        "        token_b_id=token_b_id,\n",
        "        clean_score=float(clean_score),\n",
        "        corrupt_score=float(corrupt_score),\n",
        "        clean_text=clean_text,\n",
        "        corrupt_text=corrupt_text,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting section9_sweep_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section9_sweep_driver.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "from patching_sweep import build_patching_sweep\n",
        "\n",
        "from section10_visualization import (\n",
        "    HeatmapMeta,\n",
        "    decode_prompt_token_labels,\n",
        "    plot_logit_diff_heatmap,\n",
        "    save_figure_publication_quality,\n",
        "    save_heatmap_artifacts,\n",
        ")\n",
        "\n",
        "import tokenization_protocol as tp\n",
        "\n",
        "\n",
        "def get_device() -> str:\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    set_seed(3407)\n",
        "\n",
        "    device = get_device()\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    CLEAN_TEXT = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    CORRUPT_TEXT = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "\n",
        "    # IMPORTANT: GPT-2 BPE typically needs leading spaces for mid-sequence words\n",
        "    TOKEN_A = \" Jones\"  # clean-consistent\n",
        "    TOKEN_B = \" Smith\"  # corrupt-consistent\n",
        "\n",
        "    # Load model + tokenizer\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    # Enforce the constraint: same length AND exactly one differing BPE token\n",
        "    comp = tp.validate_pair(\n",
        "        bpe=bpe,\n",
        "        clean_text=CLEAN_TEXT,\n",
        "        corrupt_text=CORRUPT_TEXT,\n",
        "        require_same_length=True,\n",
        "        require_one_token_diff=True,\n",
        "    )\n",
        "    print(tp.describe_pair(comp))\n",
        "    print(\"Changed token position:\", comp.diff_positions[0])\n",
        "\n",
        "    # Build the sweep matrix (Section 9)\n",
        "    res = build_patching_sweep(\n",
        "        model,\n",
        "        bpe,\n",
        "        clean_text=CLEAN_TEXT,\n",
        "        corrupt_text=CORRUPT_TEXT,\n",
        "        token_a_str=TOKEN_A,\n",
        "        token_b_str=TOKEN_B,\n",
        "        overwrite_cache=True,\n",
        "        progress=True,\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Baseline metric sanity ===\")\n",
        "    print(f\"clean score   = {res.clean_score:.4f}\")\n",
        "    print(f\"corrupt score = {res.corrupt_score:.4f}\")\n",
        "    print(f\"delta (corrupt - clean) = {res.corrupt_score - res.clean_score:.4f}\")\n",
        "\n",
        "    print(\"\\n=== Matrix ===\")\n",
        "    print(\"shape:\", tuple(res.matrix.shape), \"(n_layers, seq_len)\")\n",
        "    print(\"n_layers:\", res.n_layers, \"seq_len:\", res.seq_len)\n",
        "    print(\"dtype:\", res.matrix.dtype, \"device:\", res.matrix.device)\n",
        "\n",
        "    torch.save(\n",
        "        {\n",
        "            \"matrix\": res.matrix,\n",
        "            \"n_layers\": res.n_layers,\n",
        "            \"seq_len\": res.seq_len,\n",
        "            \"token_a\": res.token_a_str,\n",
        "            \"token_b\": res.token_b_str,\n",
        "            \"token_a_id\": res.token_a_id,\n",
        "            \"token_b_id\": res.token_b_id,\n",
        "            \"clean_score\": res.clean_score,\n",
        "            \"corrupt_score\": res.corrupt_score,\n",
        "            \"clean_text\": res.clean_text,\n",
        "            \"corrupt_text\": res.corrupt_text,\n",
        "        },\n",
        "        \"section9_diff_matrix.pt\",\n",
        "    )\n",
        "    print(\"\\nSaved: section9_diff_matrix.pt\")\n",
        "\n",
        "    out_dir = Path(\"artifacts/section9_and_10\")\n",
        "    token_labels = decode_prompt_token_labels(bpe, CLEAN_TEXT)\n",
        "    metric_title = f\"Logit difference heatmap: logit({repr(TOKEN_B)})  logit({repr(TOKEN_A)})\"\n",
        "\n",
        "    meta = HeatmapMeta(\n",
        "        metric_title=metric_title,\n",
        "        clean_text=CLEAN_TEXT,\n",
        "        corrupt_text=CORRUPT_TEXT,\n",
        "        token_a_str=TOKEN_A,\n",
        "        token_b_str=TOKEN_B,\n",
        "        n_layers=res.n_layers,\n",
        "        seq_len=res.seq_len,\n",
        "        token_labels=token_labels,\n",
        "    )\n",
        "\n",
        "    save_heatmap_artifacts(out_dir=out_dir, matrix=res.matrix, meta=meta)\n",
        "\n",
        "    fig, ax = plot_logit_diff_heatmap(\n",
        "        res.matrix,\n",
        "        token_labels=token_labels,\n",
        "        metric_title=metric_title,\n",
        "        show_token_strings=True,\n",
        "        center_zero=True,\n",
        "        include_pos_in_label=True,\n",
        "    )\n",
        "\n",
        "    save_figure_publication_quality(\n",
        "        fig,\n",
        "        out_basepath=out_dir / \"heatmap_logit_diff\",\n",
        "        formats=(\"png\", \"pdf\"),\n",
        "        dpi=300,\n",
        "    )\n",
        "    print(\"Saved Section 10 heatmap to:\", out_dir.resolve())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "number of parameters: 124.44M\n",
            "=== Pair summary ===\n",
            "Clean tokens:   11\n",
            "Corrupt tokens: 11\n",
            "Same length?    True\n",
            "Diff count:     1\n",
            "Diff positions: [1]\n",
            "One-token diff? True\n",
            "\n",
            "Changed token position: 1\n",
            "patching sweep: 100%|| 132/132 [00:03<00:00, 41.27it/s]\n",
            "\n",
            "=== Baseline metric sanity ===\n",
            "clean score   = -4.1241\n",
            "corrupt score = 5.6562\n",
            "delta (corrupt - clean) = 9.7803\n",
            "\n",
            "=== Matrix ===\n",
            "shape: (12, 11) (n_layers, seq_len)\n",
            "n_layers: 12 seq_len: 11\n",
            "dtype: torch.float32 device: cpu\n",
            "\n",
            "Saved: section9_diff_matrix.pt\n",
            "Saved Section 10 heatmap to: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/artifacts/section9_and_10\n"
          ]
        }
      ],
      "source": [
        "!python section9_sweep_driver.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_all_sections.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_all_sections.py\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "from mingpt.model import GPT\n",
        "\n",
        "import baseline_utils as bu\n",
        "import patching_sweep as ps\n",
        "\n",
        "\n",
        "# Dummy tokenizer (FAST, no downloads)\n",
        "class DummyTokenizer:\n",
        "    \"\"\"\n",
        "    Minimal stand-in for BPETokenizer that supports:\n",
        "    - __call__(text) -> tensor(1,T)\n",
        "    - decode(tensor([id])) -> string\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # ids for prompt chars\n",
        "        self.map = {\n",
        "            \"a\": 1, \"b\": 2, \"c\": 3, \"x\": 4, \"y\": 5, \"z\": 6,\n",
        "            # ids for \"tokens\" A/B (single-token strings)\n",
        "            \" A\": 7,\n",
        "            \" B\": 8,\n",
        "        }\n",
        "        self.inv = {v: k for k, v in self.map.items()}\n",
        "\n",
        "    def __call__(self, s: str):\n",
        "        if s in self.map:\n",
        "            ids = [self.map[s]]\n",
        "        else:\n",
        "            ids = [self.map[ch] for ch in s]  # tokenize per char for prompt\n",
        "        return torch.tensor([ids], dtype=torch.long)\n",
        "\n",
        "    def decode(self, t: torch.Tensor) -> str:\n",
        "        ids = t.flatten().tolist()\n",
        "        return \"\".join(self.inv.get(int(i), f\"<{int(i)}>\") for i in ids)\n",
        "\n",
        "\n",
        "def _make_tiny_gpt(vocab_size=80, block_size=32):\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = vocab_size\n",
        "    cfg.block_size = block_size\n",
        "    return GPT(cfg).eval()\n",
        "\n",
        "\n",
        "# Section 8 baseline utils (fast)\n",
        "def test_single_token_id_accepts_single_and_rejects_multi():\n",
        "    bpe = DummyTokenizer()\n",
        "    assert bu.single_token_id(bpe, \" A\") == 7\n",
        "    assert bu.single_token_id(bpe, \" B\") == 8\n",
        "\n",
        "    with pytest.raises(ValueError):\n",
        "        _ = bu.single_token_id(bpe, \"ab\")  # multi-token under DummyTokenizer\n",
        "\n",
        "\n",
        "def test_clean_baseline_caches_activations_and_sets_last_logits():\n",
        "    model = _make_tiny_gpt(vocab_size=60, block_size=16)\n",
        "    bpe = DummyTokenizer()\n",
        "\n",
        "    res = bu.run_clean_baseline(\n",
        "        model,\n",
        "        bpe,\n",
        "        clean_text=\"abc\",\n",
        "        token_a_str=\" A\",\n",
        "        token_b_str=\" B\",\n",
        "        device=\"cpu\",\n",
        "        top_k=5,\n",
        "        overwrite_cache=True,\n",
        "    )\n",
        "\n",
        "    assert model.clean_activations is not None\n",
        "    assert model.last_logits is not None\n",
        "    assert isinstance(res.score_logit_diff, float)\n",
        "\n",
        "\n",
        "def test_corrupt_baseline_does_not_overwrite_clean_cache():\n",
        "    model = _make_tiny_gpt(vocab_size=60, block_size=16)\n",
        "    bpe = DummyTokenizer()\n",
        "\n",
        "    _ = bu.run_clean_baseline(model, bpe, \"abc\", \" A\", \" B\", device=\"cpu\", top_k=5, overwrite_cache=True)\n",
        "    snap = [[t.clone() for t in layer] for layer in model.clean_activations]\n",
        "\n",
        "    _ = bu.run_corrupt_baseline(model, bpe, \"abx\", \" A\", \" B\", device=\"cpu\", top_k=5)\n",
        "\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(len(snap[L])):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p])\n",
        "\n",
        "\n",
        "# Section 9 sweep tests (fast, no downloads)\n",
        "def test_section9_sweep_matrix_shape_and_dtype():\n",
        "    model = _make_tiny_gpt(vocab_size=100, block_size=32)\n",
        "\n",
        "    T = 10\n",
        "    clean = torch.randint(0, 100, (1, T), dtype=torch.long)\n",
        "    corrupt = clean.clone()\n",
        "    corrupt[0, 3] = (corrupt[0, 3] + 1) % 100  # minimal corruption, same length\n",
        "\n",
        "    # cache clean activations\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    # sweep (token ids arbitrary for test)\n",
        "    mat = ps.sweep_from_ids(model, corrupt, token_a_id=1, token_b_id=2, progress=False)\n",
        "\n",
        "    assert tuple(mat.shape) == (len(model.transformer.h), T)\n",
        "    assert mat.dtype == torch.float32\n",
        "    assert mat.device.type == \"cpu\"\n",
        "    assert torch.isfinite(mat).all()\n",
        "\n",
        "\n",
        "def test_section9_sweep_does_not_mutate_clean_cache():\n",
        "    model = _make_tiny_gpt(vocab_size=100, block_size=32)\n",
        "\n",
        "    T = 8\n",
        "    clean = torch.randint(0, 100, (1, T), dtype=torch.long)\n",
        "    corrupt = clean.clone()\n",
        "    corrupt[0, 2] = (corrupt[0, 2] + 1) % 100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    snap = [[t.clone() for t in layer] for layer in model.clean_activations]\n",
        "\n",
        "    _ = ps.sweep_from_ids(model, corrupt, token_a_id=1, token_b_id=2, progress=False)\n",
        "\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(T):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p])\n",
        "\n",
        "\n",
        "def test_section9_sweep_matches_direct_single_call():\n",
        "    model = _make_tiny_gpt(vocab_size=120, block_size=32)\n",
        "\n",
        "    T = 9\n",
        "    clean = torch.randint(0, 120, (1, T), dtype=torch.long)\n",
        "    corrupt = clean.clone()\n",
        "    corrupt[0, 4] = (corrupt[0, 4] + 7) % 120\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    token_a_id, token_b_id = 5, 6\n",
        "\n",
        "    mat = ps.sweep_from_ids(model, corrupt, token_a_id=token_a_id, token_b_id=token_b_id, progress=False)\n",
        "\n",
        "    # pick one coordinate and verify equality vs direct model call\n",
        "    L, P = 0, 4\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt, layer_to_patch=L, position_to_patch=P)\n",
        "        last = model.last_logits[0].detach()\n",
        "        direct = ps.logit_diff_from_last_logits(last, token_a_id=token_a_id, token_b_id=token_b_id)\n",
        "\n",
        "    assert abs(float(mat[L, P]) - float(direct)) < 1e-6\n",
        "\n",
        "\n",
        "# Optional SLOW integration with real GPT-2 (skips if unavailable)\n",
        "@pytest.mark.slow\n",
        "def test_section9_real_gpt2_sweep_shape_if_available():\n",
        "    try:\n",
        "        from mingpt.bpe import BPETokenizer\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"BPETokenizer unavailable: {e}\")\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    try:\n",
        "        model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"GPT-2 weights/tokenizer unavailable: {e}\")\n",
        "\n",
        "    clean = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    corr  = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "    A = \" Jones\"\n",
        "    B = \" Smith\"\n",
        "\n",
        "    res = ps.build_patching_sweep(\n",
        "        model,\n",
        "        bpe,\n",
        "        clean_text=clean,\n",
        "        corrupt_text=corr,\n",
        "        token_a_str=A,\n",
        "        token_b_str=B,\n",
        "        overwrite_cache=True,\n",
        "        progress=False,\n",
        "    )\n",
        "\n",
        "    assert tuple(res.matrix.shape) == (12, res.seq_len)\n",
        "    assert torch.isfinite(res.matrix).all()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 85%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                             [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m84 passed\u001b[0m\u001b[32m in 36.96s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting section10_visualization.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section10_visualization.py\n",
        "\"\"\"\n",
        "Visualization: Heatmap Generation and Presentation Standards\n",
        "\n",
        "This module:\n",
        "- plots a (n_layers, seq_len) matrix as a heatmap using matplotlib (matshow)\n",
        "- labels axes (x: token positions or decoded token strings; y: layer indices)\n",
        "- adds colorbar + title\n",
        "- saves publication-quality figures (PNG + PDF by default)\n",
        "- optionally saves/loads matrix + metadata to/from disk for reproducibility\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass, asdict\n",
        "from pathlib import Path\n",
        "from typing import Any, List, Optional, Sequence, Tuple, Union\n",
        "\n",
        "import json\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import TwoSlopeNorm\n",
        "\n",
        "\n",
        "ArrayLike = Union[torch.Tensor, Any]  # keep flexible (torch preferred)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class HeatmapMeta:\n",
        "    metric_title: str\n",
        "    clean_text: str\n",
        "    corrupt_text: str\n",
        "    token_a_str: str\n",
        "    token_b_str: str\n",
        "    n_layers: int\n",
        "    seq_len: int\n",
        "    token_labels: Optional[List[str]] = None  # per-position decoded tokens (optional)\n",
        "\n",
        "\n",
        "def _to_2d_cpu_float(matrix: ArrayLike) -> torch.Tensor:\n",
        "    if isinstance(matrix, torch.Tensor):\n",
        "        m = matrix.detach().to(\"cpu\")\n",
        "    else:\n",
        "        m = torch.tensor(matrix)\n",
        "    if m.ndim != 2:\n",
        "        raise ValueError(f\"Expected a 2D matrix, got shape {tuple(m.shape)}\")\n",
        "    return m.to(dtype=torch.float32)\n",
        "\n",
        "\n",
        "def _tick_positions(total: int, max_ticks: int = 40, *, max_xticks: Optional[int] = None) -> List[int]:\n",
        "    \"\"\"\n",
        "    Returns a list of tick indices for a length-`total` axis.\n",
        "\n",
        "    Accepts both:\n",
        "      - max_ticks (preferred)\n",
        "      - max_xticks (backwards/alternate name)\n",
        "    \"\"\"\n",
        "    if max_xticks is not None:\n",
        "        max_ticks = int(max_xticks)\n",
        "\n",
        "    if total <= 0:\n",
        "        return []\n",
        "    if max_ticks <= 0:\n",
        "        return list(range(total))\n",
        "    if total <= max_ticks:\n",
        "        return list(range(total))\n",
        "\n",
        "    stride = int(math.ceil(total / max_ticks))\n",
        "    return list(range(0, total, stride))\n",
        "\n",
        "\n",
        "def decode_prompt_token_labels(bpe, text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Returns per-token decoded strings for the FULL prompt.\n",
        "    This is exactly what you want for x-axis labels (optional).\n",
        "    \"\"\"\n",
        "    ids_1d = bpe(text)[0].tolist()\n",
        "    labels: List[str] = []\n",
        "    for tid in ids_1d:\n",
        "        tok = bpe.decode(torch.tensor([int(tid)], dtype=torch.long))\n",
        "        labels.append(tok)\n",
        "    return labels\n",
        "\n",
        "\n",
        "def plot_logit_diff_heatmap(\n",
        "    matrix: ArrayLike,\n",
        "    *,\n",
        "    token_labels: Optional[Sequence[str]] = None,\n",
        "    metric_title: str = \"Logit difference heatmap\",\n",
        "    xlabel: str = \"Token position\",\n",
        "    ylabel: str = \"Layer\",\n",
        "    show_token_strings: bool = True,\n",
        "    max_xticks: int = 40,\n",
        "    max_token_label_len: int = 18,\n",
        "    include_pos_in_label: bool = True,\n",
        "    center_zero: bool = True,\n",
        "    vmin: Optional[float] = None,\n",
        "    vmax: Optional[float] = None,\n",
        "    figsize: Optional[Tuple[float, float]] = None,\n",
        ") -> Tuple[\"plt.Figure\", \"plt.Axes\"]:\n",
        "    \"\"\"\n",
        "    Plots the matrix using matshow.\n",
        "\n",
        "    Expected matrix shape: (n_layers, seq_len)\n",
        "      - y-axis: layer index 0..n_layers-1\n",
        "      - x-axis: token positions 0..seq_len-1\n",
        "    \"\"\"\n",
        "    m = _to_2d_cpu_float(matrix)\n",
        "    n_layers, seq_len = int(m.shape[0]), int(m.shape[1])\n",
        "\n",
        "    if figsize is None:\n",
        "        w = max(7.5, min(16.0, 0.35 * seq_len + 5.0))\n",
        "        h = max(5.0, min(10.0, 0.28 * n_layers + 4.0))\n",
        "        figsize = (w, h)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    norm = None\n",
        "    if center_zero:\n",
        "        data_min = float(m.min()) if vmin is None else float(vmin)\n",
        "        data_max = float(m.max()) if vmax is None else float(vmax)\n",
        "\n",
        "        # If data doesn't straddle 0, make a symmetric range around 0\n",
        "        max_abs = max(abs(data_min), abs(data_max))\n",
        "        if max_abs == 0.0:\n",
        "            max_abs = 1.0  # avoid degenerate norm\n",
        "\n",
        "        _vmin, _vmax = -max_abs, max_abs\n",
        "        norm = TwoSlopeNorm(vcenter=0.0, vmin=_vmin, vmax=_vmax)\n",
        "\n",
        "    im = ax.matshow(m.numpy(), norm=norm, vmin=None if norm else vmin, vmax=None if norm else vmax)\n",
        "\n",
        "    cbar = fig.colorbar(im, ax=ax)\n",
        "    cbar.set_label(\"logit(Token B)  logit(Token A)\", rotation=90)\n",
        "\n",
        "    ax.set_title(metric_title, pad=18)\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "\n",
        "    xt = _tick_positions(seq_len, max_ticks=max_xticks)\n",
        "    yt = list(range(n_layers))\n",
        "\n",
        "    ax.set_xticks(xt)\n",
        "    ax.set_yticks(yt)\n",
        "    ax.set_yticklabels([str(i) for i in yt])\n",
        "\n",
        "    if show_token_strings and token_labels is not None and len(token_labels) == seq_len:\n",
        "        labels_out: List[str] = []\n",
        "        for i in xt:\n",
        "            tok = str(token_labels[i])\n",
        "            tok_short = tok if len(tok) <= max_token_label_len else (tok[: max_token_label_len - 3] + \"...\")\n",
        "            labels_out.append(f\"{i}:{tok_short}\" if include_pos_in_label else tok_short)\n",
        "        ax.set_xticklabels(labels_out, rotation=90)\n",
        "    else:\n",
        "        ax.set_xticklabels([str(i) for i in xt], rotation=0)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def save_figure_publication_quality(\n",
        "    fig: \"plt.Figure\",\n",
        "    *,\n",
        "    out_basepath: Union[str, Path],\n",
        "    formats: Sequence[str] = (\"png\", \"pdf\"),\n",
        "    dpi: int = 300,\n",
        "    transparent: bool = False,\n",
        "    close: bool = True,\n",
        ") -> List[Path]:\n",
        "    \"\"\"\n",
        "    Saves the figure to out_basepath.{fmt} for each fmt.\n",
        "    - PNG: high dpi raster\n",
        "    - PDF: vector-friendly for reports\n",
        "    \"\"\"\n",
        "    out_base = Path(out_basepath)\n",
        "    out_base.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    saved: List[Path] = []\n",
        "    for fmt in formats:\n",
        "        p = out_base.with_suffix(\".\" + fmt.lower())\n",
        "        fig.savefig(p, dpi=dpi, bbox_inches=\"tight\", transparent=transparent)\n",
        "        saved.append(p)\n",
        "\n",
        "    if close:\n",
        "        plt.close(fig)\n",
        "\n",
        "    return saved\n",
        "\n",
        "\n",
        "def save_heatmap_artifacts(\n",
        "    *,\n",
        "    out_dir: Union[str, Path],\n",
        "    matrix: ArrayLike,\n",
        "    meta: HeatmapMeta,\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Saves:\n",
        "      - matrix.pt  (torch tensor, CPU float32)\n",
        "      - meta.json  (json)\n",
        "    \"\"\"\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    m = _to_2d_cpu_float(matrix)\n",
        "    torch.save(m, out_dir / \"matrix.pt\")\n",
        "\n",
        "    with (out_dir / \"meta.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(asdict(meta), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return out_dir\n",
        "\n",
        "\n",
        "def load_heatmap_artifacts(out_dir: Union[str, Path]) -> Tuple[torch.Tensor, HeatmapMeta]:\n",
        "    out_dir = Path(out_dir)\n",
        "    m = torch.load(out_dir / \"matrix.pt\", map_location=\"cpu\")\n",
        "\n",
        "    with (out_dir / \"meta.json\").open(\"r\", encoding=\"utf-8\") as f:\n",
        "        d = json.load(f)\n",
        "\n",
        "    meta = HeatmapMeta(**d)\n",
        "    return m, meta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting section10_visualize_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section10_visualize_driver.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import TwoSlopeNorm\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "from baseline_utils import run_clean_baseline, run_corrupt_baseline\n",
        "\n",
        "from section10_visualization import (\n",
        "    HeatmapMeta,\n",
        "    decode_prompt_token_labels,\n",
        "    plot_logit_diff_heatmap,\n",
        "    save_figure_publication_quality,\n",
        "    save_heatmap_artifacts,\n",
        ")\n",
        "\n",
        "DEFAULT_SAVED = \"section9_diff_matrix.pt\"\n",
        "\n",
        "\n",
        "def get_device() -> str:\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--out_dir\", type=str, required=True)\n",
        "    p.add_argument(\"--saved\", type=str, default=DEFAULT_SAVED, help=\"Path to section9 saved matrix (.pt)\")\n",
        "    p.add_argument(\"--show_token_strs\", action=\"store_true\", help=\"Label x-axis with decoded token strings\")\n",
        "    p.add_argument(\"--also_delta\", action=\"store_true\", help=\"Also save delta heatmap: score(L,P) - corrupt_score\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def _safe_token_label(tok: str, max_len: int = 18) -> str:\n",
        "    \"\"\"\n",
        "    Make token strings readable in tick labels:\n",
        "    - show leading space explicitly as ''\n",
        "    - escape newlines\n",
        "    - truncate long labels\n",
        "    \"\"\"\n",
        "    tok = tok.replace(\"\\n\", \"\\\\n\")\n",
        "    if tok.startswith(\" \"):\n",
        "        tok = \"\" + tok[1:]\n",
        "    if len(tok) > max_len:\n",
        "        tok = tok[: max_len - 3] + \"...\"\n",
        "    return tok\n",
        "\n",
        "\n",
        "def _load_saved_matrix(path: Path) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Loads either:\n",
        "      - dict with 'matrix' + metadata\n",
        "      - raw tensor\n",
        "    Returns (matrix_float32_cpu, meta_dict)\n",
        "    \"\"\"\n",
        "    obj = torch.load(str(path), map_location=\"cpu\")\n",
        "    meta: Dict[str, Any] = {}\n",
        "\n",
        "    if isinstance(obj, dict) and \"matrix\" in obj:\n",
        "        matrix = obj[\"matrix\"]\n",
        "        meta = dict(obj)\n",
        "    elif torch.is_tensor(obj):\n",
        "        matrix = obj\n",
        "        meta = {}\n",
        "    else:\n",
        "        raise TypeError(f\"Unexpected save format in {path}: {type(obj)}\")\n",
        "\n",
        "    if not torch.is_tensor(matrix) or matrix.ndim != 2:\n",
        "        raise ValueError(f\"Expected a 2D tensor under key 'matrix'. Got: {type(matrix)} shape={getattr(matrix, 'shape', None)}\")\n",
        "\n",
        "    return matrix.detach().to(dtype=torch.float32, device=\"cpu\"), meta\n",
        "\n",
        "\n",
        "def _tick_positions(total: int, max_ticks: int = 40) -> list[int]:\n",
        "    if total <= 0:\n",
        "        return []\n",
        "    if total <= max_ticks:\n",
        "        return list(range(total))\n",
        "    stride = int((total + max_ticks - 1) // max_ticks)\n",
        "    return list(range(0, total, stride))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    out_dir = Path(args.out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    set_seed(3407)\n",
        "    device = get_device()\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    saved_path = Path(args.saved)\n",
        "    if not saved_path.exists():\n",
        "        raise FileNotFoundError(f\"Saved matrix not found: {saved_path.resolve()}\")\n",
        "\n",
        "    matrix, saved_meta = _load_saved_matrix(saved_path)\n",
        "    n_layers, T = int(matrix.shape[0]), int(matrix.shape[1])\n",
        "    print(f\"Loaded {saved_path} with matrix shape {tuple(matrix.shape)}\")\n",
        "\n",
        "    # Defaults (used if file doesn't contain metadata)\n",
        "    CLEAN_TEXT = str(saved_meta.get(\"clean_text\", \"Michelle Jones was a top-notch student. Michelle\"))\n",
        "    CORRUPT_TEXT = str(saved_meta.get(\"corrupt_text\", \"Michelle Smith was a top-notch student. Michelle\"))\n",
        "    TOKEN_A = str(saved_meta.get(\"token_a\", \" Jones\"))\n",
        "    TOKEN_B = str(saved_meta.get(\"token_b\", \" Smith\"))\n",
        "\n",
        "    saved_clean_score = saved_meta.get(\"clean_score\", float(\"nan\"))\n",
        "    saved_corrupt_score = saved_meta.get(\"corrupt_score\", float(\"nan\"))\n",
        "\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    clean_res = run_clean_baseline(\n",
        "        model,\n",
        "        bpe,\n",
        "        clean_text=CLEAN_TEXT,\n",
        "        token_a_str=TOKEN_A,\n",
        "        token_b_str=TOKEN_B,\n",
        "        device=device,\n",
        "        top_k=20,\n",
        "        overwrite_cache=True,\n",
        "    )\n",
        "    corrupt_res = run_corrupt_baseline(\n",
        "        model,\n",
        "        bpe,\n",
        "        corrupt_text=CORRUPT_TEXT,\n",
        "        token_a_str=TOKEN_A,\n",
        "        token_b_str=TOKEN_B,\n",
        "        device=device,\n",
        "        top_k=20,\n",
        "    )\n",
        "\n",
        "    clean_score = float(clean_res.score_logit_diff)\n",
        "    corrupt_score = float(corrupt_res.score_logit_diff)\n",
        "\n",
        "    print(\"\\nBaselines (for report):\")\n",
        "    print(f\"clean_score   = {clean_score:.4f}\")\n",
        "    print(f\"corrupt_score = {corrupt_score:.4f}\")\n",
        "    print(f\"delta (corrupt-clean) = {corrupt_score - clean_score:.4f}\")\n",
        "\n",
        "    if isinstance(saved_meta, dict) and (not (saved_clean_score != saved_clean_score) or not (saved_corrupt_score != saved_corrupt_score)):\n",
        "        # Note: NaN check via (x != x)\n",
        "        print(\"\\nSaved baselines (from file, if present):\")\n",
        "        try:\n",
        "            print(f\"saved_clean_score   = {float(saved_clean_score):.4f}\")\n",
        "            print(f\"saved_corrupt_score = {float(saved_corrupt_score):.4f}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Optional: x-axis token labels\n",
        "    token_labels: Optional[list[str]] = None\n",
        "    if args.show_token_strs:\n",
        "        labels_raw = decode_prompt_token_labels(bpe, CORRUPT_TEXT)  # labels should match the prompt used for the matrix\n",
        "        if len(labels_raw) == T:\n",
        "            token_labels = [_safe_token_label(s, max_len=18) for s in labels_raw]\n",
        "        else:\n",
        "            # fallback: still show positions if mismatch\n",
        "            token_labels = None\n",
        "\n",
        "    metric_title = f\"Logit difference heatmap: logit({repr(TOKEN_B)})  logit({repr(TOKEN_A)})\\nclean={clean_score:.3f}   corrupt={corrupt_score:.3f}\"\n",
        "\n",
        "    meta = HeatmapMeta(\n",
        "        metric_title=metric_title,\n",
        "        clean_text=CLEAN_TEXT,\n",
        "        corrupt_text=CORRUPT_TEXT,\n",
        "        token_a_str=TOKEN_A,\n",
        "        token_b_str=TOKEN_B,\n",
        "        n_layers=n_layers,\n",
        "        seq_len=T,\n",
        "        token_labels=token_labels,\n",
        "    )\n",
        "\n",
        "    # Save matrix + metadata for reproducibility\n",
        "    save_heatmap_artifacts(out_dir=out_dir, matrix=matrix, meta=meta)\n",
        "\n",
        "    fig, ax = plot_logit_diff_heatmap(\n",
        "        matrix,\n",
        "        token_labels=token_labels,\n",
        "        metric_title=metric_title,\n",
        "        show_token_strings=bool(args.show_token_strs),\n",
        "        max_xticks=40,\n",
        "        center_zero=True,\n",
        "        include_pos_in_label=True,\n",
        "    )\n",
        "\n",
        "    fig_base = out_dir / \"heatmap_logit_diff\"\n",
        "    saved = save_figure_publication_quality(fig, out_basepath=fig_base, formats=(\"png\", \"pdf\"), dpi=300)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Delta heatmap (patched - corrupt) for interpretability/debug\n",
        "    if args.also_delta:\n",
        "        delta = matrix - float(corrupt_score)\n",
        "        max_abs = float(delta.abs().max())\n",
        "        norm = TwoSlopeNorm(vcenter=0.0, vmin=-max_abs, vmax=max_abs) if max_abs > 0 else None\n",
        "\n",
        "        fig2 = plt.figure(figsize=(max(10, 0.8 * T), 6))\n",
        "        ax2 = plt.gca()\n",
        "        im2 = ax2.matshow(delta.cpu().numpy(), norm=norm, aspect=\"auto\")\n",
        "        plt.colorbar(im2, label=\" = score(L,P)  corrupt_score\")\n",
        "\n",
        "        ax2.set_title(\"Delta heatmap: (patched score  corrupt_score)\", pad=18)\n",
        "        ax2.set_xlabel(\"Token position\")\n",
        "        ax2.set_ylabel(\"Layer (0=first, 11=last)\")\n",
        "\n",
        "        xt = _tick_positions(T, max_ticks=40)\n",
        "        ax2.set_xticks(xt)\n",
        "        if args.show_token_strs and (token_labels is not None) and len(token_labels) == T:\n",
        "            ax2.set_xticklabels([f\"{i}:{token_labels[i]}\" for i in xt], rotation=90, fontsize=8)\n",
        "        else:\n",
        "            ax2.set_xticklabels([str(i) for i in xt])\n",
        "\n",
        "        ax2.set_yticks(list(range(n_layers)))\n",
        "        ax2.set_yticklabels([str(i) for i in range(n_layers)])\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        fig2_base = out_dir / \"heatmap_delta\"\n",
        "        saved_delta = save_figure_publication_quality(fig2, out_basepath=fig2_base, formats=(\"png\", \"pdf\"), dpi=300)\n",
        "        plt.close(fig2)\n",
        "        saved = list(saved) + list(saved_delta)\n",
        "\n",
        "    print(\"\\nSaved figures:\")\n",
        "    for p in saved:\n",
        "        print(\" -\", Path(p).resolve())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test_section_10_all.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section_10_all.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "# Make matplotlib safe in headless pytest runs\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "from section10_visualization import (\n",
        "    plot_logit_diff_heatmap,\n",
        "    save_figure_publication_quality,\n",
        ")\n",
        "\n",
        "\n",
        "def test_section10_plot_returns_figure_and_axes_and_image():\n",
        "    # shape matches assignment: 12 layers x 11 tokens\n",
        "    m = torch.randn(12, 11)\n",
        "    token_labels = [f\" tok{i}\" for i in range(11)]\n",
        "\n",
        "    fig, ax = plot_logit_diff_heatmap(\n",
        "        m,\n",
        "        token_labels=token_labels,\n",
        "        metric_title=\"Logit difference heatmap: logit(' B')  logit(' A')\",\n",
        "        show_token_strings=True,\n",
        "        max_xticks=40,\n",
        "        center_zero=True,\n",
        "    )\n",
        "\n",
        "    # should have at least one image in axis\n",
        "    assert len(ax.images) == 1, \"Expected a single heatmap image (matshow/imshow).\"\n",
        "\n",
        "    # should have colorbar axis as well (figure axes count >= 2)\n",
        "    assert len(fig.axes) >= 2, \"Expected a colorbar to be added.\"\n",
        "\n",
        "    # title should be set\n",
        "    assert \"Logit difference\" in ax.get_title()\n",
        "\n",
        "    # x/y labels should be set\n",
        "    assert ax.get_xlabel() != \"\"\n",
        "    assert ax.get_ylabel() != \"\"\n",
        "\n",
        "\n",
        "def test_section10_save_publication_quality_creates_png_and_pdf(tmp_path: Path):\n",
        "    m = torch.randn(12, 15)\n",
        "    token_labels = [f\" tok{i}\" for i in range(15)]\n",
        "\n",
        "    fig, ax = plot_logit_diff_heatmap(\n",
        "        m,\n",
        "        token_labels=token_labels,\n",
        "        metric_title=\"Metric title\",\n",
        "        show_token_strings=True,\n",
        "        max_xticks=40,\n",
        "        center_zero=True,\n",
        "    )\n",
        "\n",
        "    out_base = tmp_path / \"heatmap_test\"\n",
        "    saved = save_figure_publication_quality(fig, out_basepath=out_base, formats=(\"png\", \"pdf\"), dpi=300)\n",
        "\n",
        "    assert len(saved) == 2\n",
        "    for p in saved:\n",
        "        assert p.exists(), f\"Expected saved file to exist: {p}\"\n",
        "        assert p.stat().st_size > 0, f\"Expected saved file to be non-empty: {p}\"\n",
        "\n",
        "\n",
        "def test_section10_token_label_fallback_to_numeric_when_disabled():\n",
        "    m = torch.randn(12, 60)\n",
        "    token_labels = [f\" tok{i}\" for i in range(60)]\n",
        "\n",
        "    fig, ax = plot_logit_diff_heatmap(\n",
        "        m,\n",
        "        token_labels=token_labels,\n",
        "        metric_title=\"Metric title\",\n",
        "        show_token_strings=False,   # forces numeric labels\n",
        "        max_xticks=12,              # reduce ticks\n",
        "        center_zero=True,\n",
        "    )\n",
        "\n",
        "    # xtick labels should be numeric strings\n",
        "    texts = [t.get_text() for t in ax.get_xticklabels()]\n",
        "    # some may be empty depending on backend/layout; keep it robust:\n",
        "    nonempty = [x for x in texts if x.strip() != \"\"]\n",
        "    assert len(nonempty) > 0\n",
        "    assert all(s.replace(\"-\", \"\").isdigit() for s in nonempty), \"Expected numeric x-tick labels in fallback mode.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                      [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 6.79s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -q test_section_10_all.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m674.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (114 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
            "Collecting pillow>=8 (from matplotlib)\n",
            "  Downloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
            "Collecting pyparsing>=3 (from matplotlib)\n",
            "  Downloading pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hUsing cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
            "Downloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m121.8/121.8 kB\u001b[0m \u001b[31m794.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pillow-12.1.0 pyparsing-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Loaded section9_diff_matrix.pt with matrix shape (12, 11)\n",
            "number of parameters: 124.44M\n",
            "\n",
            "Baselines (for report):\n",
            "clean_score   = -4.1241\n",
            "corrupt_score = 5.6562\n",
            "delta (corrupt-clean) = 9.7803\n",
            "\n",
            "Saved baselines (from file, if present):\n",
            "saved_clean_score   = -4.1241\n",
            "saved_corrupt_score = 5.6562\n",
            "\n",
            "Saved figures:\n",
            " - /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/artifacts/section10/heatmap_logit_diff.png\n",
            " - /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/artifacts/section10/heatmap_logit_diff.pdf\n"
          ]
        }
      ],
      "source": [
        "!python section10_visualize_driver.py --out_dir artifacts/section10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Loaded section9_diff_matrix.pt (dict) with matrix shape (12, 11)\n",
            "Saved clean_score=-4.1241, corrupt_score=5.6562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 124.44M\n",
            "\n",
            "[Check 1] Baselines recomputed:\n",
            "clean_score   = -4.1241\n",
            "corrupt_score = 5.6562\n",
            "delta (corrupt-clean) = 9.7803\n",
            "\n",
            "[Check 5] Matrix cell == direct recomputation (sampled):\n",
            "(L=0, P=0) stored=5.656242 direct=5.656242 abs_diff=0.00e+00\n",
            "(L=0, P=1) stored=-4.069115 direct=-4.069115 abs_diff=0.00e+00\n",
            "(L=6, P=1) stored=-3.150902 direct=-3.150902 abs_diff=0.00e+00\n",
            "(L=6, P=10) stored=4.950867 direct=4.950867 abs_diff=0.00e+00\n",
            "(L=11, P=1) stored=5.656242 direct=5.656242 abs_diff=0.00e+00\n",
            "\n",
            "[Check 6] Delta stats:\n",
            "delta min=-9.7803, max=0.0342, max_abs=9.7803\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import TwoSlopeNorm\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "from patching_sweep import logit_diff_from_last_logits, single_token_id\n",
        "\n",
        "CLEAN_TEXT   = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "CORRUPT_TEXT = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "TOKEN_A = \" Jones\"\n",
        "TOKEN_B = \" Smith\"\n",
        "\n",
        "SAVED = \"section9_diff_matrix.pt\"\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_baselines(model, bpe, device):\n",
        "    idx_clean = bpe(CLEAN_TEXT).to(device)\n",
        "    idx_corr  = bpe(CORRUPT_TEXT).to(device)\n",
        "\n",
        "    a_id = single_token_id(bpe, TOKEN_A)\n",
        "    b_id = single_token_id(bpe, TOKEN_B)\n",
        "\n",
        "    _ = model(idx_clean, cache_activations=True, overwrite_cache=True)\n",
        "    clean_score = logit_diff_from_last_logits(\n",
        "        model.last_logits[0], token_a_id=a_id, token_b_id=b_id\n",
        "    )\n",
        "\n",
        "    _ = model(idx_corr)\n",
        "    corr_score = logit_diff_from_last_logits(\n",
        "        model.last_logits[0], token_a_id=a_id, token_b_id=b_id\n",
        "    )\n",
        "\n",
        "    return clean_score, corr_score, a_id, b_id, idx_corr\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main():\n",
        "    device = get_device()\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # Load matrix if available; otherwise we'll compute a small sample check only\n",
        "    matrix = None\n",
        "    saved_clean_score = float(\"nan\")\n",
        "    saved_corr_score = float(\"nan\")\n",
        "\n",
        "    if os.path.exists(SAVED):\n",
        "        d = torch.load(SAVED, map_location=\"cpu\")\n",
        "\n",
        "        # Robust load: handle dict-with-matrix OR tensor-only saves\n",
        "        if isinstance(d, dict) and \"matrix\" in d:\n",
        "            matrix = d[\"matrix\"].to(torch.float32)\n",
        "            saved_clean_score = float(d.get(\"clean_score\", float(\"nan\")))\n",
        "            saved_corr_score  = float(d.get(\"corrupt_score\", float(\"nan\")))\n",
        "            print(f\"Loaded {SAVED} (dict) with matrix shape {tuple(matrix.shape)}\")\n",
        "            if not (torch.isnan(torch.tensor(saved_clean_score)) or torch.isnan(torch.tensor(saved_corr_score))):\n",
        "                print(f\"Saved clean_score={saved_clean_score:.4f}, corrupt_score={saved_corr_score:.4f}\")\n",
        "            else:\n",
        "                print(\"Saved baseline scores not found in file (clean_score/corrupt_score missing).\")\n",
        "\n",
        "        elif torch.is_tensor(d):\n",
        "            matrix = d.to(torch.float32)\n",
        "            print(f\"Loaded {SAVED} (tensor) with shape {tuple(matrix.shape)}\")\n",
        "            print(\"Note: baseline scores not present because file contains only the tensor.\")\n",
        "\n",
        "        else:\n",
        "            raise TypeError(f\"Unexpected save format in {SAVED}: {type(d)}\")\n",
        "\n",
        "    # Load model/tokenizer\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    # Check 1: baselines\n",
        "    clean_score, corr_score, a_id, b_id, idx_corr = compute_baselines(model, bpe, device)\n",
        "    print(\"\\n[Check 1] Baselines recomputed:\")\n",
        "    print(f\"clean_score   = {clean_score:.4f}\")\n",
        "    print(f\"corrupt_score = {corr_score:.4f}\")\n",
        "    print(f\"delta (corrupt-clean) = {corr_score - clean_score:.4f}\")\n",
        "\n",
        "    if matrix is not None:\n",
        "        # Check 5: sample a few cells and compare to direct recomputation\n",
        "        n_layers, T = matrix.shape\n",
        "        samples = [(0, 0), (0, 1), (6, 1), (6, T - 1), (n_layers - 1, 1)]\n",
        "        print(\"\\n[Check 5] Matrix cell == direct recomputation (sampled):\")\n",
        "        for (L, P) in samples:\n",
        "            _ = model(idx_corr, layer_to_patch=int(L), position_to_patch=int(P))\n",
        "            direct = logit_diff_from_last_logits(\n",
        "                model.last_logits[0], token_a_id=a_id, token_b_id=b_id\n",
        "            )\n",
        "            stored = float(matrix[int(L), int(P)])\n",
        "            print(f\"(L={L}, P={P}) stored={stored:.6f} direct={direct:.6f} abs_diff={abs(stored-direct):.2e}\")\n",
        "\n",
        "        # Check 6: delta heatmap\n",
        "        delta = matrix - float(corr_score)\n",
        "        max_abs = float(delta.abs().max())\n",
        "        print(\"\\n[Check 6] Delta stats:\")\n",
        "        print(f\"delta min={float(delta.min()):.4f}, max={float(delta.max()):.4f}, max_abs={max_abs:.4f}\")\n",
        "\n",
        "        norm = TwoSlopeNorm(vcenter=0.0, vmin=-max_abs, vmax=max_abs)\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.title(\"DELTA heatmap: (patched score - corrupt_score)\")\n",
        "        plt.imshow(delta.cpu().numpy(), norm=norm, aspect=\"auto\")\n",
        "        plt.colorbar(label=\" = score(L,P) - corrupt_score\")\n",
        "        plt.xlabel(\"Token position\")\n",
        "        plt.ylabel(\"Layer\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"\\nNo saved matrix found. If you want full verification, run section9_sweep_driver.py first to create it.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 82%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                          [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m87 passed\u001b[0m\u001b[32m in 46.67s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing wrong_source_control.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile wrong_source_control.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Dict\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Condition:\n",
        "    name: str\n",
        "    target: Tuple[int, int]          # (L_target, P_target)\n",
        "    source: Optional[Tuple[int, int]] # None => no patch; else (L_source, P_source)\n",
        "\n",
        "\n",
        "def single_token_id(bpe, token_str: str) -> int:\n",
        "    ids = bpe(token_str)[0].tolist()\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{repr(token_str)} is not a single BPE token. Got {len(ids)} ids: {ids}\")\n",
        "    return int(ids[0])\n",
        "\n",
        "\n",
        "def score_from_last_logits(last_logits_1d: torch.Tensor, token_a_id: int, token_b_id: int) -> float:\n",
        "    # score = logit(B) - logit(A)\n",
        "    return float(last_logits_1d[token_b_id] - last_logits_1d[token_a_id])\n",
        "\n",
        "\n",
        "def normalized_restoration(score_patched: float, score_clean: float, score_corr: float) -> float:\n",
        "    denom = (score_clean - score_corr)\n",
        "    if abs(denom) < 1e-12:\n",
        "        return float(\"nan\")\n",
        "    return (score_patched - score_corr) / denom\n",
        "\n",
        "\n",
        "def conditions_for_target(L: int, P: int, n_layers: int, seq_len: int) -> List[Condition]:\n",
        "    \"\"\"\n",
        "    Returns the 5-condition set (baseline + match + wrong-source variants where valid)\n",
        "    while keeping the patch TARGET fixed at (L,P).\n",
        "    \"\"\"\n",
        "    conds: List[Condition] = []\n",
        "    conds.append(Condition(\"no_patch\", (L, P), None))\n",
        "    conds.append(Condition(\"match\", (L, P), (L, P)))\n",
        "\n",
        "    # WS-pos +/- (same layer, neighbor token)\n",
        "    if P + 1 < seq_len:\n",
        "        conds.append(Condition(\"WS-pos+\", (L, P), (L, P + 1)))\n",
        "    if P - 1 >= 0:\n",
        "        conds.append(Condition(\"WS-pos-\", (L, P), (L, P - 1)))\n",
        "\n",
        "    # WS-layer +/- (same position, neighbor layer)\n",
        "    if L + 1 < n_layers:\n",
        "        conds.append(Condition(\"WS-layer+\", (L, P), (L + 1, P)))\n",
        "    if L - 1 >= 0:\n",
        "        conds.append(Condition(\"WS-layer-\", (L, P), (L - 1, P)))\n",
        "\n",
        "    return conds\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_condition(\n",
        "    model,\n",
        "    idx_corr: torch.Tensor,\n",
        "    cond: Condition,\n",
        "    token_a_id: int,\n",
        "    token_b_id: int,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Runs ONE condition and returns score + bookkeeping.\n",
        "    \"\"\"\n",
        "    if cond.source is None:\n",
        "        _ = model(idx_corr)\n",
        "        score = score_from_last_logits(model.last_logits[0], token_a_id, token_b_id)\n",
        "        return {\n",
        "            \"score\": score,\n",
        "            \"patched\": 0.0,\n",
        "            \"L_target\": float(cond.target[0]),\n",
        "            \"P_target\": float(cond.target[1]),\n",
        "            \"L_source\": float(\"nan\"),\n",
        "            \"P_source\": float(\"nan\"),\n",
        "        }\n",
        "\n",
        "    (Lt, Pt) = cond.target\n",
        "    (Ls, Ps) = cond.source\n",
        "    _ = model(\n",
        "        idx_corr,\n",
        "        layer_to_patch=Lt,\n",
        "        position_to_patch=Pt,\n",
        "        source_layer=Ls,\n",
        "        source_position=Ps,\n",
        "    )\n",
        "    score = score_from_last_logits(model.last_logits[0], token_a_id, token_b_id)\n",
        "    return {\n",
        "        \"score\": score,\n",
        "        \"patched\": 1.0,\n",
        "        \"L_target\": float(Lt),\n",
        "        \"P_target\": float(Pt),\n",
        "        \"L_source\": float(Ls),\n",
        "        \"P_source\": float(Ps),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing extra1_wrong_source_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile extra1_wrong_source_driver.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "import wrong_source_control as wsc\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# EDIT THESE (your experiment)\n",
        "# -------------------------\n",
        "CLEAN_TEXT = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "CORRUPT_TEXT = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "TOKEN_A_STR = \" Jones\"   # clean-consistent\n",
        "TOKEN_B_STR = \" Smith\"   # corrupt-consistent\n",
        "TOP_K_HOTSPOTS = 3\n",
        "\n",
        "\n",
        "def get_device() -> str:\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def tokens_same_length(bpe: BPETokenizer, a: str, b: str) -> bool:\n",
        "    return bpe(a).shape[1] == bpe(b).shape[1]\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_score(model, idx, token_a_id: int, token_b_id: int) -> float:\n",
        "    _ = model(idx)\n",
        "    return wsc.score_from_last_logits(model.last_logits[0], token_a_id, token_b_id)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_match_heatmap(\n",
        "    model,\n",
        "    idx_corr: torch.Tensor,\n",
        "    token_a_id: int,\n",
        "    token_b_id: int,\n",
        ") -> torch.Tensor:\n",
        "    n_layers = len(model.transformer.h)\n",
        "    seq_len = idx_corr.shape[1]\n",
        "    out = torch.empty((n_layers, seq_len), dtype=torch.float32)\n",
        "\n",
        "    for L in range(n_layers):\n",
        "        for P in range(seq_len):\n",
        "            _ = model(idx_corr, layer_to_patch=L, position_to_patch=P)  # source defaults to match\n",
        "            out[L, P] = wsc.score_from_last_logits(model.last_logits[0], token_a_id, token_b_id)\n",
        "    return out\n",
        "\n",
        "\n",
        "def select_hotspots_and_cold(\n",
        "    match_heatmap: torch.Tensor,\n",
        "    score_clean: float,\n",
        "    score_corr: float,\n",
        "    top_k: int = 3,\n",
        ") -> Tuple[List[Tuple[int, int]], Tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    Hotspots: largest normalized restoration R\n",
        "    Cold: smallest absolute change vs corrupted baseline\n",
        "    \"\"\"\n",
        "    n_layers, seq_len = match_heatmap.shape\n",
        "    R = torch.empty_like(match_heatmap)\n",
        "\n",
        "    denom = (score_clean - score_corr)\n",
        "    if abs(denom) < 1e-12:\n",
        "        # degenerate; just pick arbitrary cells\n",
        "        hot = [(0, 0)]\n",
        "        cold = (0, 0)\n",
        "        return hot, cold\n",
        "\n",
        "    R = (match_heatmap - score_corr) / denom\n",
        "\n",
        "    # flatten\n",
        "    flat_R = R.flatten()\n",
        "    top_vals, top_idx = torch.topk(flat_R, k=min(top_k, flat_R.numel()))\n",
        "    hotspots = []\n",
        "    used = set()\n",
        "    for idx in top_idx.tolist():\n",
        "        L = idx // seq_len\n",
        "        P = idx % seq_len\n",
        "        if (L, P) not in used:\n",
        "            hotspots.append((L, P))\n",
        "            used.add((L, P))\n",
        "        if len(hotspots) >= top_k:\n",
        "            break\n",
        "\n",
        "    # cold: minimal |score_patch - score_corr| among remaining cells\n",
        "    delta = (match_heatmap - score_corr).abs()\n",
        "    delta_flat = delta.flatten()\n",
        "    # mask out hotspots\n",
        "    mask = torch.ones_like(delta_flat, dtype=torch.bool)\n",
        "    for (L, P) in hotspots:\n",
        "        mask[L * seq_len + P] = False\n",
        "    masked_delta = delta_flat.clone()\n",
        "    masked_delta[~mask] = float(\"inf\")\n",
        "    cold_idx = int(torch.argmin(masked_delta).item())\n",
        "    cold = (cold_idx // seq_len, cold_idx % seq_len)\n",
        "\n",
        "    return hotspots, cold\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    set_seed(3407)\n",
        "    device = get_device()\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # 1) Load model + tokenizer\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    # 2) Validate same token length\n",
        "    if not tokens_same_length(bpe, CLEAN_TEXT, CORRUPT_TEXT):\n",
        "        raise RuntimeError(\n",
        "            \"CLEAN_TEXT and CORRUPT_TEXT do NOT have the same number of BPE tokens.\\n\"\n",
        "            \"Fix the texts until they tokenize to the same length.\"\n",
        "        )\n",
        "\n",
        "    # 3) Tokenize prompts\n",
        "    idx_clean = bpe(CLEAN_TEXT).to(device)     # (1, T)\n",
        "    idx_corr = bpe(CORRUPT_TEXT).to(device)    # (1, T)\n",
        "    seq_len = idx_corr.shape[1]\n",
        "    n_layers = len(model.transformer.h)\n",
        "\n",
        "    # 4) Token ids for metric\n",
        "    token_a_id = wsc.single_token_id(bpe, TOKEN_A_STR)\n",
        "    token_b_id = wsc.single_token_id(bpe, TOKEN_B_STR)\n",
        "\n",
        "    # 5) Clean baseline (cache activations)\n",
        "    _ = model(idx_clean, cache_activations=True, overwrite_cache=True)\n",
        "    score_clean = wsc.score_from_last_logits(model.last_logits[0], token_a_id, token_b_id)\n",
        "\n",
        "    # 6) Corrupted baseline\n",
        "    _ = model(idx_corr)\n",
        "    score_corr = wsc.score_from_last_logits(model.last_logits[0], token_a_id, token_b_id)\n",
        "\n",
        "    print(\"\\n=== Baselines ===\")\n",
        "    print(f\"seq_len={seq_len}, n_layers={n_layers}\")\n",
        "    print(f\"score_clean = {score_clean:.6f}\")\n",
        "    print(f\"score_corr  = {score_corr:.6f}\")\n",
        "\n",
        "    # 7) Compute MATCH heatmap (standard patch)\n",
        "    print(\"\\nComputing match heatmap (this is the same sweep as your main analysis)...\")\n",
        "    match_heatmap = compute_match_heatmap(model, idx_corr, token_a_id, token_b_id)\n",
        "    torch.save(match_heatmap.cpu(), \"match_heatmap.pt\")\n",
        "    print(\"Saved: match_heatmap.pt\")\n",
        "\n",
        "    # 8) Pick top hotspots + one cold cell\n",
        "    hotspots, cold = select_hotspots_and_cold(match_heatmap, score_clean, score_corr, top_k=TOP_K_HOTSPOTS)\n",
        "    targets = hotspots + [cold]\n",
        "\n",
        "    print(\"\\n=== Selected targets ===\")\n",
        "    for i, (L, P) in enumerate(targets):\n",
        "        s = float(match_heatmap[L, P])\n",
        "        R = wsc.normalized_restoration(s, score_clean, score_corr)\n",
        "        tag = \"COLD\" if (L, P) == cold else \"HOT\"\n",
        "        print(f\"{i+1:02d}. ({L},{P})  match_score={s:.6f}  R_match={R:.4f}  [{tag}]\")\n",
        "\n",
        "    # 9) Run wrong-source conditions per target\n",
        "    print(\"\\n=== WRONG-SOURCE CONTROL RESULTS ===\")\n",
        "    print(\"(Metric: score = logit(B) - logit(A); higher/lower direction depends on your pair)\\n\")\n",
        "\n",
        "    for (L, P) in targets:\n",
        "        conds = wsc.conditions_for_target(L, P, n_layers=n_layers, seq_len=seq_len)\n",
        "\n",
        "        print(f\"\\n--- Target (L={L}, P={P}) ---\")\n",
        "        print(f\"{'condition':12s} | {'source':10s} | {'score':>12s} | {'R':>8s}\")\n",
        "        print(\"-\" * 52)\n",
        "\n",
        "        for c in conds:\n",
        "            row = wsc.run_condition(model, idx_corr, c, token_a_id, token_b_id)\n",
        "            score = row[\"score\"]\n",
        "            R = wsc.normalized_restoration(score, score_clean, score_corr)\n",
        "\n",
        "            if c.source is None:\n",
        "                src = \"-\"\n",
        "            else:\n",
        "                src = f\"({c.source[0]},{c.source[1]})\"\n",
        "\n",
        "            print(f\"{c.name:12s} | {src:10s} | {score:12.6f} | {R:8.4f}\")\n",
        "\n",
        "    # 10) OPTIONAL: build one full wrong-source heatmap using deterministic rule (pos+1 else pos-1)\n",
        "    print(\"\\nOptional: computing a full wrong-source heatmap with rule: source=(L,P+1) else (L,P-1)\")\n",
        "    ws_heatmap = torch.empty_like(match_heatmap)\n",
        "    for L in range(n_layers):\n",
        "        for P in range(seq_len):\n",
        "            srcP = P + 1 if (P + 1 < seq_len) else (P - 1)\n",
        "            _ = model(idx_corr, layer_to_patch=L, position_to_patch=P, source_layer=L, source_position=srcP)\n",
        "            ws_heatmap[L, P] = wsc.score_from_last_logits(model.last_logits[0], token_a_id, token_b_id)\n",
        "\n",
        "    torch.save(ws_heatmap.cpu(), \"wrong_source_posshift_heatmap.pt\")\n",
        "    print(\"Saved: wrong_source_posshift_heatmap.pt\")\n",
        "    print(\"\\nDone \")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_extra1_wrong_source_control.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_extra1_wrong_source_control.py\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "from mingpt.model import GPT\n",
        "\n",
        "import wrong_source_control as wsc\n",
        "\n",
        "\n",
        "def _make_tiny():\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = 200\n",
        "    cfg.block_size = 32\n",
        "    model = GPT(cfg).eval()\n",
        "    return model, cfg\n",
        "\n",
        "\n",
        "def _make_clean_corrupt(cfg, T=12):\n",
        "    torch.manual_seed(0)\n",
        "    clean = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "    corrupt = clean.clone()\n",
        "    corrupt[0, 3] = (corrupt[0, 3] + 1) % cfg.vocab_size\n",
        "    return clean, corrupt\n",
        "\n",
        "\n",
        "def test_forward_accepts_wrong_source_parameters_and_records_source_bookkeeping():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt = _make_clean_corrupt(cfg, T=12)\n",
        "\n",
        "    # cache clean\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    L_target, P_target = 0, 3\n",
        "    L_source, P_source = 0, 4\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(\n",
        "            corrupt,\n",
        "            record_activations=True,\n",
        "            layer_to_patch=L_target,\n",
        "            position_to_patch=P_target,\n",
        "            source_layer=L_source,\n",
        "            source_position=P_source,\n",
        "        )\n",
        "\n",
        "    assert model.last_patch == (L_target, P_target)\n",
        "    assert hasattr(model, \"last_patch_source\")\n",
        "    assert model.last_patch_source == (L_source, P_source)\n",
        "\n",
        "    # patched activation at (L_target, P_target) must equal clean cache at (L_source, P_source)\n",
        "    patched_acts = model.last_activations\n",
        "    assert patched_acts is not None\n",
        "    assert torch.allclose(\n",
        "        patched_acts[L_target][P_target],\n",
        "        model.clean_activations[L_source][P_source],\n",
        "        rtol=1e-5,\n",
        "        atol=1e-6,\n",
        "    )\n",
        "\n",
        "\n",
        "def test_standard_patch_is_default_when_source_not_provided():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt = _make_clean_corrupt(cfg, T=10)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    L, P = 1, 2\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt, record_activations=True, layer_to_patch=L, position_to_patch=P)\n",
        "\n",
        "    assert model.last_patch == (L, P)\n",
        "    assert model.last_patch_source == (L, P)  # default source == target\n",
        "\n",
        "    patched_acts = model.last_activations\n",
        "    assert torch.allclose(\n",
        "        patched_acts[L][P],\n",
        "        model.clean_activations[L][P],\n",
        "        rtol=1e-5,\n",
        "        atol=1e-6,\n",
        "    )\n",
        "\n",
        "\n",
        "def test_wrong_source_pairing_rules_enforced():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt = _make_clean_corrupt(cfg, T=10)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    with pytest.raises(ValueError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(corrupt, layer_to_patch=0, position_to_patch=3, source_layer=0, source_position=None)\n",
        "\n",
        "    with pytest.raises(ValueError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(corrupt, layer_to_patch=0, position_to_patch=3, source_layer=None, source_position=4)\n",
        "\n",
        "\n",
        "def test_wrong_source_bounds_checked():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt = _make_clean_corrupt(cfg, T=8)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    # source_position out of range\n",
        "    with pytest.raises(IndexError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(corrupt, layer_to_patch=0, position_to_patch=3, source_layer=0, source_position=999)\n",
        "\n",
        "    # source_layer out of range\n",
        "    with pytest.raises(IndexError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(corrupt, layer_to_patch=0, position_to_patch=3, source_layer=999, source_position=3)\n",
        "\n",
        "\n",
        "def test_conditions_for_target_respects_boundaries():\n",
        "    n_layers = 12\n",
        "    seq_len = 10\n",
        "\n",
        "    # P=0 => no WS-pos-\n",
        "    conds = wsc.conditions_for_target(L=5, P=0, n_layers=n_layers, seq_len=seq_len)\n",
        "    names = {c.name for c in conds}\n",
        "    assert \"WS-pos-\" not in names\n",
        "    assert \"WS-pos+\" in names\n",
        "\n",
        "    # P=seq_len-1 => no WS-pos+\n",
        "    conds = wsc.conditions_for_target(L=5, P=seq_len - 1, n_layers=n_layers, seq_len=seq_len)\n",
        "    names = {c.name for c in conds}\n",
        "    assert \"WS-pos+\" not in names\n",
        "    assert \"WS-pos-\" in names\n",
        "\n",
        "    # L=0 => no WS-layer-\n",
        "    conds = wsc.conditions_for_target(L=0, P=3, n_layers=n_layers, seq_len=seq_len)\n",
        "    names = {c.name for c in conds}\n",
        "    assert \"WS-layer-\" not in names\n",
        "    assert \"WS-layer+\" in names\n",
        "\n",
        "    # L=n_layers-1 => no WS-layer+\n",
        "    conds = wsc.conditions_for_target(L=n_layers - 1, P=3, n_layers=n_layers, seq_len=seq_len)\n",
        "    names = {c.name for c in conds}\n",
        "    assert \"WS-layer+\" not in names\n",
        "    assert \"WS-layer-\" in names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                    [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 3.21s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -q test_extra1_wrong_source_control.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting wrong_source_control_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile wrong_source_control_driver.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "from mingpt.bpe import BPETokenizer\n",
        "from mingpt.model import GPT\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class PatchResult:\n",
        "    target: Tuple[int, int]\n",
        "    variant: str\n",
        "    source: Tuple[int, int]\n",
        "    score: float\n",
        "    R: float           # normalized restoration (can be > 1 if overshoot)\n",
        "    C: float           # normalized closeness-to-clean (1 is best)\n",
        "    last_patch: Optional[Tuple[int, int]]\n",
        "    last_patch_source: Optional[Tuple[int, int]]\n",
        "\n",
        "\n",
        "def get_device() -> str:\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def single_token_id(bpe: BPETokenizer, token_str: str) -> int:\n",
        "    ids = bpe(token_str)[0].tolist()\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(\n",
        "            f\"Token string must map to exactly 1 BPE token. \"\n",
        "            f\"Got {len(ids)} tokens for {repr(token_str)}: {ids}\"\n",
        "        )\n",
        "    return int(ids[0])\n",
        "\n",
        "\n",
        "def logit_diff_from_last_logits(last_logits_1d: torch.Tensor, token_b_id: int, token_a_id: int) -> float:\n",
        "    # score = logit(B) - logit(A)\n",
        "    return float(last_logits_1d[token_b_id] - last_logits_1d[token_a_id])\n",
        "\n",
        "\n",
        "def norm_restoration(score: float, score_clean: float, score_corr: float) -> float:\n",
        "    # R = (score - score_corr) / (score_clean - score_corr)\n",
        "    denom = (score_clean - score_corr)\n",
        "    if abs(denom) < 1e-12:\n",
        "        return 0.0\n",
        "    return (score - score_corr) / denom\n",
        "\n",
        "\n",
        "def norm_closeness(score: float, score_clean: float, score_corr: float) -> float:\n",
        "    # C = 1 - |score - score_clean| / |score_corr - score_clean|\n",
        "    denom = abs(score_corr - score_clean)\n",
        "    if denom < 1e-12:\n",
        "        return 1.0\n",
        "    return 1.0 - (abs(score - score_clean) / denom)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_score(\n",
        "    model: GPT,\n",
        "    idx: torch.Tensor,\n",
        "    token_a_id: int,\n",
        "    token_b_id: int,\n",
        "    *,\n",
        "    layer_to_patch: Optional[int] = None,\n",
        "    position_to_patch: Optional[int] = None,\n",
        "    source_layer: Optional[int] = None,\n",
        "    source_position: Optional[int] = None,\n",
        ") -> float:\n",
        "    _logits, _loss = model(\n",
        "        idx,\n",
        "        record_activations=False,\n",
        "        cache_activations=False,\n",
        "        overwrite_cache=False,\n",
        "        layer_to_patch=layer_to_patch,\n",
        "        position_to_patch=position_to_patch,\n",
        "        source_layer=source_layer,\n",
        "        source_position=source_position,\n",
        "    )\n",
        "    if model.last_logits is None:\n",
        "        raise RuntimeError(\"model.last_logits is None after forward().\")\n",
        "    return logit_diff_from_last_logits(model.last_logits[0], token_b_id=token_b_id, token_a_id=token_a_id)\n",
        "\n",
        "\n",
        "def build_wrong_source_variants(L: int, P: int, n_layer: int, T: int) -> List[Tuple[str, int, int]]:\n",
        "    variants: List[Tuple[str, int, int]] = []\n",
        "    variants.append((\"MATCH\", L, P))\n",
        "\n",
        "    # Position mismatch (same layer)\n",
        "    if P + 1 < T:\n",
        "        variants.append((\"WS-pos+\", L, P + 1))\n",
        "    if P - 1 >= 0:\n",
        "        variants.append((\"WS-pos-\", L, P - 1))\n",
        "\n",
        "    # Layer mismatch (same position)\n",
        "    if L + 1 < n_layer:\n",
        "        variants.append((\"WS-layer+\", L + 1, P))\n",
        "    if L - 1 >= 0:\n",
        "        variants.append((\"WS-layer-\", L - 1, P))\n",
        "\n",
        "    return variants\n",
        "\n",
        "\n",
        "def select_hotspots(match_scores: torch.Tensor, score_clean: float, k: int = 3) -> List[Tuple[int, int]]:\n",
        "    # Pick coords whose MATCH patched score is closest to clean (min |score - score_clean|)\n",
        "    n_layer, T = match_scores.shape\n",
        "    flat: List[Tuple[float, int, int]] = []\n",
        "    for L in range(n_layer):\n",
        "        for P in range(T):\n",
        "            d = abs(float(match_scores[L, P]) - score_clean)\n",
        "            flat.append((d, L, P))\n",
        "    flat.sort(key=lambda x: x[0])\n",
        "    out: List[Tuple[int, int]] = []\n",
        "    for _, L, P in flat:\n",
        "        out.append((L, P))\n",
        "        if len(out) >= k:\n",
        "            break\n",
        "    return out\n",
        "\n",
        "\n",
        "def select_coldcell(\n",
        "    match_scores: torch.Tensor,\n",
        "    score_corr: float,\n",
        "    *,\n",
        "    changed_pos: int,\n",
        ") -> Tuple[int, int]:\n",
        "    # Pick coord with minimal |score - score_corr| but avoid positions before/at the changed token\n",
        "    n_layer, T = match_scores.shape\n",
        "    pos_min = min(T - 1, changed_pos + 1)\n",
        "\n",
        "    candidates: List[Tuple[float, int, int]] = []\n",
        "    for L in range(n_layer):\n",
        "        for P in range(pos_min, T):\n",
        "            d = abs(float(match_scores[L, P]) - score_corr)\n",
        "            candidates.append((d, L, P))\n",
        "\n",
        "    # Fallback if pos_min kills all candidates\n",
        "    if not candidates:\n",
        "        for L in range(n_layer):\n",
        "            for P in range(T):\n",
        "                d = abs(float(match_scores[L, P]) - score_corr)\n",
        "                candidates.append((d, L, P))\n",
        "\n",
        "    candidates.sort(key=lambda x: x[0])\n",
        "    _, L, P = candidates[0]\n",
        "    return (L, P)\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--clean\", type=str, default=\"Michelle Jones was a top-notch student. Michelle\")\n",
        "    p.add_argument(\"--corrupt\", type=str, default=\"Michelle Smith was a top-notch student. Michelle\")\n",
        "    p.add_argument(\"--token_a\", type=str, default=\" Jones\", help=\"Token A (clean-consistent), usually with leading space\")\n",
        "    p.add_argument(\"--token_b\", type=str, default=\" Smith\", help=\"Token B (corrupt-consistent), usually with leading space\")\n",
        "    p.add_argument(\"--seed\", type=int, default=3407)\n",
        "    p.add_argument(\"--n_hot\", type=int, default=3)\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    device = get_device()\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    bpe = BPETokenizer()\n",
        "    token_a_id = single_token_id(bpe, args.token_a)\n",
        "    token_b_id = single_token_id(bpe, args.token_b)\n",
        "\n",
        "    idx_clean = bpe(args.clean).to(device)\n",
        "    idx_corr = bpe(args.corrupt).to(device)\n",
        "\n",
        "    if idx_clean.shape != idx_corr.shape:\n",
        "        raise ValueError(\n",
        "            f\"Clean/corrupt token length mismatch: clean T={idx_clean.shape[1]} vs corrupt T={idx_corr.shape[1]}\"\n",
        "        )\n",
        "\n",
        "    # Find changed token position (expect exactly one token differs)\n",
        "    clean_ids = idx_clean[0].tolist()\n",
        "    corr_ids = idx_corr[0].tolist()\n",
        "    diffs = [i for i, (a, b) in enumerate(zip(clean_ids, corr_ids)) if int(a) != int(b)]\n",
        "    if len(diffs) != 1:\n",
        "        raise ValueError(f\"Expected exactly 1 differing token position, found {len(diffs)}: {diffs}\")\n",
        "    changed_pos = int(diffs[0])\n",
        "    T = int(idx_clean.shape[1])\n",
        "\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    n_layer = int(len(model.transformer.h))\n",
        "\n",
        "    print(f\"Seq len T={T}, changed token position={changed_pos}\")\n",
        "\n",
        "    # --- Baselines ---\n",
        "    _ = model(idx_clean, cache_activations=True, overwrite_cache=True)\n",
        "    if model.last_logits is None:\n",
        "        raise RuntimeError(\"model.last_logits not set on clean run.\")\n",
        "    score_clean = logit_diff_from_last_logits(model.last_logits[0], token_b_id=token_b_id, token_a_id=token_a_id)\n",
        "\n",
        "    score_corr = run_score(model, idx_corr, token_a_id=token_a_id, token_b_id=token_b_id)\n",
        "\n",
        "    print(\"\\n=== Baselines ===\")\n",
        "    print(f\"score_clean  = {score_clean:.4f}\")\n",
        "    print(f\"score_corr   = {score_corr:.4f}\")\n",
        "    print(f\"gap (clean-corr) = {(score_clean - score_corr):.4f}\")\n",
        "\n",
        "    # --- Build MATCH heatmap (scores) ---\n",
        "    match_scores = torch.empty((n_layer, T), dtype=torch.float32)\n",
        "    for L in range(n_layer):\n",
        "        for P in range(T):\n",
        "            s = run_score(\n",
        "                model,\n",
        "                idx_corr,\n",
        "                token_a_id=token_a_id,\n",
        "                token_b_id=token_b_id,\n",
        "                layer_to_patch=L,\n",
        "                position_to_patch=P,\n",
        "                source_layer=L,\n",
        "                source_position=P,\n",
        "            )\n",
        "            match_scores[L, P] = float(s)\n",
        "\n",
        "    # --- Select coords: hot + cold ---\n",
        "    hot = select_hotspots(match_scores, score_clean=score_clean, k=args.n_hot)\n",
        "    cold = select_coldcell(match_scores, score_corr=score_corr, changed_pos=changed_pos)\n",
        "\n",
        "    print(\"\\nSelected coords:\")\n",
        "    for (L, P) in hot:\n",
        "        s = float(match_scores[L, P])\n",
        "        improvement = (score_corr - s)\n",
        "        print(f\"  HOT: (L={L}, P={P})  match_score={s:.4f}  improvement={improvement:.4f}\")\n",
        "    s_cold = float(match_scores[cold[0], cold[1]])\n",
        "    print(f\"  COLD: (L={cold[0]}, P={cold[1]})  match_score={s_cold:.4f}  improvement={(score_corr - s_cold):.4f}\")\n",
        "\n",
        "    # --- Wrong-source control table ---\n",
        "    print(\"\\n=== Wrong-source control table ===\")\n",
        "    print(\"coord | variant | source(L,P) | score | R (restoration) | C (closeness) | last_patch | last_patch_source\")\n",
        "    print(\"-\" * 110)\n",
        "\n",
        "    selected = hot + [cold]\n",
        "\n",
        "    for (L, P) in selected:\n",
        "        variants = build_wrong_source_variants(L, P, n_layer=n_layer, T=T)\n",
        "        results: List[PatchResult] = []\n",
        "\n",
        "        for (name, sL, sP) in variants:\n",
        "            s = run_score(\n",
        "                model,\n",
        "                idx_corr,\n",
        "                token_a_id=token_a_id,\n",
        "                token_b_id=token_b_id,\n",
        "                layer_to_patch=L,\n",
        "                position_to_patch=P,\n",
        "                source_layer=sL,\n",
        "                source_position=sP,\n",
        "            )\n",
        "            R = norm_restoration(s, score_clean=score_clean, score_corr=score_corr)\n",
        "            C = norm_closeness(s, score_clean=score_clean, score_corr=score_corr)\n",
        "\n",
        "            results.append(\n",
        "                PatchResult(\n",
        "                    target=(L, P),\n",
        "                    variant=name,\n",
        "                    source=(sL, sP),\n",
        "                    score=float(s),\n",
        "                    R=float(R),\n",
        "                    C=float(C),\n",
        "                    last_patch=model.last_patch,\n",
        "                    last_patch_source=model.last_patch_source,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Print rows\n",
        "        for r in results:\n",
        "            print(\n",
        "                f\"({r.target[0]:02d},{r.target[1]:02d}) | \"\n",
        "                f\"{r.variant:<9} | \"\n",
        "                f\"({r.source[0]:02d},{r.source[1]:02d})     | \"\n",
        "                f\"{r.score:>7.4f} | \"\n",
        "                f\"{r.R:>7.3f}        | \"\n",
        "                f\"{r.C:>7.3f}        | \"\n",
        "                f\"{r.last_patch} | {r.last_patch_source}\"\n",
        "            )\n",
        "\n",
        "        # Specificity index (FIX): use closeness-to-clean, not restoration fraction\n",
        "        c_match = max([rr.C for rr in results if rr.variant == \"MATCH\"], default=0.0)\n",
        "        c_wrong = [rr.C for rr in results if rr.variant != \"MATCH\"]\n",
        "        if c_wrong:\n",
        "            S = c_match - max(c_wrong)\n",
        "        else:\n",
        "            S = 0.0\n",
        "\n",
        "        print(f\"-> Specificity index S = C_match - max(C_wrong) = {S:.3f}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "number of parameters: 124.44M\n",
            "Seq len T=11, changed token position=1\n",
            "\n",
            "=== Baselines ===\n",
            "score_clean  = -4.1241\n",
            "score_corr   = 5.6562\n",
            "gap (clean-corr) = -9.7803\n",
            "\n",
            "Selected coords:\n",
            "  HOT: (L=11, P=10)  match_score=-4.1241  improvement=9.7803\n",
            "  HOT: (L=0, P=1)  match_score=-4.0691  improvement=9.7254\n",
            "  HOT: (L=1, P=1)  match_score=-4.0673  improvement=9.7235\n",
            "  COLD: (L=11, P=2)  match_score=5.6562  improvement=0.0000\n",
            "\n",
            "=== Wrong-source control table ===\n",
            "coord | variant | source(L,P) | score | R (restoration) | C (closeness) | last_patch | last_patch_source\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "(11,10) | MATCH     | (11,10)     | -4.1241 |   1.000        |   1.000        | (11, 10) | (11, 10)\n",
            "(11,10) | WS-pos-   | (11,09)     | -2.8865 |   0.873        |   0.873        | (11, 10) | (11, 9)\n",
            "(11,10) | WS-layer- | (10,10)     | -5.6192 |   1.153        |   0.847        | (11, 10) | (10, 10)\n",
            "-> Specificity index S = C_match - max(C_wrong) = 0.127\n",
            "\n",
            "(00,01) | MATCH     | (00,01)     | -4.0691 |   0.994        |   0.994        | (0, 1) | (0, 1)\n",
            "(00,01) | WS-pos+   | (00,02)     |  1.8408 |   0.390        |   0.390        | (0, 1) | (0, 2)\n",
            "(00,01) | WS-pos-   | (00,00)     |  1.6059 |   0.414        |   0.414        | (0, 1) | (0, 0)\n",
            "(00,01) | WS-layer+ | (01,01)     | -3.6126 |   0.948        |   0.948        | (0, 1) | (1, 1)\n",
            "-> Specificity index S = C_match - max(C_wrong) = 0.047\n",
            "\n",
            "(01,01) | MATCH     | (01,01)     | -4.0673 |   0.994        |   0.994        | (1, 1) | (1, 1)\n",
            "(01,01) | WS-pos+   | (01,02)     |  1.8698 |   0.387        |   0.387        | (1, 1) | (1, 2)\n",
            "(01,01) | WS-pos-   | (01,00)     |  1.5902 |   0.416        |   0.416        | (1, 1) | (1, 0)\n",
            "(01,01) | WS-layer+ | (02,01)     | -3.9423 |   0.981        |   0.981        | (1, 1) | (2, 1)\n",
            "(01,01) | WS-layer- | (00,01)     | -4.1378 |   1.001        |   0.999        | (1, 1) | (0, 1)\n",
            "-> Specificity index S = C_match - max(C_wrong) = -0.004\n",
            "\n",
            "(11,02) | MATCH     | (11,02)     |  5.6562 |  -0.000        |   0.000        | (11, 2) | (11, 2)\n",
            "(11,02) | WS-pos+   | (11,03)     |  5.6562 |  -0.000        |   0.000        | (11, 2) | (11, 3)\n",
            "(11,02) | WS-pos-   | (11,01)     |  5.6562 |  -0.000        |   0.000        | (11, 2) | (11, 1)\n",
            "(11,02) | WS-layer- | (10,02)     |  5.6562 |  -0.000        |   0.000        | (11, 2) | (10, 2)\n",
            "-> Specificity index S = C_match - max(C_wrong) = 0.000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python wrong_source_control_driver.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 78%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                     [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m92 passed\u001b[0m\u001b[32m in 42.19s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EXTRA SECTION 2: Add an interpolation sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing interpolation_sweep.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile interpolation_sweep.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "Coord = Tuple[int, int]  # (layer, position)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Baselines:\n",
        "    clean_score: float\n",
        "    corrupt_score: float\n",
        "    token_a_id: int\n",
        "    token_b_id: int\n",
        "    seq_len: int\n",
        "    n_layer: int\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Curve:\n",
        "    coord: Coord\n",
        "    alphas: List[float]\n",
        "    scores: List[float]\n",
        "    restorations: List[float]\n",
        "    alpha50: Optional[float]\n",
        "\n",
        "\n",
        "def single_token_id(bpe, token_str: str) -> int:\n",
        "    ids = bpe(token_str)[0].tolist()\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{repr(token_str)} is not a single BPE token. Got {len(ids)} ids: {ids}\")\n",
        "    return int(ids[0])\n",
        "\n",
        "\n",
        "def score_from_last_logits(last_logits_1d: torch.Tensor, token_a_id: int, token_b_id: int) -> float:\n",
        "    # score = logit(B) - logit(A)\n",
        "    return float(last_logits_1d[token_b_id] - last_logits_1d[token_a_id])\n",
        "\n",
        "\n",
        "def restoration_fraction(score: float, score_corr: float, score_clean: float) -> float:\n",
        "    denom = (score_clean - score_corr)\n",
        "    if abs(denom) < 1e-12:\n",
        "        return float(\"nan\")\n",
        "    return (score - score_corr) / denom\n",
        "\n",
        "\n",
        "def estimate_alpha50(alphas: Sequence[float], restorations: Sequence[float]) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Returns the smallest alpha where R(alpha) >= 0.5 using linear interpolation.\n",
        "    If never reaches 0.5 (or NaNs), returns None.\n",
        "    \"\"\"\n",
        "    xs = list(alphas)\n",
        "    ys = list(restorations)\n",
        "\n",
        "    # Filter NaNs but keep order\n",
        "    pairs = [(x, y) for x, y in zip(xs, ys) if (y is not None and not math.isnan(y))]\n",
        "    if len(pairs) < 2:\n",
        "        return None\n",
        "\n",
        "    for i in range(1, len(pairs)):\n",
        "        x0, y0 = pairs[i - 1]\n",
        "        x1, y1 = pairs[i]\n",
        "        if y0 >= 0.5:\n",
        "            return x0\n",
        "        if (y0 < 0.5) and (y1 >= 0.5) and (x1 != x0):\n",
        "            t = (0.5 - y0) / (y1 - y0)\n",
        "            return x0 + t * (x1 - x0)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_baselines(\n",
        "    model,\n",
        "    bpe,\n",
        "    clean_text: str,\n",
        "    corrupt_text: str,\n",
        "    token_a_str: str,\n",
        "    token_b_str: str,\n",
        "    *,\n",
        "    device: Optional[str] = None,\n",
        "    overwrite_cache: bool = True,\n",
        ") -> Baselines:\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    idx_clean = bpe(clean_text).to(device)\n",
        "    idx_corr = bpe(corrupt_text).to(device)\n",
        "\n",
        "    if idx_clean.shape[1] != idx_corr.shape[1]:\n",
        "        raise ValueError(\n",
        "            f\"Token length mismatch: clean T={idx_clean.shape[1]} vs corrupt T={idx_corr.shape[1]}. \"\n",
        "            \"They must match for activation patching.\"\n",
        "        )\n",
        "\n",
        "    token_a_id = single_token_id(bpe, token_a_str)\n",
        "    token_b_id = single_token_id(bpe, token_b_str)\n",
        "\n",
        "    # Clean run: cache activations\n",
        "    _ = model(idx_clean, cache_activations=True, overwrite_cache=overwrite_cache)\n",
        "    if model.last_logits is None:\n",
        "        raise RuntimeError(\"model.last_logits missing after clean run.\")\n",
        "    clean_score = score_from_last_logits(model.last_logits[0], token_a_id, token_b_id)\n",
        "\n",
        "    # Corrupt baseline\n",
        "    _ = model(idx_corr)\n",
        "    if model.last_logits is None:\n",
        "        raise RuntimeError(\"model.last_logits missing after corrupt run.\")\n",
        "    corrupt_score = score_from_last_logits(model.last_logits[0], token_a_id, token_b_id)\n",
        "\n",
        "    n_layer = len(model.transformer.h)\n",
        "    seq_len = int(idx_clean.shape[1])\n",
        "\n",
        "    return Baselines(\n",
        "        clean_score=clean_score,\n",
        "        corrupt_score=corrupt_score,\n",
        "        token_a_id=token_a_id,\n",
        "        token_b_id=token_b_id,\n",
        "        seq_len=seq_len,\n",
        "        n_layer=n_layer,\n",
        "    )\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def full_patch_matrix(\n",
        "    model,\n",
        "    bpe,\n",
        "    corrupt_text: str,\n",
        "    baselines: Baselines,\n",
        "    *,\n",
        "    device: Optional[str] = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the standard heatmap scores for alpha=1 patching:\n",
        "      M[L, P] = score after patching (L,P) with clean (L,P).\n",
        "    Shape: (n_layer, seq_len)\n",
        "    \"\"\"\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    idx_corr = bpe(corrupt_text).to(device)\n",
        "\n",
        "    M = torch.empty((baselines.n_layer, baselines.seq_len), dtype=torch.float32)\n",
        "    for L in range(baselines.n_layer):\n",
        "        for P in range(baselines.seq_len):\n",
        "            _ = model(\n",
        "                idx_corr,\n",
        "                layer_to_patch=L,\n",
        "                position_to_patch=P,\n",
        "                patch_alpha=1.0,\n",
        "            )\n",
        "            score = score_from_last_logits(model.last_logits[0], baselines.token_a_id, baselines.token_b_id)\n",
        "            M[L, P] = score\n",
        "    return M\n",
        "\n",
        "\n",
        "def select_hotspots(\n",
        "    M: torch.Tensor,\n",
        "    baselines: Baselines,\n",
        "    *,\n",
        "    top_k: int = 3,\n",
        ") -> Tuple[List[Coord], Coord]:\n",
        "    \"\"\"\n",
        "    Picks:\n",
        "      - hotspots: top_k cells with highest restoration at alpha=1\n",
        "      - coldspot: cell with minimal |score - corrupt_score| (near-zero effect)\n",
        "    \"\"\"\n",
        "    n_layer, T = M.shape\n",
        "    R = torch.empty_like(M)\n",
        "    for L in range(n_layer):\n",
        "        for P in range(T):\n",
        "            R[L, P] = float(\n",
        "                restoration_fraction(float(M[L, P]), baselines.corrupt_score, baselines.clean_score)\n",
        "            )\n",
        "\n",
        "    # Flatten + sort by restoration descending (best restoration first)\n",
        "    flat = []\n",
        "    for L in range(n_layer):\n",
        "        for P in range(T):\n",
        "            r = float(R[L, P])\n",
        "            if not math.isnan(r):\n",
        "                flat.append(((L, P), r))\n",
        "\n",
        "    flat.sort(key=lambda x: x[1], reverse=True)\n",
        "    hotspots = [coord for coord, _ in flat[:top_k]]\n",
        "\n",
        "    # Coldspot: closest to corrupt baseline (small absolute effect)\n",
        "    best_cold = (0, 0)\n",
        "    best_dist = float(\"inf\")\n",
        "    for L in range(n_layer):\n",
        "        for P in range(T):\n",
        "            dist = abs(float(M[L, P]) - baselines.corrupt_score)\n",
        "            if dist < best_dist:\n",
        "                best_dist = dist\n",
        "                best_cold = (L, P)\n",
        "\n",
        "    return hotspots, best_cold\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def interpolation_curve(\n",
        "    model,\n",
        "    bpe,\n",
        "    corrupt_text: str,\n",
        "    baselines: Baselines,\n",
        "    coord: Coord,\n",
        "    alphas: Sequence[float],\n",
        "    *,\n",
        "    device: Optional[str] = None,\n",
        "    source_coord: Optional[Coord] = None,  # optional wrong-source + interpolation together\n",
        ") -> Curve:\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    idx_corr = bpe(corrupt_text).to(device)\n",
        "\n",
        "    L, P = coord\n",
        "    if source_coord is None:\n",
        "        srcL, srcP = L, P\n",
        "    else:\n",
        "        srcL, srcP = source_coord\n",
        "\n",
        "    scores: List[float] = []\n",
        "    restorations: List[float] = []\n",
        "    alphas_out: List[float] = []\n",
        "\n",
        "    for a in alphas:\n",
        "        a = float(a)\n",
        "        _ = model(\n",
        "            idx_corr,\n",
        "            layer_to_patch=L,\n",
        "            position_to_patch=P,\n",
        "            source_layer=srcL,\n",
        "            source_position=srcP,\n",
        "            patch_alpha=a,\n",
        "        )\n",
        "        sc = score_from_last_logits(model.last_logits[0], baselines.token_a_id, baselines.token_b_id)\n",
        "        r = restoration_fraction(sc, baselines.corrupt_score, baselines.clean_score)\n",
        "\n",
        "        alphas_out.append(a)\n",
        "        scores.append(sc)\n",
        "        restorations.append(r)\n",
        "\n",
        "    a50 = estimate_alpha50(alphas_out, restorations)\n",
        "\n",
        "    return Curve(\n",
        "        coord=coord,\n",
        "        alphas=alphas_out,\n",
        "        scores=scores,\n",
        "        restorations=restorations,\n",
        "        alpha50=a50,\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_restoration_curves(\n",
        "    curves: Sequence[Curve],\n",
        "    *,\n",
        "    out_path: str = \"extra2_interpolation_curves.png\",\n",
        "    title: str = \"EXTRA 2: Interpolation sweep (normalized restoration R(alpha))\",\n",
        ") -> str:\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure()\n",
        "    for c in curves:\n",
        "        L, P = c.coord\n",
        "        plt.plot(c.alphas, c.restorations, marker=\"o\", label=f\"(L={L}, P={P})\")\n",
        "\n",
        "    plt.axhline(0.5, linestyle=\"--\")\n",
        "    plt.xlabel(\"alpha\")\n",
        "    plt.ylabel(\"R(alpha)\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "    return out_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing extra2_interpolation_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile extra2_interpolation_driver.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "import interpolation_sweep as isweep\n",
        "\n",
        "\n",
        "def parse_coords(s: str) -> List[Tuple[int, int]]:\n",
        "    # \"L:P,L:P\" -> [(L,P),...]\n",
        "    out = []\n",
        "    s = s.strip()\n",
        "    if not s:\n",
        "        return out\n",
        "    for part in s.split(\",\"):\n",
        "        Ls, Ps = part.strip().split(\":\")\n",
        "        out.append((int(Ls), int(Ps)))\n",
        "    return out\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--clean\", type=str, default=\"Michelle Jones was a top-notch student. Michelle\")\n",
        "    p.add_argument(\"--corrupt\", type=str, default=\"Michelle Smith was a top-notch student. Michelle\")\n",
        "    p.add_argument(\"--token_a\", type=str, default=\" Jones\")   # clean-consistent\n",
        "    p.add_argument(\"--token_b\", type=str, default=\" Smith\")   # corrupt-consistent\n",
        "    p.add_argument(\"--alphas\", type=str, default=\"0,0.25,0.5,0.75,1\")\n",
        "    p.add_argument(\"--top_k\", type=int, default=3)\n",
        "    p.add_argument(\"--coords\", type=str, default=\"\", help=\"Optional manual coords 'L:P,L:P,...' (skips hotspot search)\")\n",
        "    p.add_argument(\"--out\", type=str, default=\"extra2_interpolation_curves.png\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    alphas = [float(x.strip()) for x in args.alphas.split(\",\") if x.strip()]\n",
        "\n",
        "    # 1) baselines + cache clean activations\n",
        "    base = isweep.compute_baselines(\n",
        "        model,\n",
        "        bpe,\n",
        "        clean_text=args.clean,\n",
        "        corrupt_text=args.corrupt,\n",
        "        token_a_str=args.token_a,\n",
        "        token_b_str=args.token_b,\n",
        "        device=device,\n",
        "        overwrite_cache=True,\n",
        "    )\n",
        "    print(\"\\n=== Baselines ===\")\n",
        "    print(f\"score_clean  = {base.clean_score:.4f}\")\n",
        "    print(f\"score_corr   = {base.corrupt_score:.4f}\")\n",
        "    print(f\"Seq len T    = {base.seq_len}\")\n",
        "    print(f\"n_layer      = {base.n_layer}\")\n",
        "\n",
        "    manual_coords = parse_coords(args.coords)\n",
        "    if manual_coords:\n",
        "        hotspots = manual_coords\n",
        "        coldspot = manual_coords[-1]\n",
        "        print(\"\\nUsing manual coords:\", hotspots)\n",
        "    else:\n",
        "        # 2) compute alpha=1 patch matrix (standard heatmap values)\n",
        "        print(\"\\nComputing full alpha=1 patch matrix (for hotspot selection)...\")\n",
        "        M = isweep.full_patch_matrix(model, bpe, args.corrupt, base, device=device)\n",
        "\n",
        "        # 3) pick hotspots + coldspot\n",
        "        hotspots, coldspot = isweep.select_hotspots(M, base, top_k=args.top_k)\n",
        "        print(\"Hotspots:\", hotspots)\n",
        "        print(\"Coldspot:\", coldspot)\n",
        "\n",
        "    # 4) interpolation sweeps\n",
        "    curves = []\n",
        "    for c in hotspots:\n",
        "        curves.append(isweep.interpolation_curve(model, bpe, args.corrupt, base, c, alphas, device=device))\n",
        "    # add control curve\n",
        "    if coldspot not in hotspots:\n",
        "        curves.append(isweep.interpolation_curve(model, bpe, args.corrupt, base, coldspot, alphas, device=device))\n",
        "\n",
        "    print(\"\\n=== Curves (R(alpha)) ===\")\n",
        "    for cv in curves:\n",
        "        L, P = cv.coord\n",
        "        print(f\"\\nCoord (L={L}, P={P}) alpha50={cv.alpha50}\")\n",
        "        for a, s, r in zip(cv.alphas, cv.scores, cv.restorations):\n",
        "            print(f\"  alpha={a:>4.2f}  score={s:>8.4f}  R={r:>8.4f}\")\n",
        "\n",
        "    # 5) plot\n",
        "    out_path = isweep.plot_restoration_curves(curves, out_path=args.out)\n",
        "    print(f\"\\nSaved plot to: {out_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_extra2_interpolation.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_extra2_interpolation.py\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "from mingpt.model import GPT\n",
        "\n",
        "\n",
        "def _make_tiny():\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = 200\n",
        "    cfg.block_size = 32\n",
        "    model = GPT(cfg).eval()\n",
        "    return model, cfg\n",
        "\n",
        "\n",
        "def _make_clean_corrupt(cfg, T=12, changed_pos=3):\n",
        "    torch.manual_seed(0)\n",
        "    clean = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "    corrupt = clean.clone()\n",
        "    corrupt[0, changed_pos] = (corrupt[0, changed_pos] + 1) % cfg.vocab_size\n",
        "    return clean, corrupt, changed_pos\n",
        "\n",
        "\n",
        "def test_patch_alpha_bounds_enforced():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt, P = _make_clean_corrupt(cfg, T=10, changed_pos=3)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    with pytest.raises(ValueError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(corrupt, layer_to_patch=0, position_to_patch=P, patch_alpha=-0.1)\n",
        "\n",
        "    with pytest.raises(ValueError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(corrupt, layer_to_patch=0, position_to_patch=P, patch_alpha=1.1)\n",
        "\n",
        "\n",
        "def test_alpha0_is_noop_matches_corrupted_baseline_logits():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt, P = _make_clean_corrupt(cfg, T=12, changed_pos=3)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt)  # baseline\n",
        "        base_last = model.last_logits.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt, layer_to_patch=0, position_to_patch=P, patch_alpha=0.0)\n",
        "        patched_last = model.last_logits.clone()\n",
        "\n",
        "    assert torch.allclose(base_last, patched_last, rtol=1e-6, atol=1e-7)\n",
        "    assert model.last_patch_alpha == 0.0\n",
        "\n",
        "\n",
        "def test_alpha1_sets_activation_equal_to_clean_cache_at_that_cell():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt, P = _make_clean_corrupt(cfg, T=12, changed_pos=3)\n",
        "    L = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt, record_activations=True, layer_to_patch=L, position_to_patch=P, patch_alpha=1.0)\n",
        "\n",
        "    patched_acts = model.last_activations\n",
        "    assert patched_acts is not None\n",
        "    assert torch.allclose(\n",
        "        patched_acts[L][P],\n",
        "        model.clean_activations[L][P],\n",
        "        rtol=1e-5,\n",
        "        atol=1e-6,\n",
        "    )\n",
        "    assert model.last_patch == (L, P)\n",
        "    assert model.last_patch_source == (L, P)\n",
        "    assert model.last_patch_alpha == 1.0\n",
        "\n",
        "\n",
        "def test_alpha_half_is_exact_convex_combination_of_clean_and_corrupted_vectors():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt, P = _make_clean_corrupt(cfg, T=12, changed_pos=3)\n",
        "    L = 0\n",
        "    alpha = 0.5\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    # Record corrupted activations (baseline)\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt, record_activations=True)\n",
        "    base_acts = model.last_activations\n",
        "    assert base_acts is not None\n",
        "\n",
        "    # Patched run with alpha=0.5\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt, record_activations=True, layer_to_patch=L, position_to_patch=P, patch_alpha=alpha)\n",
        "    patched_acts = model.last_activations\n",
        "    assert patched_acts is not None\n",
        "\n",
        "    expected = (alpha * model.clean_activations[L][P]) + ((1.0 - alpha) * base_acts[L][P])\n",
        "    assert torch.allclose(patched_acts[L][P], expected, rtol=1e-5, atol=1e-6)\n",
        "    assert model.last_patch_alpha == alpha\n",
        "\n",
        "\n",
        "def test_wrong_source_plus_interpolation_uses_clean_source_in_mixture():\n",
        "    model, cfg = _make_tiny()\n",
        "    clean, corrupt, P = _make_clean_corrupt(cfg, T=12, changed_pos=3)\n",
        "    L = 0\n",
        "    srcP = 0\n",
        "    alpha = 0.25\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(corrupt, record_activations=True)\n",
        "    base_acts = model.last_activations\n",
        "    assert base_acts is not None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(\n",
        "            corrupt,\n",
        "            record_activations=True,\n",
        "            layer_to_patch=L,\n",
        "            position_to_patch=P,\n",
        "            source_layer=L,\n",
        "            source_position=srcP,\n",
        "            patch_alpha=alpha,\n",
        "        )\n",
        "    patched_acts = model.last_activations\n",
        "    assert patched_acts is not None\n",
        "\n",
        "    expected = (alpha * model.clean_activations[L][srcP]) + ((1.0 - alpha) * base_acts[L][P])\n",
        "    assert torch.allclose(patched_acts[L][P], expected, rtol=1e-5, atol=1e-6)\n",
        "\n",
        "    assert model.last_patch == (L, P)\n",
        "    assert model.last_patch_source == (L, srcP)\n",
        "    assert model.last_patch_alpha == alpha\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "number of parameters: 124.44M\n",
            "\n",
            "=== Baselines ===\n",
            "score_clean  = -4.1241\n",
            "score_corr   = 5.6562\n",
            "Seq len T    = 11\n",
            "n_layer      = 12\n",
            "\n",
            "Computing full alpha=1 patch matrix (for hotspot selection)...\n",
            "Hotspots: [(11, 10), (0, 1), (1, 1)]\n",
            "Coldspot: (0, 0)\n",
            "\n",
            "=== Curves (R(alpha)) ===\n",
            "\n",
            "Coord (L=11, P=10) alpha50=0.5127979048181796\n",
            "  alpha=0.00  score=  5.6562  R= -0.0000\n",
            "  alpha=0.25  score=  3.3084  R=  0.2401\n",
            "  alpha=0.50  score=  0.8930  R=  0.4870\n",
            "  alpha=0.75  score= -1.5861  R=  0.7405\n",
            "  alpha=1.00  score= -4.1241  R=  1.0000\n",
            "\n",
            "Coord (L=0, P=1) alpha50=0.5414549996037235\n",
            "  alpha=0.00  score=  5.6562  R= -0.0000\n",
            "  alpha=0.25  score=  3.8970  R=  0.1799\n",
            "  alpha=0.50  score=  1.2609  R=  0.4494\n",
            "  alpha=0.75  score= -1.7233  R=  0.7545\n",
            "  alpha=1.00  score= -4.0691  R=  0.9944\n",
            "\n",
            "Coord (L=1, P=1) alpha50=0.532609123057202\n",
            "  alpha=0.00  score=  5.6562  R= -0.0000\n",
            "  alpha=0.25  score=  3.7949  R=  0.1903\n",
            "  alpha=0.50  score=  1.1457  R=  0.4612\n",
            "  alpha=0.75  score= -1.7647  R=  0.7588\n",
            "  alpha=1.00  score= -4.0673  R=  0.9942\n",
            "\n",
            "Coord (L=0, P=0) alpha50=None\n",
            "  alpha=0.00  score=  5.6562  R= -0.0000\n",
            "  alpha=0.25  score=  5.6562  R= -0.0000\n",
            "  alpha=0.50  score=  5.6562  R= -0.0000\n",
            "  alpha=0.75  score=  5.6562  R= -0.0000\n",
            "  alpha=1.00  score=  5.6562  R= -0.0000\n",
            "\n",
            "Saved plot to: extra2_interpolation_curves.png\n"
          ]
        }
      ],
      "source": [
        "!python extra2_interpolation_driver.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                    [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 3.03s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -q test_extra2_interpolation.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RUN EXTRA SECTION 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing interpolation_sweep_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile interpolation_sweep_driver.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "import tokenization_protocol as tp  # you already have this\n",
        "from baseline_utils import single_token_id  # you already have this\n",
        "\n",
        "\n",
        "def parse_alphas(xs: List[str]) -> List[float]:\n",
        "    return [float(x) for x in xs]\n",
        "\n",
        "\n",
        "def score_from_last_logits(last_logits_1d: torch.Tensor, token_a_id: int, token_b_id: int) -> float:\n",
        "    # score = logit(B) - logit(A)\n",
        "    return float(last_logits_1d[token_b_id] - last_logits_1d[token_a_id])\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_once(\n",
        "    model: GPT,\n",
        "    idx: torch.Tensor,\n",
        "    *,\n",
        "    cache_clean: bool = False,\n",
        "    overwrite_cache: bool = False,\n",
        "    record_acts: bool = False,\n",
        "    layer_to_patch: Optional[int] = None,\n",
        "    position_to_patch: Optional[int] = None,\n",
        "    patch_alpha: Optional[float] = None,\n",
        ") -> Tuple[torch.Tensor, Optional[List[List[torch.Tensor]]]]:\n",
        "    logits, _ = model(\n",
        "        idx,\n",
        "        record_activations=record_acts,\n",
        "        cache_activations=cache_clean,\n",
        "        overwrite_cache=overwrite_cache,\n",
        "        layer_to_patch=layer_to_patch,\n",
        "        position_to_patch=position_to_patch,\n",
        "        patch_alpha=patch_alpha,\n",
        "    )\n",
        "    if model.last_logits is None:\n",
        "        raise RuntimeError(\"model.last_logits was not set.\")\n",
        "    acts = model.last_activations if record_acts else None\n",
        "    return model.last_logits[0].detach().clone(), acts\n",
        "\n",
        "\n",
        "def pick_best_layer_at_changed_pos(\n",
        "    model: GPT,\n",
        "    idx_corrupt: torch.Tensor,\n",
        "    token_a_id: int,\n",
        "    token_b_id: int,\n",
        "    *,\n",
        "    changed_pos: int,\n",
        "    score_corr: float,\n",
        ") -> int:\n",
        "    n_layer = len(model.transformer.h)\n",
        "    best_L = 0\n",
        "    best_restoration = -1e9\n",
        "\n",
        "    # We choose the layer whose full patch (alpha=1) moves score the most away from corrupted baseline\n",
        "    # toward *anything* different (restoration magnitude).\n",
        "    for L in range(n_layer):\n",
        "        last, _ = run_once(\n",
        "            model,\n",
        "            idx_corrupt,\n",
        "            layer_to_patch=L,\n",
        "            position_to_patch=changed_pos,\n",
        "            patch_alpha=1.0,\n",
        "        )\n",
        "        s = score_from_last_logits(last, token_a_id, token_b_id)\n",
        "        restoration = abs(s - score_corr)\n",
        "        if restoration > best_restoration:\n",
        "            best_restoration = restoration\n",
        "            best_L = L\n",
        "\n",
        "    return best_L\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--clean\", type=str, default=\"Michelle Jones was a top-notch student. Michelle\")\n",
        "    ap.add_argument(\"--corrupt\", type=str, default=\"Michelle Smith was a top-notch student. Michelle\")\n",
        "    ap.add_argument(\"--token_a\", type=str, default=\" Jones\")\n",
        "    ap.add_argument(\"--token_b\", type=str, default=\" Smith\")\n",
        "\n",
        "    ap.add_argument(\"--layer\", type=int, default=-1, help=\"Target layer L. If -1, auto-pick best L at changed token position.\")\n",
        "    ap.add_argument(\"--pos\", type=int, default=-1, help=\"Target position P. If -1, use changed token position.\")\n",
        "    ap.add_argument(\"--alphas\", nargs=\"+\", default=[\"0\", \"0.25\", \"0.5\", \"0.75\", \"1\"])\n",
        "\n",
        "    ap.add_argument(\"--device\", type=str, default=None)\n",
        "    ap.add_argument(\"--check_mixture\", action=\"store_true\", help=\"Also verify x_patched  x_clean+(1-)x_corr using recorded activations.\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    device = args.device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    # 1) Tokenization validation\n",
        "    clean_rep = tp.build_report(bpe, args.clean)\n",
        "    corrupt_rep = tp.build_report(bpe, args.corrupt)\n",
        "    comp = tp.compare_clean_corrupt(clean_rep, corrupt_rep)\n",
        "\n",
        "    if not comp.same_length:\n",
        "        raise RuntimeError(f\"Token length mismatch: clean={clean_rep.seq_len}, corrupt={corrupt_rep.seq_len}\")\n",
        "    if comp.diff_count != 1:\n",
        "        raise RuntimeError(f\"Expected exactly 1 differing token position; got {comp.diff_count}: {comp.diff_positions}\")\n",
        "\n",
        "    changed_pos = int(comp.diff_positions[0])\n",
        "    print(f\"Seq len T={clean_rep.seq_len}, changed token position={changed_pos}\")\n",
        "\n",
        "    # 2) Load model\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "\n",
        "    # Token ids for metric\n",
        "    token_a_id = single_token_id(bpe, args.token_a)\n",
        "    token_b_id = single_token_id(bpe, args.token_b)\n",
        "\n",
        "    # 3) Build tensors\n",
        "    idx_clean = bpe(args.clean).to(device)\n",
        "    idx_corrupt = bpe(args.corrupt).to(device)\n",
        "\n",
        "    # 4) Clean baseline (cache activations)\n",
        "    last_clean, _ = run_once(model, idx_clean, cache_clean=True, overwrite_cache=True)\n",
        "    score_clean = score_from_last_logits(last_clean, token_a_id, token_b_id)\n",
        "\n",
        "    # 5) Corrupt baseline\n",
        "    last_corr, acts_corr = run_once(model, idx_corrupt, record_acts=args.check_mixture)\n",
        "    score_corr = score_from_last_logits(last_corr, token_a_id, token_b_id)\n",
        "\n",
        "    print(\"\\n=== Baselines ===\")\n",
        "    print(f\"score_clean  = {score_clean:.6f}\")\n",
        "    print(f\"score_corr   = {score_corr:.6f}\")\n",
        "    print(f\"delta(corr-clean) = {score_corr - score_clean:.6f}\")\n",
        "\n",
        "    # 6) Choose (L,P)\n",
        "    P = changed_pos if args.pos < 0 else int(args.pos)\n",
        "    if args.layer < 0:\n",
        "        L = pick_best_layer_at_changed_pos(model, idx_corrupt, token_a_id, token_b_id, changed_pos=P, score_corr=score_corr)\n",
        "        print(f\"\\nAuto-picked layer L={L} at position P={P}\")\n",
        "    else:\n",
        "        L = int(args.layer)\n",
        "        print(f\"\\nUsing provided (L,P)=({L},{P})\")\n",
        "\n",
        "    alphas = parse_alphas(args.alphas)\n",
        "\n",
        "    # 7) Full patch score (alpha=1) used for endpoint check\n",
        "    last_full, _ = run_once(model, idx_corrupt, layer_to_patch=L, position_to_patch=P, patch_alpha=1.0)\n",
        "    score_full = score_from_last_logits(last_full, token_a_id, token_b_id)\n",
        "\n",
        "    # 8) Sweep\n",
        "    print(\"\\n=== Interpolation sweep ===\")\n",
        "    print(\"alpha | score(alpha) | R_hat_vs_full | meta(last_patch,last_alpha)\")\n",
        "    print(\"-\"*78)\n",
        "\n",
        "    eps_score = 1e-5\n",
        "    base_denom = (score_full - score_corr)\n",
        "\n",
        "    # Optional: mixture check uses recorded activations\n",
        "    clean_vec = model.clean_activations[L][P].to(device)\n",
        "\n",
        "    for a in alphas:\n",
        "        record = bool(args.check_mixture)\n",
        "        last_a, acts_a = run_once(\n",
        "            model,\n",
        "            idx_corrupt,\n",
        "            record_acts=record,\n",
        "            layer_to_patch=L,\n",
        "            position_to_patch=P,\n",
        "            patch_alpha=a,\n",
        "        )\n",
        "        s = score_from_last_logits(last_a, token_a_id, token_b_id)\n",
        "\n",
        "        # normalized w.r.t full patch (so endpoints are guaranteed to be 0 and 1 if correct)\n",
        "        if abs(base_denom) < 1e-12:\n",
        "            rhat = float(\"nan\")\n",
        "        else:\n",
        "            rhat = (s - score_corr) / base_denom\n",
        "\n",
        "        meta = (model.last_patch, model.last_patch_alpha)\n",
        "        print(f\"{a:>4.2f} | {s:>11.6f} | {rhat:>12.6f} | {meta}\")\n",
        "\n",
        "        # Mixture check (activation-level)\n",
        "        if args.check_mixture:\n",
        "            if acts_corr is None or acts_a is None:\n",
        "                raise RuntimeError(\"Mixture check requested but activations were not recorded.\")\n",
        "            x_corr = acts_corr[L][P].to(device)\n",
        "            x_pat = acts_a[L][P].to(device)\n",
        "            target = (a * clean_vec) + ((1.0 - a) * x_corr)\n",
        "            max_err = float((x_pat - target).abs().max().item())\n",
        "            if max_err > 1e-4:\n",
        "                raise RuntimeError(f\"Mixture check FAILED at alpha={a}: max|x_patched - mix| = {max_err:.6e}\")\n",
        "\n",
        "    # 9) Endpoint checks\n",
        "    # alpha=0\n",
        "    last_0, _ = run_once(model, idx_corrupt, layer_to_patch=L, position_to_patch=P, patch_alpha=0.0)\n",
        "    score_0 = score_from_last_logits(last_0, token_a_id, token_b_id)\n",
        "\n",
        "    # alpha=1\n",
        "    last_1, _ = run_once(model, idx_corrupt, layer_to_patch=L, position_to_patch=P, patch_alpha=1.0)\n",
        "    score_1 = score_from_last_logits(last_1, token_a_id, token_b_id)\n",
        "\n",
        "    print(\"\\n=== Endpoint checks ===\")\n",
        "    print(f\"score(alpha=0) = {score_0:.6f}  vs score_corr = {score_corr:.6f}\")\n",
        "    print(f\"score(alpha=1) = {score_1:.6f}  vs score_full = {score_full:.6f}\")\n",
        "\n",
        "    if abs(score_0 - score_corr) > eps_score:\n",
        "        raise RuntimeError(\"FAILED endpoint check =0: score(0) != score_corr (patch should be a no-op).\")\n",
        "    if abs(score_1 - score_full) > eps_score:\n",
        "        raise RuntimeError(\"FAILED endpoint check =1: score(1) != score_full (must match standard patch).\")\n",
        "\n",
        "    print(\"\\n EXTRA 2 looks correct: endpoints match and sweep ran successfully.\")\n",
        "    if args.check_mixture:\n",
        "        print(\" Mixture check passed: activations match x_clean+(1-)x_corr at the patched coordinate.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Seq len T=11, changed token position=1\n",
            "number of parameters: 124.44M\n",
            "\n",
            "=== Baselines ===\n",
            "score_clean  = -4.124077\n",
            "score_corr   = 5.656242\n",
            "delta(corr-clean) = 9.780319\n",
            "\n",
            "Auto-picked layer L=0 at position P=1\n",
            "\n",
            "=== Interpolation sweep ===\n",
            "alpha | score(alpha) | R_hat_vs_full | meta(last_patch,last_alpha)\n",
            "------------------------------------------------------------------------------\n",
            "0.00 |    5.656242 |    -0.000000 | ((0, 1), 0.0)\n",
            "0.25 |    3.897049 |     0.180887 | ((0, 1), 0.25)\n",
            "0.50 |    1.260918 |     0.451945 | ((0, 1), 0.5)\n",
            "0.75 |   -1.723251 |     0.758789 | ((0, 1), 0.75)\n",
            "1.00 |   -4.069115 |     1.000000 | ((0, 1), 1.0)\n",
            "\n",
            "=== Endpoint checks ===\n",
            "score(alpha=0) = 5.656242  vs score_corr = 5.656242\n",
            "score(alpha=1) = -4.069115  vs score_full = -4.069115\n",
            "\n",
            " EXTRA 2 looks correct: endpoints match and sweep ran successfully.\n",
            " Mixture check passed: activations match x_clean+(1-)x_corr at the patched coordinate.\n"
          ]
        }
      ],
      "source": [
        "!python interpolation_sweep_driver.py --check_mixture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Seq len T=11, changed token position=1\n",
            "number of parameters: 124.44M\n",
            "\n",
            "=== Baselines ===\n",
            "score_clean  = -4.124077\n",
            "score_corr   = 5.656242\n",
            "delta(corr-clean) = 9.780319\n",
            "\n",
            "Using provided (L,P)=(5,1)\n",
            "\n",
            "=== Interpolation sweep ===\n",
            "alpha | score(alpha) | R_hat_vs_full | meta(last_patch,last_alpha)\n",
            "------------------------------------------------------------------------------\n",
            "0.00 |    5.656242 |    -0.000000 | ((5, 1), 0.0)\n",
            "0.25 |    3.700050 |     0.208917 | ((5, 1), 0.25)\n",
            "0.50 |    1.173576 |     0.478738 | ((5, 1), 0.5)\n",
            "0.75 |   -1.493301 |     0.763554 | ((5, 1), 0.75)\n",
            "1.00 |   -3.707260 |     1.000000 | ((5, 1), 1.0)\n",
            "\n",
            "=== Endpoint checks ===\n",
            "score(alpha=0) = 5.656242  vs score_corr = 5.656242\n",
            "score(alpha=1) = -3.707260  vs score_full = -3.707260\n",
            "\n",
            " EXTRA 2 looks correct: endpoints match and sweep ran successfully.\n",
            " Mixture check passed: activations match x_clean+(1-)x_corr at the patched coordinate.\n"
          ]
        }
      ],
      "source": [
        "!python interpolation_sweep_driver.py --layer 5 --pos 1 --alphas 0 0.25 0.5 0.75 1 --check_mixture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EXTRA SECTION 3: Patch inside the block: after attention vs after MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing extra3_intrablock_sweep.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile extra3_intrablock_sweep.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Sequence, Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def _infer_device(model: torch.nn.Module) -> torch.device:\n",
        "    try:\n",
        "        return next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def single_token_id(bpe, token_str: str) -> int:\n",
        "    ids = bpe(token_str)[0].tolist()\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(\n",
        "            f\"Target token string must map to exactly 1 BPE token. \"\n",
        "            f\"Got {len(ids)} tokens for {repr(token_str)}: {ids}\"\n",
        "        )\n",
        "    return int(ids[0])\n",
        "\n",
        "\n",
        "def logit_diff_from_last_logits(last_logits_1d: torch.Tensor, *, token_a_id: int, token_b_id: int) -> float:\n",
        "    a = float(last_logits_1d[token_a_id])\n",
        "    b = float(last_logits_1d[token_b_id])\n",
        "    return b - a\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Extra3Result:\n",
        "    post_attn_matrix: torch.Tensor  # (n_layers, T) on CPU float32\n",
        "    post_mlp_matrix: torch.Tensor   # (n_layers, T) on CPU float32\n",
        "    n_layers: int\n",
        "    seq_len: int\n",
        "    clean_score: float\n",
        "    corrupt_score: float\n",
        "    token_a_str: str\n",
        "    token_b_str: str\n",
        "    token_a_id: int\n",
        "    token_b_id: int\n",
        "    clean_text: str\n",
        "    corrupt_text: str\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sweep_location_from_ids(\n",
        "    model,\n",
        "    idx_corrupt: torch.LongTensor,\n",
        "    *,\n",
        "    token_a_id: int,\n",
        "    token_b_id: int,\n",
        "    patch_location: str,\n",
        "    layers: Optional[Sequence[int]] = None,\n",
        "    positions: Optional[Sequence[int]] = None,\n",
        "    progress: bool = False,\n",
        ") -> torch.Tensor:\n",
        "    if idx_corrupt.ndim != 2 or idx_corrupt.shape[0] != 1:\n",
        "        raise ValueError(f\"Expected idx_corrupt shape (1,T). Got {tuple(idx_corrupt.shape)}\")\n",
        "\n",
        "    device = _infer_device(model)\n",
        "    idx_corrupt = idx_corrupt.to(device)\n",
        "\n",
        "    n_layers = len(model.transformer.h)\n",
        "    T = int(idx_corrupt.shape[1])\n",
        "\n",
        "    layers = list(range(n_layers)) if layers is None else list(layers)\n",
        "    positions = list(range(T)) if positions is None else list(positions)\n",
        "\n",
        "    it = [(L, P) for L in layers for P in positions]\n",
        "    if progress:\n",
        "        try:\n",
        "            from tqdm import tqdm  # type: ignore\n",
        "            it = tqdm(it, desc=f\"sweep({patch_location})\", total=len(it))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    mat = torch.empty((len(layers), len(positions)), dtype=torch.float32, device=\"cpu\")\n",
        "    layer_index = {L: i for i, L in enumerate(layers)}\n",
        "    pos_index = {P: j for j, P in enumerate(positions)}\n",
        "\n",
        "    for L, P in it:\n",
        "        _logits, _loss = model(\n",
        "            idx_corrupt,\n",
        "            layer_to_patch=int(L),\n",
        "            position_to_patch=int(P),\n",
        "            patch_location=patch_location,\n",
        "        )\n",
        "        if model.last_logits is None:\n",
        "            raise RuntimeError(\"model.last_logits was not set. Ensure forward() stores last_logits.\")\n",
        "        last = model.last_logits[0].detach()\n",
        "        score = logit_diff_from_last_logits(last, token_a_id=token_a_id, token_b_id=token_b_id)\n",
        "        mat[layer_index[L], pos_index[P]] = float(score)\n",
        "\n",
        "    return mat\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_extra3(\n",
        "    model,\n",
        "    bpe,\n",
        "    *,\n",
        "    clean_text: str,\n",
        "    corrupt_text: str,\n",
        "    token_a_str: str,\n",
        "    token_b_str: str,\n",
        "    overwrite_cache: bool = True,\n",
        "    progress: bool = True,\n",
        ") -> Extra3Result:\n",
        "    device = _infer_device(model)\n",
        "\n",
        "    idx_clean = bpe(clean_text).to(device)\n",
        "    idx_corr = bpe(corrupt_text).to(device)\n",
        "\n",
        "    if idx_clean.shape != idx_corr.shape:\n",
        "        raise ValueError(\n",
        "            f\"Clean/Corrupt token length mismatch: clean={tuple(idx_clean.shape)}, corrupt={tuple(idx_corr.shape)}. \"\n",
        "            \"They MUST have the same number of BPE tokens.\"\n",
        "        )\n",
        "\n",
        "    T = int(idx_clean.shape[1])\n",
        "    n_layers = len(model.transformer.h)\n",
        "\n",
        "    token_a_id = single_token_id(bpe, token_a_str)\n",
        "    token_b_id = single_token_id(bpe, token_b_str)\n",
        "\n",
        "    # 1) clean run: cache BOTH post_attn and post_mlp\n",
        "    _ = model(idx_clean, cache_activations=True, overwrite_cache=overwrite_cache)\n",
        "    if model.last_logits is None:\n",
        "        raise RuntimeError(\"model.last_logits missing after clean run.\")\n",
        "    clean_score = logit_diff_from_last_logits(model.last_logits[0], token_a_id=token_a_id, token_b_id=token_b_id)\n",
        "\n",
        "    # 2) corrupted baseline\n",
        "    _ = model(idx_corr)\n",
        "    if model.last_logits is None:\n",
        "        raise RuntimeError(\"model.last_logits missing after corrupt run.\")\n",
        "    corrupt_score = logit_diff_from_last_logits(model.last_logits[0], token_a_id=token_a_id, token_b_id=token_b_id)\n",
        "\n",
        "    # 3) sweeps\n",
        "    post_attn = sweep_location_from_ids(\n",
        "        model,\n",
        "        idx_corr,\n",
        "        token_a_id=token_a_id,\n",
        "        token_b_id=token_b_id,\n",
        "        patch_location=\"post_attn\",\n",
        "        progress=progress,\n",
        "    )\n",
        "    post_mlp = sweep_location_from_ids(\n",
        "        model,\n",
        "        idx_corr,\n",
        "        token_a_id=token_a_id,\n",
        "        token_b_id=token_b_id,\n",
        "        patch_location=\"post_mlp\",\n",
        "        progress=progress,\n",
        "    )\n",
        "\n",
        "    return Extra3Result(\n",
        "        post_attn_matrix=post_attn,\n",
        "        post_mlp_matrix=post_mlp,\n",
        "        n_layers=n_layers,\n",
        "        seq_len=T,\n",
        "        clean_score=float(clean_score),\n",
        "        corrupt_score=float(corrupt_score),\n",
        "        token_a_str=token_a_str,\n",
        "        token_b_str=token_b_str,\n",
        "        token_a_id=token_a_id,\n",
        "        token_b_id=token_b_id,\n",
        "        clean_text=clean_text,\n",
        "        corrupt_text=corrupt_text,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing extra3_intrablock_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile extra3_intrablock_driver.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "import tokenization_protocol as tp\n",
        "\n",
        "from extra3_intrablock_sweep import run_extra3\n",
        "from section10_visualization import (\n",
        "    HeatmapMeta,\n",
        "    decode_prompt_token_labels,\n",
        "    plot_logit_diff_heatmap,\n",
        "    save_figure_publication_quality,\n",
        "    save_heatmap_artifacts,\n",
        ")\n",
        "\n",
        "\n",
        "def get_device() -> str:\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    set_seed(3407)\n",
        "    device = get_device()\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # -------------------------\n",
        "    # EDIT THESE (your experiment)\n",
        "    # -------------------------\n",
        "    CLEAN_TEXT = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    CORRUPT_TEXT = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "    TOKEN_A = \" Jones\"  # clean-consistent\n",
        "    TOKEN_B = \" Smith\"  # corrupt-consistent\n",
        "\n",
        "    # Load model + tokenizer\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    # Validate token constraints (recommended)\n",
        "    comp = tp.validate_pair(\n",
        "        bpe=bpe,\n",
        "        clean_text=CLEAN_TEXT,\n",
        "        corrupt_text=CORRUPT_TEXT,\n",
        "        require_same_length=True,\n",
        "        require_one_token_diff=True,\n",
        "    )\n",
        "    print(tp.describe_pair(comp))\n",
        "    print(\"Changed token position:\", comp.diff_positions[0])\n",
        "\n",
        "    # Run EXTRA 3\n",
        "    res = run_extra3(\n",
        "        model,\n",
        "        bpe,\n",
        "        clean_text=CLEAN_TEXT,\n",
        "        corrupt_text=CORRUPT_TEXT,\n",
        "        token_a_str=TOKEN_A,\n",
        "        token_b_str=TOKEN_B,\n",
        "        overwrite_cache=True,\n",
        "        progress=True,\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Baselines ===\")\n",
        "    print(f\"clean score   = {res.clean_score:.4f}\")\n",
        "    print(f\"corrupt score = {res.corrupt_score:.4f}\")\n",
        "    print(f\"delta (corrupt-clean) = {res.corrupt_score - res.clean_score:.4f}\")\n",
        "\n",
        "    out_dir = Path(\"artifacts/extra3_intrablock\")\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    token_labels = decode_prompt_token_labels(bpe, CLEAN_TEXT)\n",
        "\n",
        "    # Save raw matrices for reproducibility\n",
        "    torch.save(\n",
        "        {\n",
        "            \"post_attn\": res.post_attn_matrix,\n",
        "            \"post_mlp\": res.post_mlp_matrix,\n",
        "            \"clean_score\": res.clean_score,\n",
        "            \"corrupt_score\": res.corrupt_score,\n",
        "            \"token_a\": res.token_a_str,\n",
        "            \"token_b\": res.token_b_str,\n",
        "            \"clean_text\": res.clean_text,\n",
        "            \"corrupt_text\": res.corrupt_text,\n",
        "            \"seq_len\": res.seq_len,\n",
        "            \"n_layers\": res.n_layers,\n",
        "        },\n",
        "        out_dir / \"extra3_matrices.pt\",\n",
        "    )\n",
        "    print(\"Saved:\", (out_dir / \"extra3_matrices.pt\").resolve())\n",
        "\n",
        "    # ---- Plot: post-attn ----\n",
        "    title_attn = f\"EXTRA 3  post-attn patching: logit({repr(TOKEN_B)})  logit({repr(TOKEN_A)})\"\n",
        "    meta_attn = HeatmapMeta(\n",
        "        metric_title=title_attn,\n",
        "        clean_text=CLEAN_TEXT,\n",
        "        corrupt_text=CORRUPT_TEXT,\n",
        "        token_a_str=TOKEN_A,\n",
        "        token_b_str=TOKEN_B,\n",
        "        n_layers=res.n_layers,\n",
        "        seq_len=res.seq_len,\n",
        "        token_labels=token_labels,\n",
        "    )\n",
        "    save_heatmap_artifacts(out_dir=out_dir / \"post_attn\", matrix=res.post_attn_matrix, meta=meta_attn)\n",
        "    fig, _ = plot_logit_diff_heatmap(\n",
        "        res.post_attn_matrix,\n",
        "        token_labels=token_labels,\n",
        "        metric_title=title_attn,\n",
        "        show_token_strings=True,\n",
        "        center_zero=True,\n",
        "        include_pos_in_label=True,\n",
        "    )\n",
        "    save_figure_publication_quality(fig, out_basepath=out_dir / \"post_attn\" / \"heatmap_post_attn\", formats=(\"png\", \"pdf\"))\n",
        "\n",
        "    # ---- Plot: post-MLP ----\n",
        "    title_mlp = f\"EXTRA 3  post-MLP patching: logit({repr(TOKEN_B)})  logit({repr(TOKEN_A)})\"\n",
        "    meta_mlp = HeatmapMeta(\n",
        "        metric_title=title_mlp,\n",
        "        clean_text=CLEAN_TEXT,\n",
        "        corrupt_text=CORRUPT_TEXT,\n",
        "        token_a_str=TOKEN_A,\n",
        "        token_b_str=TOKEN_B,\n",
        "        n_layers=res.n_layers,\n",
        "        seq_len=res.seq_len,\n",
        "        token_labels=token_labels,\n",
        "    )\n",
        "    save_heatmap_artifacts(out_dir=out_dir / \"post_mlp\", matrix=res.post_mlp_matrix, meta=meta_mlp)\n",
        "    fig, _ = plot_logit_diff_heatmap(\n",
        "        res.post_mlp_matrix,\n",
        "        token_labels=token_labels,\n",
        "        metric_title=title_mlp,\n",
        "        show_token_strings=True,\n",
        "        center_zero=True,\n",
        "        include_pos_in_label=True,\n",
        "    )\n",
        "    save_figure_publication_quality(fig, out_basepath=out_dir / \"post_mlp\" / \"heatmap_post_mlp\", formats=(\"png\", \"pdf\"))\n",
        "\n",
        "    print(\"\\n Done. Heatmaps saved under:\", out_dir.resolve())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_extra3_intrablock.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_extra3_intrablock.py\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "\n",
        "def make_tiny_model():\n",
        "    set_seed(123)\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"   # tiny, no downloads\n",
        "    cfg.vocab_size = 101\n",
        "    cfg.block_size = 32\n",
        "    m = GPT(cfg).eval()\n",
        "    return m\n",
        "\n",
        "\n",
        "def make_pair(vocab_size: int, T: int = 12, changed_pos: int = 3):\n",
        "    set_seed(999)\n",
        "    clean = torch.randint(0, vocab_size, (1, T), dtype=torch.long)\n",
        "    corrupt = clean.clone()\n",
        "    corrupt[0, changed_pos] = (corrupt[0, changed_pos] + 1) % vocab_size\n",
        "    return clean, corrupt, changed_pos\n",
        "\n",
        "\n",
        "def test_caches_exist_for_both_locations_after_clean_cache():\n",
        "    m = make_tiny_model()\n",
        "    vocab = m.transformer.wte.num_embeddings\n",
        "    clean, _, _ = make_pair(vocab)\n",
        "\n",
        "    _ = m(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    assert m.clean_post_attn_activations is not None\n",
        "    assert m.clean_post_mlp_activations is not None\n",
        "    assert m.clean_activations is not None  # backward compatibility (post-MLP)\n",
        "\n",
        "    n_layers = len(m.transformer.h)\n",
        "    T = clean.shape[1]\n",
        "    assert len(m.clean_post_attn_activations) == n_layers\n",
        "    assert len(m.clean_post_attn_activations[0]) == T\n",
        "    assert len(m.clean_post_mlp_activations) == n_layers\n",
        "    assert len(m.clean_post_mlp_activations[0]) == T\n",
        "\n",
        "    # clean_activations should match post-MLP cache\n",
        "    assert torch.allclose(m.clean_activations[0][0], m.clean_post_mlp_activations[0][0])\n",
        "\n",
        "\n",
        "def test_patch_post_attn_changes_logits():\n",
        "    m = make_tiny_model()\n",
        "    vocab = m.transformer.wte.num_embeddings\n",
        "    clean, corrupt, p = make_pair(vocab)\n",
        "\n",
        "    _ = m(clean, cache_activations=True, overwrite_cache=True)\n",
        "    _ = m(corrupt)\n",
        "    base = m.last_logits.clone()\n",
        "\n",
        "    _ = m(corrupt, layer_to_patch=0, position_to_patch=p, patch_location=\"post_attn\")\n",
        "    assert m.last_patch_location == \"post_attn\"\n",
        "    assert m.last_patch == (0, p)\n",
        "    assert not torch.allclose(base, m.last_logits)\n",
        "\n",
        "\n",
        "def test_patch_post_mlp_changes_logits():\n",
        "    m = make_tiny_model()\n",
        "    vocab = m.transformer.wte.num_embeddings\n",
        "    clean, corrupt, p = make_pair(vocab)\n",
        "\n",
        "    _ = m(clean, cache_activations=True, overwrite_cache=True)\n",
        "    _ = m(corrupt)\n",
        "    base = m.last_logits.clone()\n",
        "\n",
        "    _ = m(corrupt, layer_to_patch=0, position_to_patch=p, patch_location=\"post_mlp\")\n",
        "    assert m.last_patch_location == \"post_mlp\"\n",
        "    assert m.last_patch == (0, p)\n",
        "    assert not torch.allclose(base, m.last_logits)\n",
        "\n",
        "\n",
        "def test_post_attn_and_post_mlp_patches_produce_different_outputs_typically():\n",
        "    m = make_tiny_model()\n",
        "    vocab = m.transformer.wte.num_embeddings\n",
        "    clean, corrupt, p = make_pair(vocab)\n",
        "\n",
        "    _ = m(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    _ = m(corrupt, layer_to_patch=1, position_to_patch=p, patch_location=\"post_attn\")\n",
        "    out_attn = m.last_logits.clone()\n",
        "\n",
        "    _ = m(corrupt, layer_to_patch=1, position_to_patch=p, patch_location=\"post_mlp\")\n",
        "    out_mlp = m.last_logits.clone()\n",
        "\n",
        "    # In a random network these should differ (very high probability).\n",
        "    assert not torch.allclose(out_attn, out_mlp)\n",
        "\n",
        "\n",
        "def test_invalid_patch_location_raises():\n",
        "    m = make_tiny_model()\n",
        "    vocab = m.transformer.wte.num_embeddings\n",
        "    clean, corrupt, p = make_pair(vocab)\n",
        "\n",
        "    _ = m(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    with pytest.raises(ValueError):\n",
        "        _ = m(corrupt, layer_to_patch=0, position_to_patch=p, patch_location=\"after_unicorns\")\n",
        "\n",
        "\n",
        "def test_patch_and_cache_activations_is_forbidden():\n",
        "    m = make_tiny_model()\n",
        "    vocab = m.transformer.wte.num_embeddings\n",
        "    clean, corrupt, p = make_pair(vocab)\n",
        "\n",
        "    _ = m(clean, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    with pytest.raises(RuntimeError):\n",
        "        _ = m(\n",
        "            corrupt,\n",
        "            layer_to_patch=0,\n",
        "            position_to_patch=p,\n",
        "            patch_location=\"post_mlp\",\n",
        "            cache_activations=True,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 69%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                          [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m103 passed\u001b[0m\u001b[32m in 44.80s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -q test_extra3_intrablock.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EXTRA SECTION 3 RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting extra3_intrablock_check.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile extra3_intrablock_check.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "# Same example as the handout (usually stable)\n",
        "CLEAN_TEXT   = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "CORRUPT_TEXT = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "TOKEN_A = \" Jones\"  # clean-consistent\n",
        "TOKEN_B = \" Smith\"  # corrupt-consistent\n",
        "\n",
        "def single_token_id(bpe: BPETokenizer, s: str) -> int:\n",
        "    ids = bpe(s)[0].tolist()\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{s!r} is not a single BPE token. Got ids={ids}\")\n",
        "    return int(ids[0])\n",
        "\n",
        "def score_from_last_logits(last_logits_1d: torch.Tensor, *, a_id: int, b_id: int) -> float:\n",
        "    # score = logit(B) - logit(A)\n",
        "    return float(last_logits_1d[b_id] - last_logits_1d[a_id])\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    idx_clean = bpe(CLEAN_TEXT).to(device)\n",
        "    idx_corr  = bpe(CORRUPT_TEXT).to(device)\n",
        "\n",
        "    if idx_clean.shape != idx_corr.shape:\n",
        "        raise RuntimeError(f\"Token length mismatch: clean={idx_clean.shape}, corrupt={idx_corr.shape}\")\n",
        "\n",
        "    T = idx_clean.shape[1]\n",
        "    n_layers = len(model.transformer.h)\n",
        "    print(f\"Seq len T={T}, n_layers={n_layers}\")\n",
        "\n",
        "    a_id = single_token_id(bpe, TOKEN_A)\n",
        "    b_id = single_token_id(bpe, TOKEN_B)\n",
        "\n",
        "    # 1) CLEAN run (this must create BOTH intra-block caches)\n",
        "    _ = model(idx_clean, cache_activations=True, overwrite_cache=True)\n",
        "    print(\"Clean caches present?\",\n",
        "          \"post_attn:\", model.clean_post_attn_activations is not None,\n",
        "          \"| post_mlp:\", model.clean_post_mlp_activations is not None)\n",
        "\n",
        "    # Show cache dimensions (must be 12 x T for GPT-2 small)\n",
        "    print(\"clean_post_attn dims:\", len(model.clean_post_attn_activations), \"x\", len(model.clean_post_attn_activations[0]))\n",
        "    print(\"clean_post_mlp  dims:\", len(model.clean_post_mlp_activations),  \"x\", len(model.clean_post_mlp_activations[0]))\n",
        "\n",
        "    score_clean = score_from_last_logits(model.last_logits[0], a_id=a_id, b_id=b_id)\n",
        "\n",
        "    # 2) CORR baseline (no patch)\n",
        "    _ = model(idx_corr)\n",
        "    score_corr = score_from_last_logits(model.last_logits[0], a_id=a_id, b_id=b_id)\n",
        "\n",
        "    print(\"\\n=== Baselines ===\")\n",
        "    print(f\"score_clean = {score_clean:.4f}\")\n",
        "    print(f\"score_corr  = {score_corr:.4f}\")\n",
        "    print(\"Expected (for this pair): score_clean < score_corr (often clean is negative, corrupt is positive).\")\n",
        "\n",
        "    # Choose a meaningful target: the changed-token position is usually 1 for this classic prompt,\n",
        "    # but well just test a couple positions safely.\n",
        "    test_L = 6\n",
        "    test_P = 1 if T > 1 else 0\n",
        "\n",
        "    # 3) Patch at post-attn\n",
        "    _ = model(idx_corr,\n",
        "              layer_to_patch=test_L,\n",
        "              position_to_patch=test_P,\n",
        "              patch_location=\"post_attn\")\n",
        "    s_attn = score_from_last_logits(model.last_logits[0], a_id=a_id, b_id=b_id)\n",
        "    print(\"\\n=== Patch test (post_attn) ===\")\n",
        "    print(\"last_patch:\", model.last_patch, \"last_patch_location:\", model.last_patch_location)\n",
        "    print(f\"score_post_attn = {s_attn:.4f}  | delta_vs_corr = {s_attn - score_corr:+.4f}\")\n",
        "\n",
        "    # 4) Patch at post-MLP\n",
        "    _ = model(idx_corr,\n",
        "              layer_to_patch=test_L,\n",
        "              position_to_patch=test_P,\n",
        "              patch_location=\"post_mlp\")\n",
        "    s_mlp = score_from_last_logits(model.last_logits[0], a_id=a_id, b_id=b_id)\n",
        "    print(\"\\n=== Patch test (post_mlp) ===\")\n",
        "    print(\"last_patch:\", model.last_patch, \"last_patch_location:\", model.last_patch_location)\n",
        "    print(f\"score_post_mlp  = {s_mlp:.4f}  | delta_vs_corr = {s_mlp - score_corr:+.4f}\")\n",
        "\n",
        "    # 5) The key EXTRA 3 assertion: the two locations should not be identical everywhere.\n",
        "    print(\"\\n=== Intra-block location difference (single cell) ===\")\n",
        "    print(f\"abs(score_post_attn - score_post_mlp) = {abs(s_attn - s_mlp):.6f}\")\n",
        "    print(\"If this is exactly 0.0 for many tested cells, your patch_location may not be applied correctly.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "number of parameters: 124.44M\n",
            "Seq len T=11, n_layers=12\n",
            "Clean caches present? post_attn: True | post_mlp: True\n",
            "clean_post_attn dims: 12 x 11\n",
            "clean_post_mlp  dims: 12 x 11\n",
            "\n",
            "=== Baselines ===\n",
            "score_clean = -4.1241\n",
            "score_corr  = 5.6562\n",
            "Expected (for this pair): score_clean < score_corr (often clean is negative, corrupt is positive).\n",
            "\n",
            "=== Patch test (post_attn) ===\n",
            "last_patch: (6, 1) last_patch_location: post_attn\n",
            "score_post_attn = 5.6416  | delta_vs_corr = -0.0146\n",
            "\n",
            "=== Patch test (post_mlp) ===\n",
            "last_patch: (6, 1) last_patch_location: post_mlp\n",
            "score_post_mlp  = -3.1509  | delta_vs_corr = -8.8071\n",
            "\n",
            "=== Intra-block location difference (single cell) ===\n",
            "abs(score_post_attn - score_post_mlp) = 8.792526\n",
            "If this is exactly 0.0 for many tested cells, your patch_location may not be applied correctly.\n"
          ]
        }
      ],
      "source": [
        "!python extra3_intrablock_check.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting extra3_intrablock_sweep_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile extra3_intrablock_sweep_driver.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "# Reuse your plotting utilities (already in your project)\n",
        "from section10_visualization import (\n",
        "    decode_prompt_token_labels,\n",
        "    plot_logit_diff_heatmap,\n",
        "    save_figure_publication_quality,\n",
        ")\n",
        "\n",
        "CLEAN_TEXT   = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "CORRUPT_TEXT = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "TOKEN_A = \" Jones\"\n",
        "TOKEN_B = \" Smith\"\n",
        "\n",
        "def single_token_id(bpe: BPETokenizer, s: str) -> int:\n",
        "    ids = bpe(s)[0].tolist()\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{s!r} is not a single BPE token. Got ids={ids}\")\n",
        "    return int(ids[0])\n",
        "\n",
        "def score_from_last_logits(last_logits_1d: torch.Tensor, *, a_id: int, b_id: int) -> float:\n",
        "    return float(last_logits_1d[b_id] - last_logits_1d[a_id])\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    set_seed(3407)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    idx_clean = bpe(CLEAN_TEXT).to(device)\n",
        "    idx_corr  = bpe(CORRUPT_TEXT).to(device)\n",
        "    if idx_clean.shape != idx_corr.shape:\n",
        "        raise RuntimeError(f\"Token length mismatch: clean={idx_clean.shape}, corrupt={idx_corr.shape}\")\n",
        "\n",
        "    T = int(idx_clean.shape[1])\n",
        "    n_layers = len(model.transformer.h)\n",
        "    print(f\"Seq len T={T}, n_layers={n_layers}\")\n",
        "\n",
        "    a_id = single_token_id(bpe, TOKEN_A)\n",
        "    b_id = single_token_id(bpe, TOKEN_B)\n",
        "\n",
        "    # Clean cache (must populate both intra-block banks)\n",
        "    _ = model(idx_clean, cache_activations=True, overwrite_cache=True)\n",
        "    score_clean = score_from_last_logits(model.last_logits[0], a_id=a_id, b_id=b_id)\n",
        "\n",
        "    # Corrupt baseline\n",
        "    _ = model(idx_corr)\n",
        "    score_corr = score_from_last_logits(model.last_logits[0], a_id=a_id, b_id=b_id)\n",
        "\n",
        "    print(\"\\n=== Baselines ===\")\n",
        "    print(f\"score_clean = {score_clean:.4f}\")\n",
        "    print(f\"score_corr  = {score_corr:.4f}\")\n",
        "\n",
        "    out_dir = Path(\"artifacts/extra3_intrablock\")\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    token_labels = decode_prompt_token_labels(bpe, CLEAN_TEXT)\n",
        "\n",
        "    mats = {}\n",
        "    for loc in [\"post_attn\", \"post_mlp\"]:\n",
        "        print(f\"\\nSweeping patches at location: {loc}\")\n",
        "        mat = torch.empty((n_layers, T), dtype=torch.float32)\n",
        "\n",
        "        for L in range(n_layers):\n",
        "            for P in range(T):\n",
        "                _ = model(idx_corr,\n",
        "                          layer_to_patch=L,\n",
        "                          position_to_patch=P,\n",
        "                          patch_location=loc)\n",
        "                s = score_from_last_logits(model.last_logits[0], a_id=a_id, b_id=b_id)\n",
        "                mat[L, P] = float(s)\n",
        "\n",
        "        mats[loc] = mat.cpu()\n",
        "        torch.save(mat.cpu(), out_dir / f\"matrix_{loc}.pt\")\n",
        "        print(\"Saved:\", (out_dir / f\"matrix_{loc}.pt\").resolve())\n",
        "\n",
        "        title = f\"EXTRA 3  {loc}: logit({TOKEN_B!r})  logit({TOKEN_A!r})\"\n",
        "        fig, ax = plot_logit_diff_heatmap(\n",
        "            mat,\n",
        "            token_labels=token_labels,\n",
        "            metric_title=title,\n",
        "            show_token_strings=True,\n",
        "            center_zero=True,\n",
        "            include_pos_in_label=True,\n",
        "        )\n",
        "        save_figure_publication_quality(fig, out_basepath=out_dir / f\"heatmap_{loc}\", formats=(\"png\", \"pdf\"), dpi=300)\n",
        "        print(\"Saved heatmap figures for:\", loc)\n",
        "\n",
        "    # Key check: matrices should not be identical\n",
        "    diff = (mats[\"post_attn\"] - mats[\"post_mlp\"]).abs()\n",
        "    print(\"\\n=== EXTRA 3 matrix difference stats ===\")\n",
        "    print(\"max |post_attn - post_mlp| =\", float(diff.max()))\n",
        "    print(\"mean|post_attn - post_mlp| =\", float(diff.mean()))\n",
        "    print(\"If max is exactly 0.0, your intra-block split is not taking effect.\")\n",
        "\n",
        "    # Optional: show restoration deltas relative to corrupted baseline\n",
        "    # (patched - corrupt): negative means moving toward clean if clean < corrupt for your pair.\n",
        "    delta_attn = mats[\"post_attn\"] - float(score_corr)\n",
        "    delta_mlp  = mats[\"post_mlp\"]  - float(score_corr)\n",
        "    print(\"\\nDelta sanity (patched - corrupt):\")\n",
        "    print(\"post_attn: min=\", float(delta_attn.min()), \"max=\", float(delta_attn.max()))\n",
        "    print(\"post_mlp : min=\", float(delta_mlp.min()),  \"max=\", float(delta_mlp.max()))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "number of parameters: 124.44M\n",
            "Seq len T=11, n_layers=12\n",
            "\n",
            "=== Baselines ===\n",
            "score_clean = -4.1241\n",
            "score_corr  = 5.6562\n",
            "\n",
            "Sweeping patches at location: post_attn\n",
            "Saved: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/artifacts/extra3_intrablock/matrix_post_attn.pt\n",
            "Saved heatmap figures for: post_attn\n",
            "\n",
            "Sweeping patches at location: post_mlp\n",
            "Saved: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/artifacts/extra3_intrablock/matrix_post_mlp.pt\n",
            "Saved heatmap figures for: post_mlp\n",
            "\n",
            "=== EXTRA 3 matrix difference stats ===\n",
            "max |post_attn - post_mlp| = 9.767166137695312\n",
            "mean|post_attn - post_mlp| = 0.8032954335212708\n",
            "If max is exactly 0.0, your intra-block split is not taking effect.\n",
            "\n",
            "Delta sanity (patched - corrupt):\n",
            "post_attn: min= -3.4107208251953125 max= 0.04366302490234375\n",
            "post_mlp : min= -9.780319213867188 max= 0.0342254638671875\n"
          ]
        }
      ],
      "source": [
        "!python extra3_intrablock_sweep_driver.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 69%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                          [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m103 passed\u001b[0m\u001b[32m in 49.46s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -q"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "048b2b328d174740a7f4eb64fd2f8b7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c789b9007024556915b3f9e3cabe35e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1209e810b3fc41808b444108a87afca1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16ca502911134178a532cda1cf139ca4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20ea57fda15f42d18d60048c165cb019": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0449079fc7a48a3a941c34c7cf2fdc9",
            "placeholder": "",
            "style": "IPY_MODEL_9a67d3d3a8744fd881b8a2d5359e78ab",
            "value": "model.safetensors: 100%"
          }
        },
        "260906d3854c4ff78c2cda042bae0a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_048b2b328d174740a7f4eb64fd2f8b7f",
            "placeholder": "",
            "style": "IPY_MODEL_c033d8e19c084b90bd8a88a3e5bdb5b8",
            "value": " 124/124 [00:00&lt;00:00, 3.38kB/s]"
          }
        },
        "37af4bb362b94e87b7c128e6966534f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ba0334c76344e0a824177d3572e6dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16ca502911134178a532cda1cf139ca4",
            "placeholder": "",
            "style": "IPY_MODEL_ca37e879895f4707bb2e39d9229c65dd",
            "value": "generation_config.json: 100%"
          }
        },
        "3ce2905b2f8e41adbc9eb724aeb2f62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44854839f29f4de08577daf5c4db4c63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52f6eb03404a425aabda1a87b86afe15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cbfa6b49da84ee781492abacea18884": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eec0e26428c4034867633d70b4b3915": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "648a553be0bf49cba777f0eaaac741b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c789b9007024556915b3f9e3cabe35e",
            "placeholder": "",
            "style": "IPY_MODEL_f6620c81dc4e4285a332cc40b7655caf",
            "value": " 665/665 [00:00&lt;00:00, 14.7kB/s]"
          }
        },
        "74d877d849d1498fbf87968a79b436d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d95517340925450d9544a147d0064f8f",
              "IPY_MODEL_795a4f3580c5469f86d0bcb96edc1f13",
              "IPY_MODEL_648a553be0bf49cba777f0eaaac741b2"
            ],
            "layout": "IPY_MODEL_37af4bb362b94e87b7c128e6966534f9"
          }
        },
        "784c12d3beb34f25a3c3387217b9a80a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20ea57fda15f42d18d60048c165cb019",
              "IPY_MODEL_8f25e6d89124473688b788bf69c1bfba",
              "IPY_MODEL_bc9fe7054cc849e0ae9a0d69393de9d2"
            ],
            "layout": "IPY_MODEL_b858fc0c1caa4872b91ec7638d6dbf68"
          }
        },
        "795a4f3580c5469f86d0bcb96edc1f13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cbfa6b49da84ee781492abacea18884",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9bcaec9f9c5467aa6e35316d3296302",
            "value": 665
          }
        },
        "80c335cb65a5408dae9ecd4ec9eda4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ba0334c76344e0a824177d3572e6dd7",
              "IPY_MODEL_eff3788657d8436e96a328e73cab52cc",
              "IPY_MODEL_260906d3854c4ff78c2cda042bae0a2e"
            ],
            "layout": "IPY_MODEL_aa21a3379eb44eb28a52028cc5a0590b"
          }
        },
        "8f25e6d89124473688b788bf69c1bfba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a08a849c10e24dffb645cfe75c4bb66a",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efb3d94f64314da6a0e9b3e4d1e123e4",
            "value": 548105171
          }
        },
        "9a67d3d3a8744fd881b8a2d5359e78ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a08a849c10e24dffb645cfe75c4bb66a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9d6ce4b456f4b6b872a832ffe858e06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa21a3379eb44eb28a52028cc5a0590b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b858fc0c1caa4872b91ec7638d6dbf68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc9fe7054cc849e0ae9a0d69393de9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9d6ce4b456f4b6b872a832ffe858e06",
            "placeholder": "",
            "style": "IPY_MODEL_52f6eb03404a425aabda1a87b86afe15",
            "value": " 548M/548M [00:04&lt;00:00, 122MB/s]"
          }
        },
        "c033d8e19c084b90bd8a88a3e5bdb5b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca37e879895f4707bb2e39d9229c65dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0449079fc7a48a3a941c34c7cf2fdc9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d95517340925450d9544a147d0064f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1209e810b3fc41808b444108a87afca1",
            "placeholder": "",
            "style": "IPY_MODEL_3ce2905b2f8e41adbc9eb724aeb2f62a",
            "value": "config.json: 100%"
          }
        },
        "efb3d94f64314da6a0e9b3e4d1e123e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eff3788657d8436e96a328e73cab52cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44854839f29f4de08577daf5c4db4c63",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5eec0e26428c4034867633d70b4b3915",
            "value": 124
          }
        },
        "f6620c81dc4e4285a332cc40b7655caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9bcaec9f9c5467aa6e35316d3296302": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

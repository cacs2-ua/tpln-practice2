{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lTgJSTKuzsM"
      },
      "source": [
        "# minGPT\n",
        "\n",
        "**Note:** The `autoreload` extension allows the interpreter to reload modules every time a cell is executed. This is useful when editing the code in a module. The following cell enables the extension and downloads the minGPT package from Github. You can now double-click on a file like model.py, edit its contents, and press Ctrl+S to save it. If you then re-run the notebook cells, including those that create an object of the corresponding class, you will see the changes reflected. Note that the next cell should *only be executed once*, as running `pip install` again will overwrite the modified contents of the module.\n",
        "\n",
        "Recall that changes in the files (except the notebook itself) are not persistent unless you connect them to your Google Drive account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEghw4-QA6oY",
        "outputId": "cbb70c42-df3d-42dd-8df1-43e663f09c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Obtaining mingpt from git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26#egg=mingpt\n",
            "  Skipping because already up-to-date.\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting torch (from mingpt)\n",
            "  Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting filelock (from torch->mingpt)\n",
            "  Using cached filelock-3.20.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch->mingpt)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting setuptools (from torch->mingpt)\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting sympy>=1.13.3 (from torch->mingpt)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx>=2.5.1 (from torch->mingpt)\n",
            "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting jinja2 (from torch->mingpt)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec>=0.8.5 (from torch->mingpt)\n",
            "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->mingpt)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->mingpt)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->mingpt)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch->mingpt)\n",
            "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->mingpt)\n",
            "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->mingpt)\n",
            "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->mingpt)\n",
            "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->mingpt)\n",
            "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->mingpt)\n",
            "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch->mingpt)\n",
            "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch->mingpt)\n",
            "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch->mingpt)\n",
            "  Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->mingpt)\n",
            "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->mingpt)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->mingpt)\n",
            "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch->mingpt)\n",
            "  Using cached triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->mingpt)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch->mingpt)\n",
            "  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading filelock-3.20.2-py3-none-any.whl (16 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Building wheels for collected packages: mingpt\n",
            "  Building editable for mingpt (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for mingpt: filename=mingpt-0.0.1-0.editable-py3-none-any.whl size=3597 sha256=c7c0661c2a57cbb73d7ecf59ef01ca1d0146cd3476d206bb0f0bebf7f2f1d4bb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lby91wwz/wheels/00/63/d9/a8cbf56faebea087e4c2e61b7041cb6af80e18d658b27c351e\n",
            "Successfully built mingpt\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, triton, sympy, setuptools, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, mingpt\n",
            "Successfully installed MarkupSafe-3.0.3 filelock-3.20.2 fsspec-2025.12.0 jinja2-3.1.6 mingpt-0.0.1 mpmath-1.3.0 networkx-3.6.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 setuptools-80.9.0 sympy-1.14.0 torch-2.9.1 triton-3.5.1 typing-extensions-4.15.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "sed: can't read /content/src/mingpt/mingpt/model.py: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%pip install -e 'git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26#egg=mingpt'\n",
        "\n",
        "# Fix this issue: https://github.com/karpathy/minGPT/issues/120\n",
        "!sed -i '200s/.*/        assert len(keys) == len([k for k in sd if not k.endswith(\".attn.bias\")])/' /content/src/mingpt/mingpt/model.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkm4BPSu17LG"
      },
      "source": [
        "Add module's location to PYTHONPATH, which tells your Python interpreter where to search modules for. The previous `pip install -e` changes the variable in a subshell and the interpreter is therefore not aware of the updated value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SWZ69BeU1v49"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/src/mingpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMaLovlTyIZs",
        "outputId": "a69bba5f-09f9-465d-dcbf-51b30e68a25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m303.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.20.2)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy>=1.17 (from transformers)\n",
            "  Downloading numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests (from transformers)\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
            "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
            "  Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers)\n",
            "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
            "  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
            "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
            "Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, tqdm, safetensors, regex, pyyaml, numpy, idna, hf-xet, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed certifi-2026.1.4 charset_normalizer-3.4.4 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 numpy-2.4.0 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 tokenizers-0.22.2 tqdm-4.67.1 transformers-4.57.3 urllib3-2.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-otHTa0guzsT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from mingpt.model import GPT\n",
        "from mingpt.utils import set_seed\n",
        "from mingpt.bpe import BPETokenizer\n",
        "set_seed(3407)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yyjR2C7BuzsW"
      },
      "outputs": [],
      "source": [
        "use_mingpt = True # use minGPT or huggingface/transformers model?\n",
        "model_type = 'gpt2'\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311,
          "referenced_widgets": [
            "74d877d849d1498fbf87968a79b436d5",
            "d95517340925450d9544a147d0064f8f",
            "795a4f3580c5469f86d0bcb96edc1f13",
            "648a553be0bf49cba777f0eaaac741b2",
            "37af4bb362b94e87b7c128e6966534f9",
            "1209e810b3fc41808b444108a87afca1",
            "3ce2905b2f8e41adbc9eb724aeb2f62a",
            "5cbfa6b49da84ee781492abacea18884",
            "f9bcaec9f9c5467aa6e35316d3296302",
            "0c789b9007024556915b3f9e3cabe35e",
            "f6620c81dc4e4285a332cc40b7655caf",
            "784c12d3beb34f25a3c3387217b9a80a",
            "20ea57fda15f42d18d60048c165cb019",
            "8f25e6d89124473688b788bf69c1bfba",
            "bc9fe7054cc849e0ae9a0d69393de9d2",
            "b858fc0c1caa4872b91ec7638d6dbf68",
            "d0449079fc7a48a3a941c34c7cf2fdc9",
            "9a67d3d3a8744fd881b8a2d5359e78ab",
            "a08a849c10e24dffb645cfe75c4bb66a",
            "efb3d94f64314da6a0e9b3e4d1e123e4",
            "a9d6ce4b456f4b6b872a832ffe858e06",
            "52f6eb03404a425aabda1a87b86afe15",
            "80c335cb65a5408dae9ecd4ec9eda4ba",
            "3ba0334c76344e0a824177d3572e6dd7",
            "eff3788657d8436e96a328e73cab52cc",
            "260906d3854c4ff78c2cda042bae0a2e",
            "aa21a3379eb44eb28a52028cc5a0590b",
            "16ca502911134178a532cda1cf139ca4",
            "ca37e879895f4707bb2e39d9229c65dd",
            "44854839f29f4de08577daf5c4db4c63",
            "5eec0e26428c4034867633d70b4b3915",
            "048b2b328d174740a7f4eb64fd2f8b7f",
            "c033d8e19c084b90bd8a88a3e5bdb5b8"
          ]
        },
        "id": "7zgx5CsVuzsW",
        "outputId": "d82696bb-980b-427b-8dac-d85c19137a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 124.44M\n"
          ]
        }
      ],
      "source": [
        "if use_mingpt:\n",
        "    model = GPT.from_pretrained(model_type)\n",
        "else:\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "    model.config.pad_token_id = model.config.eos_token_id # suppress a warning\n",
        "\n",
        "# ship model to device and set to eval mode\n",
        "model.to(device)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vizgt_f4uzsY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
        "\n",
        "    # tokenize the input prompt into integer input sequence\n",
        "    if use_mingpt:\n",
        "        tokenizer = BPETokenizer()\n",
        "        if prompt == '':\n",
        "            # to create unconditional samples...\n",
        "            # manually create a tensor with only the special <|endoftext|> token\n",
        "            # similar to what openai's code does here https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\n",
        "            x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
        "        else:\n",
        "            x = tokenizer(prompt).to(device)\n",
        "    else:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
        "        if prompt == '':\n",
        "            # to create unconditional samples...\n",
        "            # huggingface/transformers tokenizer special cases these strings\n",
        "            prompt = '<|endoftext|>'\n",
        "        encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "        x = encoded_input['input_ids']\n",
        "\n",
        "    # we'll process all desired num_samples in a batch, so expand out the batch dim\n",
        "    x = x.expand(num_samples, -1)\n",
        "\n",
        "    # forward the model `steps` times to get samples, in a batch\n",
        "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
        "        print('-'*80)\n",
        "        print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxpoqSOWuzsZ",
        "outputId": "ee342ba7-57a5-48d5-96da-800bffc3b8ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json to /home/bledyx/.cache/mingpt/encoder.json\n",
            "downloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe to /home/bledyx/.cache/mingpt/vocab.bpe\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on NASA's Juno mission, will also receive this year's award.\n",
            "\n",
            "While his experience shows that\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on Russia's delegation to the G20 summit, spoke about his work to end climate change in his blog\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the United Nations Security Council who was asked on Monday to take a position on climate change in order to\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the UN climate conference in Paris where he was one of the leading climate advocates, said the decision to\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the panel of the U.N. Climate Change Conference on Nov. 17 (UNCIT),\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on an expedition to the moon,\" Karpathy said in an interview with SPACE.com, referring to\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the committee, said, \"We are not convinced that the planet is as good as it should be\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the NASA Goddard Centre for Astrophysics project and a senior fellow at NASA's Goddard Institute for Space\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the Council of Ministers, has declared that there is no possibility of any major change in the trajectory of\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the Energetic System, agrees with his co-organizer and is hopeful the process will prove\n"
          ]
        }
      ],
      "source": [
        "generate(prompt='Andrej Karpathy, the Earth representative on', num_samples=10, steps=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing repo_orientation.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile repo_orientation.py\n",
        "\"\"\"\n",
        "Section 2 helper: repository/codebase orientation for minGPT in Colab.\n",
        "\n",
        "What this script does:\n",
        "- Prints where mingpt is installed.\n",
        "- Locates mingpt/model.py.\n",
        "- Extracts/prints the key lines of GPT.forward that matter for the assignment:\n",
        "  embeddings -> transformer blocks -> ln_f -> lm_head -> logits\n",
        "- Provides programmatic checks used by unit tests.\n",
        "\n",
        "This does NOT implement activation caching/patching yet. It only verifies\n",
        "we understand where it would go later.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import inspect\n",
        "import pathlib\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ForwardLandmarks:\n",
        "    has_tok_emb: bool\n",
        "    has_pos_emb: bool\n",
        "    has_blocks_loop: bool\n",
        "    has_ln_f: bool\n",
        "    has_lm_head: bool\n",
        "\n",
        "\n",
        "def get_paths() -> Dict[str, str]:\n",
        "    pkg_path = pathlib.Path(mingpt.__file__).resolve()\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return {\n",
        "        \"mingpt.__file__\": str(pkg_path),\n",
        "        \"mingpt.model.__file__\": str(model_path),\n",
        "    }\n",
        "\n",
        "\n",
        "def read_model_source() -> str:\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return model_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "def attn_bias_fix_present(model_source: str) -> bool:\n",
        "    # Required fix: assert len(keys) == len([k for k in sd if not k.endswith(\".attn.bias\")])\n",
        "    return 'len([k for k in sd if not k.endswith(\".attn.bias\")])' in model_source\n",
        "\n",
        "\n",
        "def forward_source() -> str:\n",
        "    return inspect.getsource(GPT.forward)\n",
        "\n",
        "\n",
        "def find_forward_landmarks(src: str) -> ForwardLandmarks:\n",
        "    # We intentionally check for robust substrings (not exact formatting).\n",
        "    has_tok_emb = \"tok_emb\" in src and \"wte\" in src\n",
        "    has_pos_emb = \"pos_emb\" in src and \"wpe\" in src\n",
        "    has_blocks_loop = (\"for block in self.transformer.h\" in src) or (\"for block in self.transformer['h']\" in src)\n",
        "    has_ln_f = \"ln_f\" in src\n",
        "    has_lm_head = \"lm_head\" in src and \"logits\" in src\n",
        "    return ForwardLandmarks(\n",
        "        has_tok_emb=has_tok_emb,\n",
        "        has_pos_emb=has_pos_emb,\n",
        "        has_blocks_loop=has_blocks_loop,\n",
        "        has_ln_f=has_ln_f,\n",
        "        has_lm_head=has_lm_head,\n",
        "    )\n",
        "\n",
        "\n",
        "def print_forward_snippet(src: str, max_lines: int = 80) -> None:\n",
        "    lines = src.splitlines()\n",
        "    print(\"=== GPT.forward (snippet) ===\")\n",
        "    for i, line in enumerate(lines[:max_lines], start=1):\n",
        "        print(f\"{i:03d}: {line}\")\n",
        "    if len(lines) > max_lines:\n",
        "        print(f\"... ({len(lines)-max_lines} more lines)\")\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    paths = get_paths()\n",
        "    print(\"=== Installed paths ===\")\n",
        "    for k, v in paths.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    model_src = read_model_source()\n",
        "    print(\"\\n=== .attn.bias fix present? ===\")\n",
        "    print(attn_bias_fix_present(model_src))\n",
        "\n",
        "    fwd_src = forward_source()\n",
        "    landmarks = find_forward_landmarks(fwd_src)\n",
        "    print(\"\\n=== Forward pipeline landmarks ===\")\n",
        "    print(landmarks)\n",
        "\n",
        "    print()\n",
        "    print_forward_snippet(fwd_src)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Installed paths ===\n",
            "mingpt.__file__: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/src/mingpt/mingpt/__init__.py\n",
            "mingpt.model.__file__: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/src/mingpt/mingpt/model.py\n",
            "\n",
            "=== .attn.bias fix present? ===\n",
            "True\n",
            "\n",
            "=== Forward pipeline landmarks ===\n",
            "ForwardLandmarks(has_tok_emb=True, has_pos_emb=True, has_blocks_loop=True, has_ln_f=True, has_lm_head=True)\n",
            "\n",
            "=== GPT.forward (snippet) ===\n",
            "001:     def forward(self, idx, targets=None):\n",
            "002:         device = idx.device\n",
            "003:         b, t = idx.size()\n",
            "004:         assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
            "005:         pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
            "006: \n",
            "007:         # forward the GPT model itself\n",
            "008:         tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
            "009:         pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
            "010:         x = self.transformer.drop(tok_emb + pos_emb)\n",
            "011:         for block in self.transformer.h:\n",
            "012:             x = block(x)\n",
            "013:         x = self.transformer.ln_f(x)\n",
            "014:         logits = self.lm_head(x)\n",
            "015: \n",
            "016:         # if we are given some desired targets also calculate the loss\n",
            "017:         loss = None\n",
            "018:         if targets is not None:\n",
            "019:             loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
            "020: \n",
            "021:         return logits, loss\n"
          ]
        }
      ],
      "source": [
        "!python repo_orientation.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing generate_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile generate_driver.py\n",
        "\"\"\"\n",
        "Section 2 driver skeleton (will be extended in Sections 3+ and especially 5–7).\n",
        "\n",
        "Right now it only:\n",
        "- loads GPT-2 small via GPT.from_pretrained('gpt2')\n",
        "- tokenizes a prompt with BPETokenizer\n",
        "- runs a single forward pass to confirm logits shape\n",
        "- runs model.generate to confirm decoding loop works\n",
        "\n",
        "Later, you'll add:\n",
        "- control flags passed into GPT.forward (save_activations, patch params, etc.)\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "\n",
        "def get_device() -> str:\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    set_seed(3407)\n",
        "\n",
        "    device = get_device()\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    model = GPT.from_pretrained(\"gpt2\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    bpe = BPETokenizer()\n",
        "    prompt = \"Andrej Karpathy, the Earth representative on\"\n",
        "    idx = bpe(prompt).to(device)  # shape (1, T)\n",
        "\n",
        "    # forward pass (logits for each position)\n",
        "    logits, loss = model(idx)\n",
        "    print(\"Input shape:\", tuple(idx.shape))\n",
        "    print(\"Logits shape:\", tuple(logits.shape))\n",
        "    assert logits.ndim == 3, \"Expected (B, T, V) logits\"\n",
        "    assert logits.shape[0] == idx.shape[0] and logits.shape[1] == idx.shape[1], \"B,T must match input\"\n",
        "\n",
        "    # generate a short continuation (just to prove decoding loop works)\n",
        "    out_idx = model.generate(idx, max_new_tokens=20, do_sample=True, top_k=40)\n",
        "    out_text = bpe.decode(out_idx[0].cpu())\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(out_text)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "number of parameters: 124.44M\n",
            "Input shape: (1, 10)\n",
            "Logits shape: (1, 10, 50257)\n",
            "\n",
            "=== Generated ===\n",
            "Andrej Karpathy, the Earth representative on NASA's Mars Exploration Rover Curiosity, talks about the success of the science rover Curiosity, which now has\n"
          ]
        }
      ],
      "source": [
        "!python generate_driver.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test_all.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_all.py\n",
        "import os\n",
        "import pathlib\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "import repo_orientation as ro\n",
        "\n",
        "\n",
        "def test_mingpt_importable_and_paths_exist():\n",
        "    paths = ro.get_paths()\n",
        "    assert \"mingpt.__file__\" in paths and \"mingpt.model.__file__\" in paths\n",
        "\n",
        "    pkg_path = pathlib.Path(paths[\"mingpt.__file__\"])\n",
        "    model_path = pathlib.Path(paths[\"mingpt.model.__file__\"])\n",
        "    assert pkg_path.exists(), f\"mingpt package file not found: {pkg_path}\"\n",
        "    assert model_path.exists(), f\"mingpt.model file not found: {model_path}\"\n",
        "\n",
        "\n",
        "def test_attn_bias_fix_present_or_applied():\n",
        "    src = ro.read_model_source()\n",
        "    assert ro.attn_bias_fix_present(src), (\n",
        "        \"Required fix not found in mingpt/model.py. \"\n",
        "        \"Expected assert to ignore keys ending with .attn.bias.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def test_forward_pipeline_landmarks_present():\n",
        "    fwd_src = ro.forward_source()\n",
        "    lm = ro.find_forward_landmarks(fwd_src)\n",
        "    assert lm.has_tok_emb, \"Expected token embedding (wte/tok_emb) usage in forward.\"\n",
        "    assert lm.has_pos_emb, \"Expected positional embedding (wpe/pos_emb) usage in forward.\"\n",
        "    assert lm.has_blocks_loop, \"Expected loop over transformer blocks in forward.\"\n",
        "    assert lm.has_ln_f, \"Expected final layer norm ln_f in forward.\"\n",
        "    assert lm.has_lm_head, \"Expected lm_head/logits in forward.\"\n",
        "\n",
        "\n",
        "def test_fast_forward_and_generate_from_scratch():\n",
        "    # Fast test: avoid downloading HF weights.\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"  # tiny\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "    assert logits.shape == (1, 10, cfg.vocab_size)\n",
        "    assert loss is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=5, do_sample=False)\n",
        "    assert out.shape[1] == 15\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_slow_from_pretrained_gpt2_loads_and_runs():\n",
        "    # Slow test: tries to download and load GPT-2 weights.\n",
        "    # If network/cache issues happen in Colab, we skip rather than fail hard.\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    try:\n",
        "        model = GPT.from_pretrained(\"gpt2\")\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping from_pretrained test due to load/download error: {e}\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, 50257, (1, 8), dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "\n",
        "    assert logits.shape == (1, 8, 50257)\n",
        "    assert loss is None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing pytest.ini\n"
          ]
        }
      ],
      "source": [
        "%%writefile pytest.ini\n",
        "[pytest]\n",
        "markers =\n",
        "    slow: marks tests as slow (deselect with '-m \"not slow\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kernel python: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/bin/python\n",
            "torch: 2.9.1+cu128\n",
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                    [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 11.96s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(\"Kernel python:\", sys.executable)\n",
        "\n",
        "!{sys.executable} -m pip install -q pytest\n",
        "!{sys.executable} -c \"import torch; print('torch:', torch.__version__)\"\n",
        "!{sys.executable} -m pytest -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing tokenization_protocol.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tokenization_protocol.py\n",
        "\"\"\"\n",
        "Section 3: Tokenization Protocol and \"Same Number of Tokens\" Guarantee.\n",
        "\n",
        "This module provides:\n",
        "- Tokenization reports (token ids, per-token decoded strings, token count)\n",
        "- Pair comparison (same-length check, diff positions, one-token-diff check)\n",
        "- Report-friendly Markdown export for token-by-token decomposition\n",
        "- Heuristic suggestions to fix token length mismatches\n",
        "\n",
        "Designed for minGPT's BPETokenizer (mingpt/bpe.py).\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Sequence, Tuple, Dict\n",
        "\n",
        "import torch\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Data structures\n",
        "# -----------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TokenizationReport:\n",
        "    text: str\n",
        "    token_ids: List[int]\n",
        "    token_strs: List[str]  # decoded per-token strings (may include leading spaces)\n",
        "    seq_len: int\n",
        "    decoded_roundtrip: str\n",
        "\n",
        "    def short_preview(self, max_chars: int = 120) -> str:\n",
        "        s = self.text.replace(\"\\n\", \"\\\\n\")\n",
        "        return s if len(s) <= max_chars else s[: max_chars - 3] + \"...\"\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class PairComparison:\n",
        "    clean: TokenizationReport\n",
        "    corrupt: TokenizationReport\n",
        "    same_length: bool\n",
        "    diff_positions: List[int]\n",
        "    diff_count: int\n",
        "\n",
        "    @property\n",
        "    def one_token_diff(self) -> bool:\n",
        "        return self.same_length and self.diff_count == 1\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Core tokenization helpers\n",
        "# -----------------------------\n",
        "\n",
        "def tokenize_2d(bpe: BPETokenizer, text: str, device: Optional[str] = None) -> torch.LongTensor:\n",
        "    \"\"\"\n",
        "    Returns token ids as a 2D tensor of shape (1, T) as BPETokenizer does.\n",
        "    \"\"\"\n",
        "    ids_2d = bpe(text)  # (1, T)\n",
        "    if device is not None:\n",
        "        ids_2d = ids_2d.to(device)\n",
        "    return ids_2d\n",
        "\n",
        "\n",
        "def tokenize_1d_ids(bpe: BPETokenizer, text: str) -> List[int]:\n",
        "    \"\"\"\n",
        "    Returns token ids as a python list[int] (1D).\n",
        "    \"\"\"\n",
        "    ids = bpe(text)[0].tolist()\n",
        "    return [int(x) for x in ids]\n",
        "\n",
        "\n",
        "def decode_token_id(bpe: BPETokenizer, token_id: int) -> str:\n",
        "    \"\"\"\n",
        "    Decode a single token id into its string form.\n",
        "    \"\"\"\n",
        "    t = torch.tensor([token_id], dtype=torch.long)\n",
        "    return bpe.decode(t)\n",
        "\n",
        "\n",
        "def decode_tokens_1d(bpe: BPETokenizer, token_ids: Sequence[int]) -> str:\n",
        "    \"\"\"\n",
        "    Decode a sequence of token ids back into a string.\n",
        "    \"\"\"\n",
        "    t = torch.tensor(list(token_ids), dtype=torch.long)\n",
        "    return bpe.decode(t)\n",
        "\n",
        "\n",
        "def per_token_strings(bpe: BPETokenizer, token_ids: Sequence[int]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Per-token decoded strings (important for inspecting leading spaces).\n",
        "    \"\"\"\n",
        "    return [decode_token_id(bpe, int(tid)) for tid in token_ids]\n",
        "\n",
        "\n",
        "def build_report(bpe: BPETokenizer, text: str) -> TokenizationReport:\n",
        "    \"\"\"\n",
        "    Build a complete tokenization report for one text.\n",
        "    \"\"\"\n",
        "    token_ids = tokenize_1d_ids(bpe, text)\n",
        "    token_strs = per_token_strings(bpe, token_ids)\n",
        "    decoded = decode_tokens_1d(bpe, token_ids)\n",
        "    return TokenizationReport(\n",
        "        text=text,\n",
        "        token_ids=token_ids,\n",
        "        token_strs=token_strs,\n",
        "        seq_len=len(token_ids),\n",
        "        decoded_roundtrip=decoded,\n",
        "    )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Comparison and validations\n",
        "# -----------------------------\n",
        "\n",
        "def diff_positions(a: Sequence[int], b: Sequence[int]) -> List[int]:\n",
        "    \"\"\"\n",
        "    Returns a list of positions where sequences differ.\n",
        "    If lengths differ, extra positions beyond min length are included as diffs.\n",
        "    \"\"\"\n",
        "    la, lb = len(a), len(b)\n",
        "    m = min(la, lb)\n",
        "    diffs = [i for i in range(m) if int(a[i]) != int(b[i])]\n",
        "    if la != lb:\n",
        "        diffs.extend(list(range(m, max(la, lb))))\n",
        "    return diffs\n",
        "\n",
        "\n",
        "def compare_clean_corrupt(clean: TokenizationReport, corrupt: TokenizationReport) -> PairComparison:\n",
        "    diffs = diff_positions(clean.token_ids, corrupt.token_ids)\n",
        "    same_len = (clean.seq_len == corrupt.seq_len)\n",
        "    return PairComparison(\n",
        "        clean=clean,\n",
        "        corrupt=corrupt,\n",
        "        same_length=same_len,\n",
        "        diff_positions=diffs,\n",
        "        diff_count=len(diffs),\n",
        "    )\n",
        "\n",
        "\n",
        "def assert_same_length(clean: TokenizationReport, corrupt: TokenizationReport) -> None:\n",
        "    if clean.seq_len != corrupt.seq_len:\n",
        "        raise ValueError(\n",
        "            f\"Token length mismatch: clean={clean.seq_len}, corrupt={corrupt.seq_len}.\\n\"\n",
        "            f\"Clean preview: {clean.short_preview()}\\n\"\n",
        "            f\"Corrupt preview: {corrupt.short_preview()}\"\n",
        "        )\n",
        "\n",
        "\n",
        "def assert_one_token_difference(comp: PairComparison) -> None:\n",
        "    if not comp.same_length:\n",
        "        raise ValueError(\n",
        "            f\"Cannot check one-token-diff: lengths differ (clean={comp.clean.seq_len}, corrupt={comp.corrupt.seq_len}).\"\n",
        "        )\n",
        "    if comp.diff_count != 1:\n",
        "        raise ValueError(\n",
        "            f\"Expected exactly 1 differing token position, found {comp.diff_count}: {comp.diff_positions}\\n\"\n",
        "            f\"Tip: inspect the per-token strings and adjust the text until only one BPE token changes.\"\n",
        "        )\n",
        "\n",
        "\n",
        "def validate_pair(\n",
        "    bpe: BPETokenizer,\n",
        "    clean_text: str,\n",
        "    corrupt_text: str,\n",
        "    require_same_length: bool = True,\n",
        "    require_one_token_diff: bool = True,\n",
        ") -> PairComparison:\n",
        "    \"\"\"\n",
        "    Tokenize both texts, compare, and (optionally) enforce constraints by raising errors.\n",
        "    \"\"\"\n",
        "    clean = build_report(bpe, clean_text)\n",
        "    corrupt = build_report(bpe, corrupt_text)\n",
        "    comp = compare_clean_corrupt(clean, corrupt)\n",
        "\n",
        "    if require_same_length:\n",
        "        assert_same_length(clean, corrupt)\n",
        "    if require_one_token_diff:\n",
        "        assert_one_token_difference(comp)\n",
        "    return comp\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Printing / report exports\n",
        "# -----------------------------\n",
        "\n",
        "def format_token_list_for_console(rep: TokenizationReport) -> str:\n",
        "    \"\"\"\n",
        "    Console-friendly token list.\n",
        "    Shows position, token_id, and repr(token_str) to make spaces visible.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for i, (tid, s) in enumerate(zip(rep.token_ids, rep.token_strs)):\n",
        "        lines.append(f\"{i:02d} | {tid:5d} | {repr(s)}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def format_pair_diff_markdown(comp: PairComparison) -> str:\n",
        "    \"\"\"\n",
        "    Markdown table: position-wise clean vs corrupt tokens.\n",
        "    Great for pasting into the report.\n",
        "    \"\"\"\n",
        "    clean = comp.clean\n",
        "    corrupt = comp.corrupt\n",
        "    max_len = max(clean.seq_len, corrupt.seq_len)\n",
        "\n",
        "    header = \"| pos | clean_id | clean_tok | corrupt_id | corrupt_tok | diff? |\\n|---:|---:|---|---:|---|:---:|\\n\"\n",
        "    rows = []\n",
        "    for i in range(max_len):\n",
        "        c_id = clean.token_ids[i] if i < clean.seq_len else None\n",
        "        k_id = corrupt.token_ids[i] if i < corrupt.seq_len else None\n",
        "        c_tok = clean.token_strs[i] if i < clean.seq_len else \"\"\n",
        "        k_tok = corrupt.token_strs[i] if i < corrupt.seq_len else \"\"\n",
        "        diff = \"✅\" if i in comp.diff_positions else \"\"\n",
        "        rows.append(\n",
        "            f\"| {i} | {'' if c_id is None else c_id} | {repr(c_tok)} | {'' if k_id is None else k_id} | {repr(k_tok)} | {diff} |\"\n",
        "        )\n",
        "    return header + \"\\n\".join(rows) + \"\\n\"\n",
        "\n",
        "\n",
        "def describe_pair(comp: PairComparison) -> str:\n",
        "    \"\"\"\n",
        "    Human-readable summary.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        \"=== Pair summary ===\\n\"\n",
        "        f\"Clean tokens:   {comp.clean.seq_len}\\n\"\n",
        "        f\"Corrupt tokens: {comp.corrupt.seq_len}\\n\"\n",
        "        f\"Same length?    {comp.same_length}\\n\"\n",
        "        f\"Diff count:     {comp.diff_count}\\n\"\n",
        "        f\"Diff positions: {comp.diff_positions}\\n\"\n",
        "        f\"One-token diff? {comp.one_token_diff}\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Heuristic suggestions (for mismatch debugging)\n",
        "# -----------------------------\n",
        "\n",
        "def suggest_fixes(clean: TokenizationReport, corrupt: TokenizationReport) -> List[str]:\n",
        "    \"\"\"\n",
        "    Heuristics to help the user fix length mismatches / multi-token mismatches.\n",
        "    Not an automatic fixer; it gives actionable suggestions.\n",
        "    \"\"\"\n",
        "    suggestions: List[str] = []\n",
        "\n",
        "    # Length mismatch guidance\n",
        "    if clean.seq_len != corrupt.seq_len:\n",
        "        suggestions.append(\n",
        "            \"Token length mismatch detected. Common causes: whitespace differences, punctuation attachment, \"\n",
        "            \"or swapping a word that tokenizes into a different number of BPE tokens.\"\n",
        "        )\n",
        "        suggestions.append(\n",
        "            \"Try keeping punctuation identical (e.g., 'student.' vs 'student .') and keep spaces consistent around the changed word.\"\n",
        "        )\n",
        "        suggestions.append(\n",
        "            \"Proper nouns are often unstable: try swapping to a more common single-token alternative and re-check.\"\n",
        "        )\n",
        "\n",
        "    # Multi-token difference guidance\n",
        "    diffs = diff_positions(clean.token_ids, corrupt.token_ids)\n",
        "    if clean.seq_len == corrupt.seq_len and len(diffs) != 1:\n",
        "        suggestions.append(\n",
        "            f\"More than one token differs ({len(diffs)}). You want exactly 1 differing BPE token position.\"\n",
        "        )\n",
        "        suggestions.append(\n",
        "            \"Inspect per-token strings around the diff positions; often a punctuation or whitespace token is also changing.\"\n",
        "        )\n",
        "\n",
        "    # Space-specific hint\n",
        "    suggestions.append(\n",
        "        \"Remember GPT-2 BPE: tokens in the middle often include a leading space. \"\n",
        "        \"If you care about the token 'Jones', the actual token is usually ' Jones'.\"\n",
        "    )\n",
        "\n",
        "    return suggestions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing tokenization_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tokenization_driver.py\n",
        "\"\"\"\n",
        "Section 3 driver: tokenize clean/corrupt prompts, enforce same-length and one-token-diff,\n",
        "print per-token decomposition, and export a Markdown token table for the report.\n",
        "\n",
        "Usage in Colab:\n",
        "!python tokenization_driver.py\n",
        "\n",
        "Or override defaults by editing the CLEAN_TEXT / CORRUPT_TEXT constants below.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "import tokenization_protocol as tp\n",
        "\n",
        "\n",
        "# Edit these defaults for your own experiment.\n",
        "CLEAN_TEXT = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "CORRUPT_TEXT = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--clean\", type=str, default=CLEAN_TEXT, help=\"Clean prompt text\")\n",
        "    p.add_argument(\"--corrupt\", type=str, default=CORRUPT_TEXT, help=\"Corrupted prompt text\")\n",
        "    p.add_argument(\"--no-require-one-diff\", action=\"store_true\", help=\"Do not require exactly 1 token difference\")\n",
        "    p.add_argument(\"--out_md\", type=str, default=\"token_table.md\", help=\"Output markdown file for token table\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    clean_rep = tp.build_report(bpe, args.clean)\n",
        "    corrupt_rep = tp.build_report(bpe, args.corrupt)\n",
        "    comp = tp.compare_clean_corrupt(clean_rep, corrupt_rep)\n",
        "\n",
        "    print(tp.describe_pair(comp))\n",
        "\n",
        "    print(\"=== Clean prompt ===\")\n",
        "    print(clean_rep.text)\n",
        "    print(\"\\n=== Clean tokens (pos | id | repr(token)) ===\")\n",
        "    print(tp.format_token_list_for_console(clean_rep))\n",
        "\n",
        "    print(\"\\n=== Corrupt prompt ===\")\n",
        "    print(corrupt_rep.text)\n",
        "    print(\"\\n=== Corrupt tokens (pos | id | repr(token)) ===\")\n",
        "    print(tp.format_token_list_for_console(corrupt_rep))\n",
        "\n",
        "    # Enforce constraints as requested by the assignment\n",
        "    require_one = not args.no_require_one_diff\n",
        "    try:\n",
        "        _ = tp.validate_pair(\n",
        "            bpe=bpe,\n",
        "            clean_text=args.clean,\n",
        "            corrupt_text=args.corrupt,\n",
        "            require_same_length=True,\n",
        "            require_one_token_diff=require_one,\n",
        "        )\n",
        "        print(\"\\n✅ Validation passed.\")\n",
        "    except Exception as e:\n",
        "        print(\"\\n❌ Validation failed:\")\n",
        "        print(e)\n",
        "        print(\"\\nSuggestions:\")\n",
        "        for s in tp.suggest_fixes(clean_rep, corrupt_rep):\n",
        "            print(\"-\", s)\n",
        "\n",
        "    # Export markdown table for report\n",
        "    md = tp.format_pair_diff_markdown(comp)\n",
        "    out_path = Path(args.out_md)\n",
        "    out_path.write_text(md, encoding=\"utf-8\")\n",
        "    print(f\"\\nWrote Markdown token table to: {out_path.resolve()}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Pair summary ===\n",
            "Clean tokens:   11\n",
            "Corrupt tokens: 11\n",
            "Same length?    True\n",
            "Diff count:     1\n",
            "Diff positions: [1]\n",
            "One-token diff? True\n",
            "\n",
            "=== Clean prompt ===\n",
            "Michelle Jones was a top-notch student. Michelle\n",
            "\n",
            "=== Clean tokens (pos | id | repr(token)) ===\n",
            "00 | 48736 | 'Michelle'\n",
            "01 |  5437 | ' Jones'\n",
            "02 |   373 | ' was'\n",
            "03 |   257 | ' a'\n",
            "04 |  1353 | ' top'\n",
            "05 |    12 | '-'\n",
            "06 |  1662 | 'not'\n",
            "07 |   354 | 'ch'\n",
            "08 |  3710 | ' student'\n",
            "09 |    13 | '.'\n",
            "10 | 16738 | ' Michelle'\n",
            "\n",
            "=== Corrupt prompt ===\n",
            "Michelle Smith was a top-notch student. Michelle\n",
            "\n",
            "=== Corrupt tokens (pos | id | repr(token)) ===\n",
            "00 | 48736 | 'Michelle'\n",
            "01 |  4176 | ' Smith'\n",
            "02 |   373 | ' was'\n",
            "03 |   257 | ' a'\n",
            "04 |  1353 | ' top'\n",
            "05 |    12 | '-'\n",
            "06 |  1662 | 'not'\n",
            "07 |   354 | 'ch'\n",
            "08 |  3710 | ' student'\n",
            "09 |    13 | '.'\n",
            "10 | 16738 | ' Michelle'\n",
            "\n",
            "✅ Validation passed.\n",
            "\n",
            "Wrote Markdown token table to: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/token_table.md\n"
          ]
        }
      ],
      "source": [
        "!python tokenization_driver.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| pos | clean_id | clean_tok | corrupt_id | corrupt_tok | diff? |\n",
            "|---:|---:|---|---:|---|:---:|\n",
            "| 0 | 48736 | 'Michelle' | 48736 | 'Michelle' |  |\n",
            "| 1 | 5437 | ' Jones' | 4176 | ' Smith' | ✅ |\n",
            "| 2 | 373 | ' was' | 373 | ' was' |  |\n",
            "| 3 | 257 | ' a' | 257 | ' a' |  |\n",
            "| 4 | 1353 | ' top' | 1353 | ' top' |  |\n",
            "| 5 | 12 | '-' | 12 | '-' |  |\n",
            "| 6 | 1662 | 'not' | 1662 | 'not' |  |\n",
            "| 7 | 354 | 'ch' | 354 | 'ch' |  |\n",
            "| 8 | 3710 | ' student' | 3710 | ' student' |  |\n",
            "| 9 | 13 | '.' | 13 | '.' |  |\n",
            "| 10 | 16738 | ' Michelle' | 16738 | ' Michelle' |  |\n"
          ]
        }
      ],
      "source": [
        "!sed -n '1,120p' token_table.md\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test_all.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_all.py\n",
        "import pathlib\n",
        "import sys\n",
        "\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "# Colab-friendly: ensure mingpt editable install path is visible during pytest subprocess\n",
        "COLAB_MINGPT_PATH = pathlib.Path(\"/content/src/mingpt\")\n",
        "if COLAB_MINGPT_PATH.exists():\n",
        "    sys.path.append(str(COLAB_MINGPT_PATH))\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "import repo_orientation as ro\n",
        "import tokenization_protocol as tp\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Section 2 tests (repo orientation)\n",
        "# --------------------------\n",
        "\n",
        "def test_mingpt_importable_and_paths_exist():\n",
        "    paths = ro.get_paths()\n",
        "    assert \"mingpt.__file__\" in paths and \"mingpt.model.__file__\" in paths\n",
        "\n",
        "    pkg_path = pathlib.Path(paths[\"mingpt.__file__\"])\n",
        "    model_path = pathlib.Path(paths[\"mingpt.model.__file__\"])\n",
        "    assert pkg_path.exists(), f\"mingpt package file not found: {pkg_path}\"\n",
        "    assert model_path.exists(), f\"mingpt.model file not found: {model_path}\"\n",
        "\n",
        "\n",
        "def test_attn_bias_fix_present_or_applied():\n",
        "    src = ro.read_model_source()\n",
        "    assert ro.attn_bias_fix_present(src), (\n",
        "        \"Required fix not found in mingpt/model.py. \"\n",
        "        \"Expected assert to ignore keys ending with .attn.bias.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def test_forward_pipeline_landmarks_present():\n",
        "    fwd_src = ro.forward_source()\n",
        "    lm = ro.find_forward_landmarks(fwd_src)\n",
        "    assert lm.has_tok_emb, \"Expected token embedding (wte/tok_emb) usage in forward.\"\n",
        "    assert lm.has_pos_emb, \"Expected positional embedding (wpe/pos_emb) usage in forward.\"\n",
        "    assert lm.has_blocks_loop, \"Expected loop over transformer blocks in forward.\"\n",
        "    assert lm.has_ln_f, \"Expected final layer norm ln_f in forward.\"\n",
        "    assert lm.has_lm_head, \"Expected lm_head/logits in forward.\"\n",
        "\n",
        "\n",
        "def test_fast_forward_and_generate_from_scratch():\n",
        "    # Fast test: avoid downloading HF weights.\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"  # tiny\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "    assert logits.shape == (1, 10, cfg.vocab_size)\n",
        "    assert loss is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=5, do_sample=False)\n",
        "    assert out.shape[1] == 15\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Section 3 tests (tokenization protocol)\n",
        "# --------------------------\n",
        "\n",
        "def test_diff_positions_length_mismatch_includes_tail():\n",
        "    a = [1, 2, 3]\n",
        "    b = [1, 2, 3, 4, 5]\n",
        "    diffs = tp.diff_positions(a, b)\n",
        "    assert diffs == [3, 4]\n",
        "\n",
        "\n",
        "def test_compare_reports_detects_one_token_diff_synthetic():\n",
        "    clean = tp.TokenizationReport(\n",
        "        text=\"clean\",\n",
        "        token_ids=[10, 20, 30],\n",
        "        token_strs=[\"a\", \"b\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"abc\",\n",
        "    )\n",
        "    corrupt = tp.TokenizationReport(\n",
        "        text=\"corrupt\",\n",
        "        token_ids=[10, 99, 30],\n",
        "        token_strs=[\"a\", \"X\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"aXc\",\n",
        "    )\n",
        "    comp = tp.compare_clean_corrupt(clean, corrupt)\n",
        "    assert comp.same_length is True\n",
        "    assert comp.diff_positions == [1]\n",
        "    assert comp.diff_count == 1\n",
        "    assert comp.one_token_diff is True\n",
        "\n",
        "\n",
        "def test_assert_one_token_difference_raises_when_multi_diff():\n",
        "    clean = tp.TokenizationReport(\n",
        "        text=\"clean\",\n",
        "        token_ids=[1, 2, 3],\n",
        "        token_strs=[\"a\", \"b\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"abc\",\n",
        "    )\n",
        "    corrupt = tp.TokenizationReport(\n",
        "        text=\"corrupt\",\n",
        "        token_ids=[9, 2, 8],\n",
        "        token_strs=[\"X\", \"b\", \"Y\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"XbY\",\n",
        "    )\n",
        "    comp = tp.compare_clean_corrupt(clean, corrupt)\n",
        "    assert comp.diff_count == 2\n",
        "    with pytest.raises(ValueError):\n",
        "        tp.assert_one_token_difference(comp)\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_bpe_tokenization_roundtrip_and_lengths():\n",
        "    \"\"\"\n",
        "    Slow-ish test because BPETokenizer may download merges/vocab on first use in a fresh runtime.\n",
        "    \"\"\"\n",
        "    from mingpt.bpe import BPETokenizer\n",
        "\n",
        "    try:\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping BPETokenizer test due to tokenizer init/download error: {e}\")\n",
        "\n",
        "    text = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    rep = tp.build_report(bpe, text)\n",
        "\n",
        "    # Basic sanity\n",
        "    assert rep.seq_len > 0\n",
        "    assert len(rep.token_ids) == rep.seq_len\n",
        "    assert len(rep.token_strs) == rep.seq_len\n",
        "\n",
        "    # Roundtrip should contain the key content (exact equality may vary by whitespace normalization)\n",
        "    assert \"Michelle\" in rep.decoded_roundtrip\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_bpe_pair_validation_example_michelle_jones_smith():\n",
        "    \"\"\"\n",
        "    Uses the assignment's canonical-style example to ensure:\n",
        "    - same token length\n",
        "    - ideally a one-token difference (it usually is, but tokenizer quirks can vary)\n",
        "    \"\"\"\n",
        "    from mingpt.bpe import BPETokenizer\n",
        "\n",
        "    try:\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping BPETokenizer test due to tokenizer init/download error: {e}\")\n",
        "\n",
        "    clean = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    corrupt = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "\n",
        "    clean_rep = tp.build_report(bpe, clean)\n",
        "    corrupt_rep = tp.build_report(bpe, corrupt)\n",
        "    comp = tp.compare_clean_corrupt(clean_rep, corrupt_rep)\n",
        "\n",
        "    assert comp.same_length is True, f\"Expected same token length; got {clean_rep.seq_len} vs {corrupt_rep.seq_len}\"\n",
        "\n",
        "    # We prefer one-token diff; if it isn't, we still show it's a valid pair for same-length constraint.\n",
        "    # But for the assignment report you should aim for diff_count == 1.\n",
        "    assert comp.diff_count >= 1\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_slow_from_pretrained_gpt2_loads_and_runs():\n",
        "    \"\"\"\n",
        "    Slow test: downloads and loads GPT-2 weights.\n",
        "    If network/cache issues happen in Colab, we skip rather than fail hard.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    try:\n",
        "        model = GPT.from_pretrained(\"gpt2\")\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping from_pretrained test due to load/download error: {e}\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, 50257, (1, 8), dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "\n",
        "    assert logits.shape == (1, 8, 50257)\n",
        "    assert loss is None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                               [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m10 passed\u001b[0m\u001b[32m in 16.71s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!{sys.executable} -m pytest -q"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "048b2b328d174740a7f4eb64fd2f8b7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c789b9007024556915b3f9e3cabe35e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1209e810b3fc41808b444108a87afca1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16ca502911134178a532cda1cf139ca4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20ea57fda15f42d18d60048c165cb019": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0449079fc7a48a3a941c34c7cf2fdc9",
            "placeholder": "​",
            "style": "IPY_MODEL_9a67d3d3a8744fd881b8a2d5359e78ab",
            "value": "model.safetensors: 100%"
          }
        },
        "260906d3854c4ff78c2cda042bae0a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_048b2b328d174740a7f4eb64fd2f8b7f",
            "placeholder": "​",
            "style": "IPY_MODEL_c033d8e19c084b90bd8a88a3e5bdb5b8",
            "value": " 124/124 [00:00&lt;00:00, 3.38kB/s]"
          }
        },
        "37af4bb362b94e87b7c128e6966534f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ba0334c76344e0a824177d3572e6dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16ca502911134178a532cda1cf139ca4",
            "placeholder": "​",
            "style": "IPY_MODEL_ca37e879895f4707bb2e39d9229c65dd",
            "value": "generation_config.json: 100%"
          }
        },
        "3ce2905b2f8e41adbc9eb724aeb2f62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44854839f29f4de08577daf5c4db4c63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52f6eb03404a425aabda1a87b86afe15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cbfa6b49da84ee781492abacea18884": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eec0e26428c4034867633d70b4b3915": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "648a553be0bf49cba777f0eaaac741b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c789b9007024556915b3f9e3cabe35e",
            "placeholder": "​",
            "style": "IPY_MODEL_f6620c81dc4e4285a332cc40b7655caf",
            "value": " 665/665 [00:00&lt;00:00, 14.7kB/s]"
          }
        },
        "74d877d849d1498fbf87968a79b436d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d95517340925450d9544a147d0064f8f",
              "IPY_MODEL_795a4f3580c5469f86d0bcb96edc1f13",
              "IPY_MODEL_648a553be0bf49cba777f0eaaac741b2"
            ],
            "layout": "IPY_MODEL_37af4bb362b94e87b7c128e6966534f9"
          }
        },
        "784c12d3beb34f25a3c3387217b9a80a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20ea57fda15f42d18d60048c165cb019",
              "IPY_MODEL_8f25e6d89124473688b788bf69c1bfba",
              "IPY_MODEL_bc9fe7054cc849e0ae9a0d69393de9d2"
            ],
            "layout": "IPY_MODEL_b858fc0c1caa4872b91ec7638d6dbf68"
          }
        },
        "795a4f3580c5469f86d0bcb96edc1f13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cbfa6b49da84ee781492abacea18884",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9bcaec9f9c5467aa6e35316d3296302",
            "value": 665
          }
        },
        "80c335cb65a5408dae9ecd4ec9eda4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ba0334c76344e0a824177d3572e6dd7",
              "IPY_MODEL_eff3788657d8436e96a328e73cab52cc",
              "IPY_MODEL_260906d3854c4ff78c2cda042bae0a2e"
            ],
            "layout": "IPY_MODEL_aa21a3379eb44eb28a52028cc5a0590b"
          }
        },
        "8f25e6d89124473688b788bf69c1bfba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a08a849c10e24dffb645cfe75c4bb66a",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efb3d94f64314da6a0e9b3e4d1e123e4",
            "value": 548105171
          }
        },
        "9a67d3d3a8744fd881b8a2d5359e78ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a08a849c10e24dffb645cfe75c4bb66a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9d6ce4b456f4b6b872a832ffe858e06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa21a3379eb44eb28a52028cc5a0590b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b858fc0c1caa4872b91ec7638d6dbf68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc9fe7054cc849e0ae9a0d69393de9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9d6ce4b456f4b6b872a832ffe858e06",
            "placeholder": "​",
            "style": "IPY_MODEL_52f6eb03404a425aabda1a87b86afe15",
            "value": " 548M/548M [00:04&lt;00:00, 122MB/s]"
          }
        },
        "c033d8e19c084b90bd8a88a3e5bdb5b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca37e879895f4707bb2e39d9229c65dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0449079fc7a48a3a941c34c7cf2fdc9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d95517340925450d9544a147d0064f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1209e810b3fc41808b444108a87afca1",
            "placeholder": "​",
            "style": "IPY_MODEL_3ce2905b2f8e41adbc9eb724aeb2f62a",
            "value": "config.json: 100%"
          }
        },
        "efb3d94f64314da6a0e9b3e4d1e123e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eff3788657d8436e96a328e73cab52cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44854839f29f4de08577daf5c4db4c63",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5eec0e26428c4034867633d70b4b3915",
            "value": 124
          }
        },
        "f6620c81dc4e4285a332cc40b7655caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9bcaec9f9c5467aa6e35316d3296302": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

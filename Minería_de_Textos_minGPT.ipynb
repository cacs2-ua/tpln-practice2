{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lTgJSTKuzsM"
      },
      "source": [
        "# minGPT\n",
        "\n",
        "**Note:** The `autoreload` extension allows the interpreter to reload modules every time a cell is executed. This is useful when editing the code in a module. The following cell enables the extension and downloads the minGPT package from Github. You can now double-click on a file like model.py, edit its contents, and press Ctrl+S to save it. If you then re-run the notebook cells, including those that create an object of the corresponding class, you will see the changes reflected. Note that the next cell should *only be executed once*, as running `pip install` again will overwrite the modified contents of the module.\n",
        "\n",
        "Recall that changes in the files (except the notebook itself) are not persistent unless you connect them to your Google Drive account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEghw4-QA6oY",
        "outputId": "cbb70c42-df3d-42dd-8df1-43e663f09c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining mingpt from git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26#egg=mingpt\n",
            "  Cloning https://github.com/karpathy/minGPT.git (to revision 37baab71b9abea1b76ab957409a1cc2fbfba8a26) to ./.venv/src/mingpt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/karpathy/minGPT.git /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/src/mingpt\n",
            "  Running command git rev-parse -q --verify 'sha^37baab71b9abea1b76ab957409a1cc2fbfba8a26'\n",
            "  Running command git fetch -q https://github.com/karpathy/minGPT.git 37baab71b9abea1b76ab957409a1cc2fbfba8a26\n",
            "  Resolved https://github.com/karpathy/minGPT.git to commit 37baab71b9abea1b76ab957409a1cc2fbfba8a26\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from mingpt) (2.9.1)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (4.15.0)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (2025.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.12/site-packages (from torch->mingpt) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch->mingpt) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch->mingpt) (3.0.3)\n",
            "Building wheels for collected packages: mingpt\n",
            "  Building editable for mingpt (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for mingpt: filename=mingpt-0.0.1-0.editable-py3-none-any.whl size=3597 sha256=a1f8c699c4ffb40066f884608efc6214f047b59949bf9470c470f6d6965d4c0c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b75hez0r/wheels/00/63/d9/a8cbf56faebea087e4c2e61b7041cb6af80e18d658b27c351e\n",
            "Successfully built mingpt\n",
            "Installing collected packages: mingpt\n",
            "  Attempting uninstall: mingpt\n",
            "    Found existing installation: minGPT 0.0.1\n",
            "    Uninstalling minGPT-0.0.1:\n",
            "      Successfully uninstalled minGPT-0.0.1\n",
            "Successfully installed mingpt-0.0.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%pip install -e 'git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26#egg=mingpt'\n",
        "\n",
        "# Fix this issue: https://github.com/karpathy/minGPT/issues/120\n",
        "#!sed -i '200s/.*/        assert len(keys) == len([k for k in sd if not k.endswith(\".attn.bias\")])/' /content/src/mingpt/mingpt/model.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkm4BPSu17LG"
      },
      "source": [
        "Add module's location to PYTHONPATH, which tells your Python interpreter where to search modules for. The previous `pip install -e` changes the variable in a subshell and the interpreter is therefore not aware of the updated value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26\n",
            "  Using cached mingpt-0.0.1-py3-none-any.whl\n",
            "Collecting torch (from mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting filelock (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached filelock-3.20.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting setuptools (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting sympy>=1.13.3 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx>=2.5.1 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting jinja2 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec>=0.8.5 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch->mingpt@ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26)\n",
            "  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Using cached triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
            "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Using cached filelock-3.20.2-py3-none-any.whl (16 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, triton, sympy, setuptools, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, mingpt\n",
            "Successfully installed MarkupSafe-3.0.3 filelock-3.20.2 fsspec-2025.12.0 jinja2-3.1.6 mingpt-0.0.1 mpmath-1.3.0 networkx-3.6.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 setuptools-80.9.0 sympy-1.14.0 torch-2.9.1 triton-3.5.1 typing-extensions-4.15.0\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install --upgrade --force-reinstall \"mingpt @ git+https://github.com/karpathy/minGPT.git@37baab71b9abea1b76ab957409a1cc2fbfba8a26\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SWZ69BeU1v49"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/src/mingpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMaLovlTyIZs",
        "outputId": "a69bba5f-09f9-465d-dcbf-51b30e68a25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.20.2)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy>=1.17 (from transformers)\n",
            "  Using cached numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Using cached regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting requests (from transformers)\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
            "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
            "  Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers)\n",
            "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
            "  Using cached urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
            "  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "Using cached numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
            "Using cached regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
            "Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
            "Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
            "Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Using cached urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
            "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, numpy, idna, hf-xet, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed certifi-2026.1.4 charset_normalizer-3.4.4 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 numpy-2.4.0 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 tokenizers-0.22.2 tqdm-4.67.1 transformers-4.57.3 urllib3-2.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-otHTa0guzsT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from mingpt.model import GPT\n",
        "from mingpt.utils import set_seed\n",
        "from mingpt.bpe import BPETokenizer\n",
        "set_seed(3407)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yyjR2C7BuzsW"
      },
      "outputs": [],
      "source": [
        "use_mingpt = True # use minGPT or huggingface/transformers model?\n",
        "model_type = 'gpt2'\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311,
          "referenced_widgets": [
            "74d877d849d1498fbf87968a79b436d5",
            "d95517340925450d9544a147d0064f8f",
            "795a4f3580c5469f86d0bcb96edc1f13",
            "648a553be0bf49cba777f0eaaac741b2",
            "37af4bb362b94e87b7c128e6966534f9",
            "1209e810b3fc41808b444108a87afca1",
            "3ce2905b2f8e41adbc9eb724aeb2f62a",
            "5cbfa6b49da84ee781492abacea18884",
            "f9bcaec9f9c5467aa6e35316d3296302",
            "0c789b9007024556915b3f9e3cabe35e",
            "f6620c81dc4e4285a332cc40b7655caf",
            "784c12d3beb34f25a3c3387217b9a80a",
            "20ea57fda15f42d18d60048c165cb019",
            "8f25e6d89124473688b788bf69c1bfba",
            "bc9fe7054cc849e0ae9a0d69393de9d2",
            "b858fc0c1caa4872b91ec7638d6dbf68",
            "d0449079fc7a48a3a941c34c7cf2fdc9",
            "9a67d3d3a8744fd881b8a2d5359e78ab",
            "a08a849c10e24dffb645cfe75c4bb66a",
            "efb3d94f64314da6a0e9b3e4d1e123e4",
            "a9d6ce4b456f4b6b872a832ffe858e06",
            "52f6eb03404a425aabda1a87b86afe15",
            "80c335cb65a5408dae9ecd4ec9eda4ba",
            "3ba0334c76344e0a824177d3572e6dd7",
            "eff3788657d8436e96a328e73cab52cc",
            "260906d3854c4ff78c2cda042bae0a2e",
            "aa21a3379eb44eb28a52028cc5a0590b",
            "16ca502911134178a532cda1cf139ca4",
            "ca37e879895f4707bb2e39d9229c65dd",
            "44854839f29f4de08577daf5c4db4c63",
            "5eec0e26428c4034867633d70b4b3915",
            "048b2b328d174740a7f4eb64fd2f8b7f",
            "c033d8e19c084b90bd8a88a3e5bdb5b8"
          ]
        },
        "id": "7zgx5CsVuzsW",
        "outputId": "d82696bb-980b-427b-8dac-d85c19137a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 124.44M\n"
          ]
        }
      ],
      "source": [
        "if use_mingpt:\n",
        "    model = GPT.from_pretrained(model_type)\n",
        "else:\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "    model.config.pad_token_id = model.config.eos_token_id # suppress a warning\n",
        "\n",
        "# ship model to device and set to eval mode\n",
        "model.to(device)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vizgt_f4uzsY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
        "\n",
        "    # tokenize the input prompt into integer input sequence\n",
        "    if use_mingpt:\n",
        "        tokenizer = BPETokenizer()\n",
        "        if prompt == '':\n",
        "            # to create unconditional samples...\n",
        "            # manually create a tensor with only the special <|endoftext|> token\n",
        "            # similar to what openai's code does here https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\n",
        "            x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
        "        else:\n",
        "            x = tokenizer(prompt).to(device)\n",
        "    else:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
        "        if prompt == '':\n",
        "            # to create unconditional samples...\n",
        "            # huggingface/transformers tokenizer special cases these strings\n",
        "            prompt = '<|endoftext|>'\n",
        "        encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "        x = encoded_input['input_ids']\n",
        "\n",
        "    # we'll process all desired num_samples in a batch, so expand out the batch dim\n",
        "    x = x.expand(num_samples, -1)\n",
        "\n",
        "    # forward the model `steps` times to get samples, in a batch\n",
        "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
        "        print('-'*80)\n",
        "        print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxpoqSOWuzsZ",
        "outputId": "ee342ba7-57a5-48d5-96da-800bffc3b8ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on NASA's Juno mission, will also receive this year's award.\n",
            "\n",
            "While his experience shows that\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on Russia's delegation to the G20 summit, spoke about his work to end climate change in his blog\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the United Nations Security Council who was asked on Monday to take a position on climate change in order to\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the UN climate conference in Paris where he was one of the leading climate advocates, said the decision to\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the panel of the U.N. Climate Change Conference on Nov. 17 (UNCIT),\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on an expedition to the moon,\" Karpathy said in an interview with SPACE.com, referring to\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the committee, said, \"We are not convinced that the planet is as good as it should be\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the NASA Goddard Centre for Astrophysics project and a senior fellow at NASA's Goddard Institute for Space\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the Council of Ministers, has declared that there is no possibility of any major change in the trajectory of\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Earth representative on the Energetic System, agrees with his co-organizer and is hopeful the process will prove\n"
          ]
        }
      ],
      "source": [
        "generate(prompt='Andrej Karpathy, the Earth representative on', num_samples=10, steps=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting repo_orientation.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile repo_orientation.py\n",
        "\"\"\"\n",
        "Section 2 helper: repository/codebase orientation for minGPT in Colab.\n",
        "\n",
        "What this script does:\n",
        "- Prints where mingpt is installed.\n",
        "- Locates mingpt/model.py.\n",
        "- Extracts/prints the key lines of GPT.forward that matter for the assignment:\n",
        "  embeddings -> transformer blocks -> ln_f -> lm_head -> logits\n",
        "- Provides programmatic checks used by unit tests.\n",
        "\n",
        "This does NOT implement activation caching/patching yet. It only verifies\n",
        "we understand where it would go later.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import inspect\n",
        "import pathlib\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ForwardLandmarks:\n",
        "    has_tok_emb: bool\n",
        "    has_pos_emb: bool\n",
        "    has_blocks_loop: bool\n",
        "    has_ln_f: bool\n",
        "    has_lm_head: bool\n",
        "\n",
        "\n",
        "def get_paths() -> Dict[str, str]:\n",
        "    pkg_path = pathlib.Path(mingpt.__file__).resolve()\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return {\n",
        "        \"mingpt.__file__\": str(pkg_path),\n",
        "        \"mingpt.model.__file__\": str(model_path),\n",
        "    }\n",
        "\n",
        "\n",
        "def read_model_source() -> str:\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return model_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "def attn_bias_fix_present(model_source: str) -> bool:\n",
        "    # Required fix: assert len(keys) == len([k for k in sd if not k.endswith(\".attn.bias\")])\n",
        "    return 'len([k for k in sd if not k.endswith(\".attn.bias\")])' in model_source\n",
        "\n",
        "\n",
        "def forward_source() -> str:\n",
        "    return inspect.getsource(GPT.forward)\n",
        "\n",
        "\n",
        "def find_forward_landmarks(src: str) -> ForwardLandmarks:\n",
        "    # We intentionally check for robust substrings (not exact formatting).\n",
        "    has_tok_emb = \"tok_emb\" in src and \"wte\" in src\n",
        "    has_pos_emb = \"pos_emb\" in src and \"wpe\" in src\n",
        "    has_blocks_loop = (\"for block in self.transformer.h\" in src) or (\"for block in self.transformer['h']\" in src)\n",
        "    has_ln_f = \"ln_f\" in src\n",
        "    has_lm_head = \"lm_head\" in src and \"logits\" in src\n",
        "    return ForwardLandmarks(\n",
        "        has_tok_emb=has_tok_emb,\n",
        "        has_pos_emb=has_pos_emb,\n",
        "        has_blocks_loop=has_blocks_loop,\n",
        "        has_ln_f=has_ln_f,\n",
        "        has_lm_head=has_lm_head,\n",
        "    )\n",
        "\n",
        "\n",
        "def print_forward_snippet(src: str, max_lines: int = 80) -> None:\n",
        "    lines = src.splitlines()\n",
        "    print(\"=== GPT.forward (snippet) ===\")\n",
        "    for i, line in enumerate(lines[:max_lines], start=1):\n",
        "        print(f\"{i:03d}: {line}\")\n",
        "    if len(lines) > max_lines:\n",
        "        print(f\"... ({len(lines)-max_lines} more lines)\")\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    paths = get_paths()\n",
        "    print(\"=== Installed paths ===\")\n",
        "    for k, v in paths.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    model_src = read_model_source()\n",
        "    print(\"\\n=== .attn.bias fix present? ===\")\n",
        "    print(attn_bias_fix_present(model_src))\n",
        "\n",
        "    fwd_src = forward_source()\n",
        "    landmarks = find_forward_landmarks(fwd_src)\n",
        "    print(\"\\n=== Forward pipeline landmarks ===\")\n",
        "    print(landmarks)\n",
        "\n",
        "    print()\n",
        "    print_forward_snippet(fwd_src)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Installed paths ===\n",
            "mingpt.__file__: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/src/mingpt/mingpt/__init__.py\n",
            "mingpt.model.__file__: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/src/mingpt/mingpt/model.py\n",
            "\n",
            "=== .attn.bias fix present? ===\n",
            "True\n",
            "\n",
            "=== Forward pipeline landmarks ===\n",
            "ForwardLandmarks(has_tok_emb=True, has_pos_emb=True, has_blocks_loop=True, has_ln_f=True, has_lm_head=True)\n",
            "\n",
            "=== GPT.forward (snippet) ===\n",
            "001:     def forward(self, idx, targets=None):\n",
            "002:         device = idx.device\n",
            "003:         b, t = idx.size()\n",
            "004:         assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
            "005:         pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
            "006: \n",
            "007:         # forward the GPT model itself\n",
            "008:         tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
            "009:         pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
            "010:         x = self.transformer.drop(tok_emb + pos_emb)\n",
            "011:         for block in self.transformer.h:\n",
            "012:             x = block(x)\n",
            "013:         x = self.transformer.ln_f(x)\n",
            "014:         logits = self.lm_head(x)\n",
            "015: \n",
            "016:         # if we are given some desired targets also calculate the loss\n",
            "017:         loss = None\n",
            "018:         if targets is not None:\n",
            "019:             loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
            "020: \n",
            "021:         return logits, loss\n"
          ]
        }
      ],
      "source": [
        "!python repo_orientation.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting generate_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile generate_driver.py\n",
        "\"\"\"\n",
        "Section 2 driver skeleton (will be extended in Sections 3+ and especially 5â€“7).\n",
        "\n",
        "Right now it only:\n",
        "- loads GPT-2 small via GPT.from_pretrained('gpt2')\n",
        "- tokenizes a prompt with BPETokenizer\n",
        "- runs a single forward pass to confirm logits shape\n",
        "- runs model.generate to confirm decoding loop works\n",
        "\n",
        "Later, you'll add:\n",
        "- control flags passed into GPT.forward (save_activations, patch params, etc.)\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "\n",
        "def get_device() -> str:\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    set_seed(3407)\n",
        "\n",
        "    device = get_device()\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    model = GPT.from_pretrained(\"gpt2\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    bpe = BPETokenizer()\n",
        "    prompt = \"Andrej Karpathy, the Earth representative on\"\n",
        "    idx = bpe(prompt).to(device)  # shape (1, T)\n",
        "\n",
        "    # forward pass (logits for each position)\n",
        "    logits, loss = model(idx)\n",
        "    print(\"Input shape:\", tuple(idx.shape))\n",
        "    print(\"Logits shape:\", tuple(logits.shape))\n",
        "    assert logits.ndim == 3, \"Expected (B, T, V) logits\"\n",
        "    assert logits.shape[0] == idx.shape[0] and logits.shape[1] == idx.shape[1], \"B,T must match input\"\n",
        "\n",
        "    # generate a short continuation (just to prove decoding loop works)\n",
        "    out_idx = model.generate(idx, max_new_tokens=20, do_sample=True, top_k=40)\n",
        "    out_text = bpe.decode(out_idx[0].cpu())\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(out_text)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "number of parameters: 124.44M\n",
            "Input shape: (1, 10)\n",
            "Logits shape: (1, 10, 50257)\n",
            "\n",
            "=== Generated ===\n",
            "Andrej Karpathy, the Earth representative on NASA's Mars Exploration Rover Curiosity, talks about the success of the science rover Curiosity, which now has\n"
          ]
        }
      ],
      "source": [
        "!python generate_driver.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_section_2.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section_2.py\n",
        "import os\n",
        "import pathlib\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "import repo_orientation as ro\n",
        "\n",
        "\n",
        "def test_mingpt_importable_and_paths_exist():\n",
        "    paths = ro.get_paths()\n",
        "    assert \"mingpt.__file__\" in paths and \"mingpt.model.__file__\" in paths\n",
        "\n",
        "    pkg_path = pathlib.Path(paths[\"mingpt.__file__\"])\n",
        "    model_path = pathlib.Path(paths[\"mingpt.model.__file__\"])\n",
        "    assert pkg_path.exists(), f\"mingpt package file not found: {pkg_path}\"\n",
        "    assert model_path.exists(), f\"mingpt.model file not found: {model_path}\"\n",
        "\n",
        "\n",
        "def test_attn_bias_fix_present_or_applied():\n",
        "    src = ro.read_model_source()\n",
        "    assert ro.attn_bias_fix_present(src), (\n",
        "        \"Required fix not found in mingpt/model.py. \"\n",
        "        \"Expected assert to ignore keys ending with .attn.bias.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def test_forward_pipeline_landmarks_present():\n",
        "    fwd_src = ro.forward_source()\n",
        "    lm = ro.find_forward_landmarks(fwd_src)\n",
        "    assert lm.has_tok_emb, \"Expected token embedding (wte/tok_emb) usage in forward.\"\n",
        "    assert lm.has_pos_emb, \"Expected positional embedding (wpe/pos_emb) usage in forward.\"\n",
        "    assert lm.has_blocks_loop, \"Expected loop over transformer blocks in forward.\"\n",
        "    assert lm.has_ln_f, \"Expected final layer norm ln_f in forward.\"\n",
        "    assert lm.has_lm_head, \"Expected lm_head/logits in forward.\"\n",
        "\n",
        "\n",
        "def test_fast_forward_and_generate_from_scratch():\n",
        "    # Fast test: avoid downloading HF weights.\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"  # tiny\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "    assert logits.shape == (1, 10, cfg.vocab_size)\n",
        "    assert loss is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=5, do_sample=False)\n",
        "    assert out.shape[1] == 15\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_slow_from_pretrained_gpt2_loads_and_runs():\n",
        "    # Slow test: tries to download and load GPT-2 weights.\n",
        "    # If network/cache issues happen in Colab, we skip rather than fail hard.\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    try:\n",
        "        model = GPT.from_pretrained(\"gpt2\")\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping from_pretrained test due to load/download error: {e}\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, 50257, (1, 8), dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "\n",
        "    assert logits.shape == (1, 8, 50257)\n",
        "    assert loss is None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting pytest.ini\n"
          ]
        }
      ],
      "source": [
        "%%writefile pytest.ini\n",
        "[pytest]\n",
        "markers =\n",
        "    slow: marks tests as slow (deselect with '-m \"not slow\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tokenization_protocol.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tokenization_protocol.py\n",
        "\"\"\"\n",
        "Section 3: Tokenization Protocol and \"Same Number of Tokens\" Guarantee.\n",
        "\n",
        "This module provides:\n",
        "- Tokenization reports (token ids, per-token decoded strings, token count)\n",
        "- Pair comparison (same-length check, diff positions, one-token-diff check)\n",
        "- Report-friendly Markdown export for token-by-token decomposition\n",
        "- Heuristic suggestions to fix token length mismatches\n",
        "\n",
        "Designed for minGPT's BPETokenizer (mingpt/bpe.py).\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Sequence, Tuple, Dict\n",
        "\n",
        "import torch\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Data structures\n",
        "# -----------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TokenizationReport:\n",
        "    text: str\n",
        "    token_ids: List[int]\n",
        "    token_strs: List[str]  # decoded per-token strings (may include leading spaces)\n",
        "    seq_len: int\n",
        "    decoded_roundtrip: str\n",
        "\n",
        "    def short_preview(self, max_chars: int = 120) -> str:\n",
        "        s = self.text.replace(\"\\n\", \"\\\\n\")\n",
        "        return s if len(s) <= max_chars else s[: max_chars - 3] + \"...\"\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class PairComparison:\n",
        "    clean: TokenizationReport\n",
        "    corrupt: TokenizationReport\n",
        "    same_length: bool\n",
        "    diff_positions: List[int]\n",
        "    diff_count: int\n",
        "\n",
        "    @property\n",
        "    def one_token_diff(self) -> bool:\n",
        "        return self.same_length and self.diff_count == 1\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Core tokenization helpers\n",
        "# -----------------------------\n",
        "\n",
        "def tokenize_2d(bpe: BPETokenizer, text: str, device: Optional[str] = None) -> torch.LongTensor:\n",
        "    \"\"\"\n",
        "    Returns token ids as a 2D tensor of shape (1, T) as BPETokenizer does.\n",
        "    \"\"\"\n",
        "    ids_2d = bpe(text)  # (1, T)\n",
        "    if device is not None:\n",
        "        ids_2d = ids_2d.to(device)\n",
        "    return ids_2d\n",
        "\n",
        "\n",
        "def tokenize_1d_ids(bpe: BPETokenizer, text: str) -> List[int]:\n",
        "    \"\"\"\n",
        "    Returns token ids as a python list[int] (1D).\n",
        "    \"\"\"\n",
        "    ids = bpe(text)[0].tolist()\n",
        "    return [int(x) for x in ids]\n",
        "\n",
        "\n",
        "def decode_token_id(bpe: BPETokenizer, token_id: int) -> str:\n",
        "    \"\"\"\n",
        "    Decode a single token id into its string form.\n",
        "    \"\"\"\n",
        "    t = torch.tensor([token_id], dtype=torch.long)\n",
        "    return bpe.decode(t)\n",
        "\n",
        "\n",
        "def decode_tokens_1d(bpe: BPETokenizer, token_ids: Sequence[int]) -> str:\n",
        "    \"\"\"\n",
        "    Decode a sequence of token ids back into a string.\n",
        "    \"\"\"\n",
        "    t = torch.tensor(list(token_ids), dtype=torch.long)\n",
        "    return bpe.decode(t)\n",
        "\n",
        "\n",
        "def per_token_strings(bpe: BPETokenizer, token_ids: Sequence[int]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Per-token decoded strings (important for inspecting leading spaces).\n",
        "    \"\"\"\n",
        "    return [decode_token_id(bpe, int(tid)) for tid in token_ids]\n",
        "\n",
        "\n",
        "def build_report(bpe: BPETokenizer, text: str) -> TokenizationReport:\n",
        "    \"\"\"\n",
        "    Build a complete tokenization report for one text.\n",
        "    \"\"\"\n",
        "    token_ids = tokenize_1d_ids(bpe, text)\n",
        "    token_strs = per_token_strings(bpe, token_ids)\n",
        "    decoded = decode_tokens_1d(bpe, token_ids)\n",
        "    return TokenizationReport(\n",
        "        text=text,\n",
        "        token_ids=token_ids,\n",
        "        token_strs=token_strs,\n",
        "        seq_len=len(token_ids),\n",
        "        decoded_roundtrip=decoded,\n",
        "    )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Comparison and validations\n",
        "# -----------------------------\n",
        "\n",
        "def diff_positions(a: Sequence[int], b: Sequence[int]) -> List[int]:\n",
        "    \"\"\"\n",
        "    Returns a list of positions where sequences differ.\n",
        "    If lengths differ, extra positions beyond min length are included as diffs.\n",
        "    \"\"\"\n",
        "    la, lb = len(a), len(b)\n",
        "    m = min(la, lb)\n",
        "    diffs = [i for i in range(m) if int(a[i]) != int(b[i])]\n",
        "    if la != lb:\n",
        "        diffs.extend(list(range(m, max(la, lb))))\n",
        "    return diffs\n",
        "\n",
        "\n",
        "def compare_clean_corrupt(clean: TokenizationReport, corrupt: TokenizationReport) -> PairComparison:\n",
        "    diffs = diff_positions(clean.token_ids, corrupt.token_ids)\n",
        "    same_len = (clean.seq_len == corrupt.seq_len)\n",
        "    return PairComparison(\n",
        "        clean=clean,\n",
        "        corrupt=corrupt,\n",
        "        same_length=same_len,\n",
        "        diff_positions=diffs,\n",
        "        diff_count=len(diffs),\n",
        "    )\n",
        "\n",
        "\n",
        "def assert_same_length(clean: TokenizationReport, corrupt: TokenizationReport) -> None:\n",
        "    if clean.seq_len != corrupt.seq_len:\n",
        "        raise ValueError(\n",
        "            f\"Token length mismatch: clean={clean.seq_len}, corrupt={corrupt.seq_len}.\\n\"\n",
        "            f\"Clean preview: {clean.short_preview()}\\n\"\n",
        "            f\"Corrupt preview: {corrupt.short_preview()}\"\n",
        "        )\n",
        "\n",
        "\n",
        "def assert_one_token_difference(comp: PairComparison) -> None:\n",
        "    if not comp.same_length:\n",
        "        raise ValueError(\n",
        "            f\"Cannot check one-token-diff: lengths differ (clean={comp.clean.seq_len}, corrupt={comp.corrupt.seq_len}).\"\n",
        "        )\n",
        "    if comp.diff_count != 1:\n",
        "        raise ValueError(\n",
        "            f\"Expected exactly 1 differing token position, found {comp.diff_count}: {comp.diff_positions}\\n\"\n",
        "            f\"Tip: inspect the per-token strings and adjust the text until only one BPE token changes.\"\n",
        "        )\n",
        "\n",
        "\n",
        "def validate_pair(\n",
        "    bpe: BPETokenizer,\n",
        "    clean_text: str,\n",
        "    corrupt_text: str,\n",
        "    require_same_length: bool = True,\n",
        "    require_one_token_diff: bool = True,\n",
        ") -> PairComparison:\n",
        "    \"\"\"\n",
        "    Tokenize both texts, compare, and (optionally) enforce constraints by raising errors.\n",
        "    \"\"\"\n",
        "    clean = build_report(bpe, clean_text)\n",
        "    corrupt = build_report(bpe, corrupt_text)\n",
        "    comp = compare_clean_corrupt(clean, corrupt)\n",
        "\n",
        "    if require_same_length:\n",
        "        assert_same_length(clean, corrupt)\n",
        "    if require_one_token_diff:\n",
        "        assert_one_token_difference(comp)\n",
        "    return comp\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Printing / report exports\n",
        "# -----------------------------\n",
        "\n",
        "def format_token_list_for_console(rep: TokenizationReport) -> str:\n",
        "    \"\"\"\n",
        "    Console-friendly token list.\n",
        "    Shows position, token_id, and repr(token_str) to make spaces visible.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for i, (tid, s) in enumerate(zip(rep.token_ids, rep.token_strs)):\n",
        "        lines.append(f\"{i:02d} | {tid:5d} | {repr(s)}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def format_pair_diff_markdown(comp: PairComparison) -> str:\n",
        "    \"\"\"\n",
        "    Markdown table: position-wise clean vs corrupt tokens.\n",
        "    Great for pasting into the report.\n",
        "    \"\"\"\n",
        "    clean = comp.clean\n",
        "    corrupt = comp.corrupt\n",
        "    max_len = max(clean.seq_len, corrupt.seq_len)\n",
        "\n",
        "    header = \"| pos | clean_id | clean_tok | corrupt_id | corrupt_tok | diff? |\\n|---:|---:|---|---:|---|:---:|\\n\"\n",
        "    rows = []\n",
        "    for i in range(max_len):\n",
        "        c_id = clean.token_ids[i] if i < clean.seq_len else None\n",
        "        k_id = corrupt.token_ids[i] if i < corrupt.seq_len else None\n",
        "        c_tok = clean.token_strs[i] if i < clean.seq_len else \"\"\n",
        "        k_tok = corrupt.token_strs[i] if i < corrupt.seq_len else \"\"\n",
        "        diff = \"âœ…\" if i in comp.diff_positions else \"\"\n",
        "        rows.append(\n",
        "            f\"| {i} | {'' if c_id is None else c_id} | {repr(c_tok)} | {'' if k_id is None else k_id} | {repr(k_tok)} | {diff} |\"\n",
        "        )\n",
        "    return header + \"\\n\".join(rows) + \"\\n\"\n",
        "\n",
        "\n",
        "def describe_pair(comp: PairComparison) -> str:\n",
        "    \"\"\"\n",
        "    Human-readable summary.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        \"=== Pair summary ===\\n\"\n",
        "        f\"Clean tokens:   {comp.clean.seq_len}\\n\"\n",
        "        f\"Corrupt tokens: {comp.corrupt.seq_len}\\n\"\n",
        "        f\"Same length?    {comp.same_length}\\n\"\n",
        "        f\"Diff count:     {comp.diff_count}\\n\"\n",
        "        f\"Diff positions: {comp.diff_positions}\\n\"\n",
        "        f\"One-token diff? {comp.one_token_diff}\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Heuristic suggestions (for mismatch debugging)\n",
        "# -----------------------------\n",
        "\n",
        "def suggest_fixes(clean: TokenizationReport, corrupt: TokenizationReport) -> List[str]:\n",
        "    \"\"\"\n",
        "    Heuristics to help the user fix length mismatches / multi-token mismatches.\n",
        "    Not an automatic fixer; it gives actionable suggestions.\n",
        "    \"\"\"\n",
        "    suggestions: List[str] = []\n",
        "\n",
        "    # Length mismatch guidance\n",
        "    if clean.seq_len != corrupt.seq_len:\n",
        "        suggestions.append(\n",
        "            \"Token length mismatch detected. Common causes: whitespace differences, punctuation attachment, \"\n",
        "            \"or swapping a word that tokenizes into a different number of BPE tokens.\"\n",
        "        )\n",
        "        suggestions.append(\n",
        "            \"Try keeping punctuation identical (e.g., 'student.' vs 'student .') and keep spaces consistent around the changed word.\"\n",
        "        )\n",
        "        suggestions.append(\n",
        "            \"Proper nouns are often unstable: try swapping to a more common single-token alternative and re-check.\"\n",
        "        )\n",
        "\n",
        "    # Multi-token difference guidance\n",
        "    diffs = diff_positions(clean.token_ids, corrupt.token_ids)\n",
        "    if clean.seq_len == corrupt.seq_len and len(diffs) != 1:\n",
        "        suggestions.append(\n",
        "            f\"More than one token differs ({len(diffs)}). You want exactly 1 differing BPE token position.\"\n",
        "        )\n",
        "        suggestions.append(\n",
        "            \"Inspect per-token strings around the diff positions; often a punctuation or whitespace token is also changing.\"\n",
        "        )\n",
        "\n",
        "    # Space-specific hint\n",
        "    suggestions.append(\n",
        "        \"Remember GPT-2 BPE: tokens in the middle often include a leading space. \"\n",
        "        \"If you care about the token 'Jones', the actual token is usually ' Jones'.\"\n",
        "    )\n",
        "\n",
        "    return suggestions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tokenization_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tokenization_driver.py\n",
        "\"\"\"\n",
        "Section 3 driver: tokenize clean/corrupt prompts, enforce same-length and one-token-diff,\n",
        "print per-token decomposition, and export a Markdown token table for the report.\n",
        "\n",
        "Usage in Colab:\n",
        "!python tokenization_driver.py\n",
        "\n",
        "Or override defaults by editing the CLEAN_TEXT / CORRUPT_TEXT constants below.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "import tokenization_protocol as tp\n",
        "\n",
        "\n",
        "# Edit these defaults for your own experiment.\n",
        "CLEAN_TEXT = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "CORRUPT_TEXT = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--clean\", type=str, default=CLEAN_TEXT, help=\"Clean prompt text\")\n",
        "    p.add_argument(\"--corrupt\", type=str, default=CORRUPT_TEXT, help=\"Corrupted prompt text\")\n",
        "    p.add_argument(\"--no-require-one-diff\", action=\"store_true\", help=\"Do not require exactly 1 token difference\")\n",
        "    p.add_argument(\"--out_md\", type=str, default=\"token_table.md\", help=\"Output markdown file for token table\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    clean_rep = tp.build_report(bpe, args.clean)\n",
        "    corrupt_rep = tp.build_report(bpe, args.corrupt)\n",
        "    comp = tp.compare_clean_corrupt(clean_rep, corrupt_rep)\n",
        "\n",
        "    print(tp.describe_pair(comp))\n",
        "\n",
        "    print(\"=== Clean prompt ===\")\n",
        "    print(clean_rep.text)\n",
        "    print(\"\\n=== Clean tokens (pos | id | repr(token)) ===\")\n",
        "    print(tp.format_token_list_for_console(clean_rep))\n",
        "\n",
        "    print(\"\\n=== Corrupt prompt ===\")\n",
        "    print(corrupt_rep.text)\n",
        "    print(\"\\n=== Corrupt tokens (pos | id | repr(token)) ===\")\n",
        "    print(tp.format_token_list_for_console(corrupt_rep))\n",
        "\n",
        "    # Enforce constraints as requested by the assignment\n",
        "    require_one = not args.no_require_one_diff\n",
        "    try:\n",
        "        _ = tp.validate_pair(\n",
        "            bpe=bpe,\n",
        "            clean_text=args.clean,\n",
        "            corrupt_text=args.corrupt,\n",
        "            require_same_length=True,\n",
        "            require_one_token_diff=require_one,\n",
        "        )\n",
        "        print(\"\\nâœ… Validation passed.\")\n",
        "    except Exception as e:\n",
        "        print(\"\\nâŒ Validation failed:\")\n",
        "        print(e)\n",
        "        print(\"\\nSuggestions:\")\n",
        "        for s in tp.suggest_fixes(clean_rep, corrupt_rep):\n",
        "            print(\"-\", s)\n",
        "\n",
        "    # Export markdown table for report\n",
        "    md = tp.format_pair_diff_markdown(comp)\n",
        "    out_path = Path(args.out_md)\n",
        "    out_path.write_text(md, encoding=\"utf-8\")\n",
        "    print(f\"\\nWrote Markdown token table to: {out_path.resolve()}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Pair summary ===\n",
            "Clean tokens:   11\n",
            "Corrupt tokens: 11\n",
            "Same length?    True\n",
            "Diff count:     1\n",
            "Diff positions: [1]\n",
            "One-token diff? True\n",
            "\n",
            "=== Clean prompt ===\n",
            "Michelle Jones was a top-notch student. Michelle\n",
            "\n",
            "=== Clean tokens (pos | id | repr(token)) ===\n",
            "00 | 48736 | 'Michelle'\n",
            "01 |  5437 | ' Jones'\n",
            "02 |   373 | ' was'\n",
            "03 |   257 | ' a'\n",
            "04 |  1353 | ' top'\n",
            "05 |    12 | '-'\n",
            "06 |  1662 | 'not'\n",
            "07 |   354 | 'ch'\n",
            "08 |  3710 | ' student'\n",
            "09 |    13 | '.'\n",
            "10 | 16738 | ' Michelle'\n",
            "\n",
            "=== Corrupt prompt ===\n",
            "Michelle Smith was a top-notch student. Michelle\n",
            "\n",
            "=== Corrupt tokens (pos | id | repr(token)) ===\n",
            "00 | 48736 | 'Michelle'\n",
            "01 |  4176 | ' Smith'\n",
            "02 |   373 | ' was'\n",
            "03 |   257 | ' a'\n",
            "04 |  1353 | ' top'\n",
            "05 |    12 | '-'\n",
            "06 |  1662 | 'not'\n",
            "07 |   354 | 'ch'\n",
            "08 |  3710 | ' student'\n",
            "09 |    13 | '.'\n",
            "10 | 16738 | ' Michelle'\n",
            "\n",
            "âœ… Validation passed.\n",
            "\n",
            "Wrote Markdown token table to: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/token_table.md\n"
          ]
        }
      ],
      "source": [
        "!python tokenization_driver.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| pos | clean_id | clean_tok | corrupt_id | corrupt_tok | diff? |\n",
            "|---:|---:|---|---:|---|:---:|\n",
            "| 0 | 48736 | 'Michelle' | 48736 | 'Michelle' |  |\n",
            "| 1 | 5437 | ' Jones' | 4176 | ' Smith' | âœ… |\n",
            "| 2 | 373 | ' was' | 373 | ' was' |  |\n",
            "| 3 | 257 | ' a' | 257 | ' a' |  |\n",
            "| 4 | 1353 | ' top' | 1353 | ' top' |  |\n",
            "| 5 | 12 | '-' | 12 | '-' |  |\n",
            "| 6 | 1662 | 'not' | 1662 | 'not' |  |\n",
            "| 7 | 354 | 'ch' | 354 | 'ch' |  |\n",
            "| 8 | 3710 | ' student' | 3710 | ' student' |  |\n",
            "| 9 | 13 | '.' | 13 | '.' |  |\n",
            "| 10 | 16738 | ' Michelle' | 16738 | ' Michelle' |  |\n"
          ]
        }
      ],
      "source": [
        "!sed -n '1,120p' token_table.md\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_section_3.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section_3.py\n",
        "import pathlib\n",
        "import sys\n",
        "\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "# Colab-friendly: ensure mingpt editable install path is visible during pytest subprocess\n",
        "COLAB_MINGPT_PATH = pathlib.Path(\"/content/src/mingpt\")\n",
        "if COLAB_MINGPT_PATH.exists():\n",
        "    sys.path.append(str(COLAB_MINGPT_PATH))\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "import repo_orientation as ro\n",
        "import tokenization_protocol as tp\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Section 2 tests (repo orientation)\n",
        "# --------------------------\n",
        "\n",
        "def test_mingpt_importable_and_paths_exist():\n",
        "    paths = ro.get_paths()\n",
        "    assert \"mingpt.__file__\" in paths and \"mingpt.model.__file__\" in paths\n",
        "\n",
        "    pkg_path = pathlib.Path(paths[\"mingpt.__file__\"])\n",
        "    model_path = pathlib.Path(paths[\"mingpt.model.__file__\"])\n",
        "    assert pkg_path.exists(), f\"mingpt package file not found: {pkg_path}\"\n",
        "    assert model_path.exists(), f\"mingpt.model file not found: {model_path}\"\n",
        "\n",
        "\n",
        "def test_attn_bias_fix_present_or_applied():\n",
        "    src = ro.read_model_source()\n",
        "    assert ro.attn_bias_fix_present(src), (\n",
        "        \"Required fix not found in mingpt/model.py. \"\n",
        "        \"Expected assert to ignore keys ending with .attn.bias.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def test_forward_pipeline_landmarks_present():\n",
        "    fwd_src = ro.forward_source()\n",
        "    lm = ro.find_forward_landmarks(fwd_src)\n",
        "    assert lm.has_tok_emb, \"Expected token embedding (wte/tok_emb) usage in forward.\"\n",
        "    assert lm.has_pos_emb, \"Expected positional embedding (wpe/pos_emb) usage in forward.\"\n",
        "    assert lm.has_blocks_loop, \"Expected loop over transformer blocks in forward.\"\n",
        "    assert lm.has_ln_f, \"Expected final layer norm ln_f in forward.\"\n",
        "    assert lm.has_lm_head, \"Expected lm_head/logits in forward.\"\n",
        "\n",
        "\n",
        "def test_fast_forward_and_generate_from_scratch():\n",
        "    # Fast test: avoid downloading HF weights.\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"  # tiny\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "    assert logits.shape == (1, 10, cfg.vocab_size)\n",
        "    assert loss is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=5, do_sample=False)\n",
        "    assert out.shape[1] == 15\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Section 3 tests (tokenization protocol)\n",
        "# --------------------------\n",
        "\n",
        "def test_diff_positions_length_mismatch_includes_tail():\n",
        "    a = [1, 2, 3]\n",
        "    b = [1, 2, 3, 4, 5]\n",
        "    diffs = tp.diff_positions(a, b)\n",
        "    assert diffs == [3, 4]\n",
        "\n",
        "\n",
        "def test_compare_reports_detects_one_token_diff_synthetic():\n",
        "    clean = tp.TokenizationReport(\n",
        "        text=\"clean\",\n",
        "        token_ids=[10, 20, 30],\n",
        "        token_strs=[\"a\", \"b\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"abc\",\n",
        "    )\n",
        "    corrupt = tp.TokenizationReport(\n",
        "        text=\"corrupt\",\n",
        "        token_ids=[10, 99, 30],\n",
        "        token_strs=[\"a\", \"X\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"aXc\",\n",
        "    )\n",
        "    comp = tp.compare_clean_corrupt(clean, corrupt)\n",
        "    assert comp.same_length is True\n",
        "    assert comp.diff_positions == [1]\n",
        "    assert comp.diff_count == 1\n",
        "    assert comp.one_token_diff is True\n",
        "\n",
        "\n",
        "def test_assert_one_token_difference_raises_when_multi_diff():\n",
        "    clean = tp.TokenizationReport(\n",
        "        text=\"clean\",\n",
        "        token_ids=[1, 2, 3],\n",
        "        token_strs=[\"a\", \"b\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"abc\",\n",
        "    )\n",
        "    corrupt = tp.TokenizationReport(\n",
        "        text=\"corrupt\",\n",
        "        token_ids=[9, 2, 8],\n",
        "        token_strs=[\"X\", \"b\", \"Y\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"XbY\",\n",
        "    )\n",
        "    comp = tp.compare_clean_corrupt(clean, corrupt)\n",
        "    assert comp.diff_count == 2\n",
        "    with pytest.raises(ValueError):\n",
        "        tp.assert_one_token_difference(comp)\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_bpe_tokenization_roundtrip_and_lengths():\n",
        "    \"\"\"\n",
        "    Slow-ish test because BPETokenizer may download merges/vocab on first use in a fresh runtime.\n",
        "    \"\"\"\n",
        "    from mingpt.bpe import BPETokenizer\n",
        "\n",
        "    try:\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping BPETokenizer test due to tokenizer init/download error: {e}\")\n",
        "\n",
        "    text = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    rep = tp.build_report(bpe, text)\n",
        "\n",
        "    # Basic sanity\n",
        "    assert rep.seq_len > 0\n",
        "    assert len(rep.token_ids) == rep.seq_len\n",
        "    assert len(rep.token_strs) == rep.seq_len\n",
        "\n",
        "    # Roundtrip should contain the key content (exact equality may vary by whitespace normalization)\n",
        "    assert \"Michelle\" in rep.decoded_roundtrip\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_bpe_pair_validation_example_michelle_jones_smith():\n",
        "    \"\"\"\n",
        "    Uses the assignment's canonical-style example to ensure:\n",
        "    - same token length\n",
        "    - ideally a one-token difference (it usually is, but tokenizer quirks can vary)\n",
        "    \"\"\"\n",
        "    from mingpt.bpe import BPETokenizer\n",
        "\n",
        "    try:\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping BPETokenizer test due to tokenizer init/download error: {e}\")\n",
        "\n",
        "    clean = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    corrupt = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "\n",
        "    clean_rep = tp.build_report(bpe, clean)\n",
        "    corrupt_rep = tp.build_report(bpe, corrupt)\n",
        "    comp = tp.compare_clean_corrupt(clean_rep, corrupt_rep)\n",
        "\n",
        "    assert comp.same_length is True, f\"Expected same token length; got {clean_rep.seq_len} vs {corrupt_rep.seq_len}\"\n",
        "\n",
        "    # We prefer one-token diff; if it isn't, we still show it's a valid pair for same-length constraint.\n",
        "    # But for the assignment report you should aim for diff_count == 1.\n",
        "    assert comp.diff_count >= 1\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_slow_from_pretrained_gpt2_loads_and_runs():\n",
        "    \"\"\"\n",
        "    Slow test: downloads and loads GPT-2 weights.\n",
        "    If network/cache issues happen in Colab, we skip rather than fail hard.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    try:\n",
        "        model = GPT.from_pretrained(\"gpt2\")\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping from_pretrained test due to load/download error: {e}\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, 50257, (1, 8), dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "\n",
        "    assert logits.shape == (1, 8, 50257)\n",
        "    assert loss is None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                               [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m10 passed\u001b[0m\u001b[32m in 12.91s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!{sys.executable} -m pytest -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiment_design.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiment_design.py\n",
        "\"\"\"\n",
        "Section 4: Experimental Design (clean/corrupted prompts + target tokens + hypothesis)\n",
        "\n",
        "This module provides:\n",
        "- A structured ExperimentSpec (clean/corrupt prompts, target tokens A/B, hypothesis)\n",
        "- Validation utilities:\n",
        "  - clean/corrupt differ by EXACTLY one BPE token\n",
        "  - same number of tokens\n",
        "  - target tokens A/B are SINGLE BPE tokens (usually with leading space)\n",
        "- Convenience: candidate specs + \"pick the first valid one\" to avoid tokenization surprises.\n",
        "\n",
        "It depends on tokenization_protocol.py (Section 3), which you already implemented.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "import tokenization_protocol as tp\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Data structures\n",
        "# -----------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ExperimentSpec:\n",
        "    \"\"\"\n",
        "    Section 4 \"contract\" for the experiment.\n",
        "\n",
        "    clean_text and corrupt_text:\n",
        "      - must have same BPE token length\n",
        "      - must differ by exactly one BPE token\n",
        "\n",
        "    token_a_str and token_b_str:\n",
        "      - intended to be single BPE tokens for the next-token prediction\n",
        "      - recommended to include leading space (e.g., \" Paris\", \" def\")\n",
        "    \"\"\"\n",
        "    clean_text: str\n",
        "    corrupt_text: str\n",
        "    token_a_str: str\n",
        "    token_b_str: str\n",
        "    hypothesis: str\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ValidatedExperiment:\n",
        "    spec: ExperimentSpec\n",
        "    comparison: tp.PairComparison\n",
        "    changed_position: int\n",
        "    token_a_id: int\n",
        "    token_b_id: int\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Token helpers (Token A / Token B)\n",
        "# -----------------------------\n",
        "\n",
        "def ensure_leading_space(token_str: str) -> str:\n",
        "    \"\"\"\n",
        "    GPT-2 BPE typically encodes mid-sequence words with a leading space.\n",
        "    This helper makes it harder to forget that, but does NOT guarantee single-token.\n",
        "    \"\"\"\n",
        "    if token_str.startswith(\" \"):\n",
        "        return token_str\n",
        "    return \" \" + token_str\n",
        "\n",
        "\n",
        "def single_token_id(bpe: BPETokenizer, token_str: str) -> int:\n",
        "    \"\"\"\n",
        "    Convert a string (e.g., \" def\") into a SINGLE BPE token id.\n",
        "    Raises ValueError if the string tokenizes into multiple tokens.\n",
        "    \"\"\"\n",
        "    ids_2d = bpe(token_str)  # (1, T)\n",
        "    ids = ids_2d[0].tolist()\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(\n",
        "            f\"Target token string must be a single BPE token, but got {len(ids)} tokens for {repr(token_str)}: {ids}\"\n",
        "        )\n",
        "    return int(ids[0])\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Core validation utilities\n",
        "# -----------------------------\n",
        "\n",
        "def changed_token_position(comp: tp.PairComparison) -> int:\n",
        "    \"\"\"\n",
        "    Returns the unique position where clean vs corrupt differ.\n",
        "    Raises if not exactly one differing token.\n",
        "    \"\"\"\n",
        "    tp.assert_one_token_difference(comp)\n",
        "    return int(comp.diff_positions[0])\n",
        "\n",
        "\n",
        "def default_hypothesis(changed_pos: int) -> str:\n",
        "    \"\"\"\n",
        "    A report-ready hypothesis for GPT-2 small activation patching heatmaps.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        f\"Hypothesis: The changed token position (position {changed_pos}) should matter most, \"\n",
        "        \"so patching activations at this position across early-to-mid layers should strongly restore the clean-consistent continuation. \"\n",
        "        \"Middle layers are expected to dominate because they often integrate and route the key conditioning fact/entity forward through the residual stream. \"\n",
        "        \"Late layers may also show a secondary effect near the final token position because they directly refine the next-token logits.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def validate_experiment(bpe: BPETokenizer, spec: ExperimentSpec) -> ValidatedExperiment:\n",
        "    \"\"\"\n",
        "    Full Section 4 validation:\n",
        "    - clean/corrupt must be same length AND differ by exactly one BPE token\n",
        "    - token A and token B must each be single BPE tokens (usually with leading space)\n",
        "    \"\"\"\n",
        "    comp = tp.validate_pair(\n",
        "        bpe=bpe,\n",
        "        clean_text=spec.clean_text,\n",
        "        corrupt_text=spec.corrupt_text,\n",
        "        require_same_length=True,\n",
        "        require_one_token_diff=True,\n",
        "    )\n",
        "    pos = changed_token_position(comp)\n",
        "\n",
        "    token_a_id = single_token_id(bpe, spec.token_a_str)\n",
        "    token_b_id = single_token_id(bpe, spec.token_b_str)\n",
        "\n",
        "    return ValidatedExperiment(\n",
        "        spec=spec,\n",
        "        comparison=comp,\n",
        "        changed_position=pos,\n",
        "        token_a_id=token_a_id,\n",
        "        token_b_id=token_b_id,\n",
        "    )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Candidate specs + auto-pick\n",
        "# -----------------------------\n",
        "\n",
        "def candidate_experiments() -> List[ExperimentSpec]:\n",
        "    \"\"\"\n",
        "    A small pool of \"creative but simple\" candidates.\n",
        "    We DO NOT assume these always pass one-token-diff constraints;\n",
        "    that's why pick_first_valid_experiment() exists.\n",
        "    \"\"\"\n",
        "    # Note: token strings here include leading spaces on purpose.\n",
        "    return [\n",
        "        ExperimentSpec(\n",
        "            clean_text=\"In Python, the keyword to define a function is\",\n",
        "            corrupt_text=\"In JavaScript, the keyword to define a function is\",\n",
        "            token_a_str=\" def\",\n",
        "            token_b_str=\" function\",\n",
        "            hypothesis=\"(auto)\",\n",
        "        ),\n",
        "        ExperimentSpec(\n",
        "            clean_text=\"The capital of France is\",\n",
        "            corrupt_text=\"The capital of Germany is\",\n",
        "            token_a_str=\" Paris\",\n",
        "            token_b_str=\" Berlin\",\n",
        "            hypothesis=\"(auto)\",\n",
        "        ),\n",
        "        ExperimentSpec(\n",
        "            clean_text=\"The chemical symbol for water is\",\n",
        "            corrupt_text=\"The chemical symbol for salt is\",\n",
        "            token_a_str=\" H\",\n",
        "            token_b_str=\" Na\",\n",
        "            hypothesis=\"(auto)\",\n",
        "        ),\n",
        "        ExperimentSpec(\n",
        "            clean_text=\"A triangle has three sides. A\",\n",
        "            corrupt_text=\"A square has three sides. A\",\n",
        "            token_a_str=\" triangle\",\n",
        "            token_b_str=\" square\",\n",
        "            hypothesis=\"(auto)\",\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "\n",
        "def pick_first_valid_experiment(bpe: BPETokenizer, specs: Optional[List[ExperimentSpec]] = None) -> ValidatedExperiment:\n",
        "    \"\"\"\n",
        "    Tries a list of candidate ExperimentSpec and returns the first one that:\n",
        "    - differs by exactly one BPE token\n",
        "    - has equal BPE length\n",
        "    - has single-token target tokens A/B\n",
        "\n",
        "    If none works, raises ValueError with a helpful message.\n",
        "    \"\"\"\n",
        "    specs = specs or candidate_experiments()\n",
        "    errors: List[str] = []\n",
        "\n",
        "    for i, s in enumerate(specs):\n",
        "        # If hypothesis was left as \"(auto)\", fill it after we know the changed position\n",
        "        try:\n",
        "            tmp = validate_experiment(bpe, s if s.hypothesis != \"(auto)\" else s)\n",
        "            if tmp.spec.hypothesis == \"(auto)\":\n",
        "                auto_h = default_hypothesis(tmp.changed_position)\n",
        "                s2 = ExperimentSpec(\n",
        "                    clean_text=s.clean_text,\n",
        "                    corrupt_text=s.corrupt_text,\n",
        "                    token_a_str=s.token_a_str,\n",
        "                    token_b_str=s.token_b_str,\n",
        "                    hypothesis=auto_h,\n",
        "                )\n",
        "                tmp = validate_experiment(bpe, s2)\n",
        "            return tmp\n",
        "        except Exception as e:\n",
        "            errors.append(f\"[Candidate {i}] {e}\")\n",
        "\n",
        "    raise ValueError(\n",
        "        \"None of the candidate experiments passed the strict Section 4 constraints.\\n\"\n",
        "        \"This is normal: GPT-2 BPE tokenization can be surprising.\\n\\n\"\n",
        "        \"What to do:\\n\"\n",
        "        \"1) Provide your own clean/corrupt prompts and re-run the driver with --clean/--corrupt.\\n\"\n",
        "        \"2) Ensure the two prompts differ by only one BPE token (see token tables).\\n\"\n",
        "        \"3) Ensure token A/B are single BPE tokens (often with leading spaces).\\n\\n\"\n",
        "        \"Errors from candidates:\\n\" + \"\\n\".join(errors)\n",
        "    )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Report-oriented formatting\n",
        "# -----------------------------\n",
        "\n",
        "def section4_markdown(valid: ValidatedExperiment) -> str:\n",
        "    \"\"\"\n",
        "    Produces a compact Markdown block you can paste into the PDF report (Section 4).\n",
        "    Includes prompts, token stats, target tokens, and the hypothesis.\n",
        "    \"\"\"\n",
        "    comp = valid.comparison\n",
        "    md_table = tp.format_pair_diff_markdown(comp)\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\"## 4) Experimental Design: Clean/Corrupted Pair + Hypothesis\\n\")\n",
        "    lines.append(\"**Clean prompt:**\")\n",
        "    lines.append(f\"`{valid.spec.clean_text}`\\n\")\n",
        "    lines.append(\"**Corrupted prompt:**\")\n",
        "    lines.append(f\"`{valid.spec.corrupt_text}`\\n\")\n",
        "\n",
        "    lines.append(f\"**Tokenization constraint:** both prompts have **{comp.clean.seq_len}** BPE tokens and differ at exactly **one** token position: **{valid.changed_position}**.\\n\")\n",
        "    lines.append(\"**Token-by-token comparison (diff highlighted):**\\n\")\n",
        "    lines.append(md_table)\n",
        "\n",
        "    lines.append(\"**Target competing tokens (next-token prediction at the last position):**\")\n",
        "    lines.append(f\"- Token A (clean-consistent): `{valid.spec.token_a_str}`  (token id: {valid.token_a_id})\")\n",
        "    lines.append(f\"- Token B (corrupted-consistent): `{valid.spec.token_b_str}`  (token id: {valid.token_b_id})\\n\")\n",
        "\n",
        "    lines.append(\"**Metric used later (matches the handout):**\")\n",
        "    lines.append(\"`logit(Token B) âˆ’ logit(Token A)` from the last-position logits.\\n\")\n",
        "\n",
        "    lines.append(\"**Hypothesis:**\")\n",
        "    lines.append(valid.spec.hypothesis + \"\\n\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiment_design_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiment_design_driver.py\n",
        "\"\"\"\n",
        "Section 4 driver (Colab-friendly).\n",
        "\n",
        "What it does:\n",
        "- Builds/validates an ExperimentSpec:\n",
        "  - clean/corrupt prompts: same length and exactly 1 token difference\n",
        "  - target tokens A/B: each must be a single BPE token id\n",
        "- Prints all evidence needed for the report\n",
        "- Writes a report-ready Markdown file section4.md\n",
        "\n",
        "Run:\n",
        "!python experiment_design_driver.py\n",
        "\n",
        "Or override:\n",
        "!python experiment_design_driver.py --clean \"...\" --corrupt \"...\" --token_a \" ...\" --token_b \" ...\"\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "import tokenization_protocol as tp\n",
        "import experiment_design as ed\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--clean\", type=str, default=None, help=\"Clean prompt text (optional)\")\n",
        "    p.add_argument(\"--corrupt\", type=str, default=None, help=\"Corrupted prompt text (optional)\")\n",
        "    p.add_argument(\"--token_a\", type=str, default=None, help=\"Token A string (optional, usually with leading space)\")\n",
        "    p.add_argument(\"--token_b\", type=str, default=None, help=\"Token B string (optional, usually with leading space)\")\n",
        "    p.add_argument(\"--out_md\", type=str, default=\"section4.md\", help=\"Output markdown file for Section 4\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    if args.clean and args.corrupt and args.token_a and args.token_b:\n",
        "        spec = ed.ExperimentSpec(\n",
        "            clean_text=args.clean,\n",
        "            corrupt_text=args.corrupt,\n",
        "            token_a_str=args.token_a,\n",
        "            token_b_str=args.token_b,\n",
        "            hypothesis=\"(auto)\",  # we will fill after validation\n",
        "        )\n",
        "        valid = ed.validate_experiment(bpe, spec)\n",
        "        # Fill automatic hypothesis (now that we know changed position)\n",
        "        spec2 = ed.ExperimentSpec(\n",
        "            clean_text=spec.clean_text,\n",
        "            corrupt_text=spec.corrupt_text,\n",
        "            token_a_str=spec.token_a_str,\n",
        "            token_b_str=spec.token_b_str,\n",
        "            hypothesis=ed.default_hypothesis(valid.changed_position),\n",
        "        )\n",
        "        valid = ed.validate_experiment(bpe, spec2)\n",
        "    else:\n",
        "        # Auto-pick the first candidate that satisfies strict constraints\n",
        "        valid = ed.pick_first_valid_experiment(bpe)\n",
        "\n",
        "    comp = valid.comparison\n",
        "\n",
        "    print(tp.describe_pair(comp))\n",
        "    print(f\"Changed token position: {valid.changed_position}\")\n",
        "\n",
        "    print(\"\\n=== Clean prompt ===\")\n",
        "    print(valid.spec.clean_text)\n",
        "    print(\"\\n=== Corrupted prompt ===\")\n",
        "    print(valid.spec.corrupt_text)\n",
        "\n",
        "    print(\"\\n=== Token-by-token (clean) ===\")\n",
        "    print(tp.format_token_list_for_console(comp.clean))\n",
        "\n",
        "    print(\"\\n=== Token-by-token (corrupt) ===\")\n",
        "    print(tp.format_token_list_for_console(comp.corrupt))\n",
        "\n",
        "    print(\"\\n=== Target tokens ===\")\n",
        "    print(f\"Token A (clean-consistent): {repr(valid.spec.token_a_str)} -> id {valid.token_a_id}\")\n",
        "    print(f\"Token B (corrupted-consistent): {repr(valid.spec.token_b_str)} -> id {valid.token_b_id}\")\n",
        "\n",
        "    print(\"\\n=== Hypothesis ===\")\n",
        "    print(valid.spec.hypothesis)\n",
        "\n",
        "    md = ed.section4_markdown(valid)\n",
        "    out_path = Path(args.out_md)\n",
        "    out_path.write_text(md, encoding=\"utf-8\")\n",
        "    print(f\"\\nWrote Section 4 Markdown to: {out_path.resolve()}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_section_4.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section_4.py\n",
        "import pathlib\n",
        "import sys\n",
        "\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "# Colab-friendly: ensure mingpt editable install path is visible during pytest subprocess\n",
        "COLAB_MINGPT_PATH = pathlib.Path(\"/content/src/mingpt\")\n",
        "if COLAB_MINGPT_PATH.exists():\n",
        "    sys.path.append(str(COLAB_MINGPT_PATH))\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "import repo_orientation as ro\n",
        "import tokenization_protocol as tp\n",
        "import experiment_design as ed\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Section 2 tests (repo orientation)\n",
        "# --------------------------\n",
        "\n",
        "def test_mingpt_importable_and_paths_exist():\n",
        "    paths = ro.get_paths()\n",
        "    assert \"mingpt.__file__\" in paths and \"mingpt.model.__file__\" in paths\n",
        "\n",
        "    pkg_path = pathlib.Path(paths[\"mingpt.__file__\"])\n",
        "    model_path = pathlib.Path(paths[\"mingpt.model.__file__\"])\n",
        "    assert pkg_path.exists(), f\"mingpt package file not found: {pkg_path}\"\n",
        "    assert model_path.exists(), f\"mingpt.model file not found: {model_path}\"\n",
        "\n",
        "\n",
        "def test_attn_bias_fix_present_or_applied():\n",
        "    src = ro.read_model_source()\n",
        "    assert ro.attn_bias_fix_present(src), (\n",
        "        \"Required fix not found in mingpt/model.py. \"\n",
        "        \"Expected assert to ignore keys ending with .attn.bias.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def test_forward_pipeline_landmarks_present():\n",
        "    fwd_src = ro.forward_source()\n",
        "    lm = ro.find_forward_landmarks(fwd_src)\n",
        "    assert lm.has_tok_emb, \"Expected token embedding (wte/tok_emb) usage in forward.\"\n",
        "    assert lm.has_pos_emb, \"Expected positional embedding (wpe/pos_emb) usage in forward.\"\n",
        "    assert lm.has_blocks_loop, \"Expected loop over transformer blocks in forward.\"\n",
        "    assert lm.has_ln_f, \"Expected final layer norm ln_f in forward.\"\n",
        "    assert lm.has_lm_head, \"Expected lm_head/logits in forward.\"\n",
        "\n",
        "\n",
        "def test_fast_forward_and_generate_from_scratch():\n",
        "    # Fast test: avoid downloading HF weights.\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"  # tiny\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "    assert logits.shape == (1, 10, cfg.vocab_size)\n",
        "    assert loss is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=5, do_sample=False)\n",
        "    assert out.shape[1] == 15\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Section 3 tests (tokenization protocol)\n",
        "# --------------------------\n",
        "\n",
        "def test_diff_positions_length_mismatch_includes_tail():\n",
        "    a = [1, 2, 3]\n",
        "    b = [1, 2, 3, 4, 5]\n",
        "    diffs = tp.diff_positions(a, b)\n",
        "    assert diffs == [3, 4]\n",
        "\n",
        "\n",
        "def test_compare_reports_detects_one_token_diff_synthetic():\n",
        "    clean = tp.TokenizationReport(\n",
        "        text=\"clean\",\n",
        "        token_ids=[10, 20, 30],\n",
        "        token_strs=[\"a\", \"b\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"abc\",\n",
        "    )\n",
        "    corrupt = tp.TokenizationReport(\n",
        "        text=\"corrupt\",\n",
        "        token_ids=[10, 99, 30],\n",
        "        token_strs=[\"a\", \"X\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"aXc\",\n",
        "    )\n",
        "    comp = tp.compare_clean_corrupt(clean, corrupt)\n",
        "    assert comp.same_length is True\n",
        "    assert comp.diff_positions == [1]\n",
        "    assert comp.diff_count == 1\n",
        "    assert comp.one_token_diff is True\n",
        "\n",
        "\n",
        "def test_assert_one_token_difference_raises_when_multi_diff():\n",
        "    clean = tp.TokenizationReport(\n",
        "        text=\"clean\",\n",
        "        token_ids=[1, 2, 3],\n",
        "        token_strs=[\"a\", \"b\", \"c\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"abc\",\n",
        "    )\n",
        "    corrupt = tp.TokenizationReport(\n",
        "        text=\"corrupt\",\n",
        "        token_ids=[9, 2, 8],\n",
        "        token_strs=[\"X\", \"b\", \"Y\"],\n",
        "        seq_len=3,\n",
        "        decoded_roundtrip=\"XbY\",\n",
        "    )\n",
        "    comp = tp.compare_clean_corrupt(clean, corrupt)\n",
        "    assert comp.diff_count == 2\n",
        "    with pytest.raises(ValueError):\n",
        "        tp.assert_one_token_difference(comp)\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_bpe_tokenization_roundtrip_and_lengths():\n",
        "    \"\"\"\n",
        "    Slow-ish test because BPETokenizer may download merges/vocab on first use in a fresh runtime.\n",
        "    \"\"\"\n",
        "    from mingpt.bpe import BPETokenizer\n",
        "\n",
        "    try:\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping BPETokenizer test due to tokenizer init/download error: {e}\")\n",
        "\n",
        "    text = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    rep = tp.build_report(bpe, text)\n",
        "\n",
        "    # Basic sanity\n",
        "    assert rep.seq_len > 0\n",
        "    assert len(rep.token_ids) == rep.seq_len\n",
        "    assert len(rep.token_strs) == rep.seq_len\n",
        "\n",
        "    # Roundtrip should contain the key content (exact equality may vary by whitespace normalization)\n",
        "    assert \"Michelle\" in rep.decoded_roundtrip\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_bpe_pair_validation_example_michelle_jones_smith():\n",
        "    \"\"\"\n",
        "    Uses the assignment's canonical-style example to ensure:\n",
        "    - same token length\n",
        "    - at least one differing token (ideally one)\n",
        "    \"\"\"\n",
        "    from mingpt.bpe import BPETokenizer\n",
        "\n",
        "    try:\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping BPETokenizer test due to tokenizer init/download error: {e}\")\n",
        "\n",
        "    clean = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    corrupt = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "\n",
        "    clean_rep = tp.build_report(bpe, clean)\n",
        "    corrupt_rep = tp.build_report(bpe, corrupt)\n",
        "    comp = tp.compare_clean_corrupt(clean_rep, corrupt_rep)\n",
        "\n",
        "    assert comp.same_length is True, f\"Expected same token length; got {clean_rep.seq_len} vs {corrupt_rep.seq_len}\"\n",
        "    assert comp.diff_count >= 1\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Section 4 tests (experiment design)\n",
        "# --------------------------\n",
        "\n",
        "def test_changed_token_position_returns_correct_pos_synthetic():\n",
        "    clean = tp.TokenizationReport(\n",
        "        text=\"clean\",\n",
        "        token_ids=[1, 2, 3, 4],\n",
        "        token_strs=[\"a\", \"b\", \"c\", \"d\"],\n",
        "        seq_len=4,\n",
        "        decoded_roundtrip=\"abcd\",\n",
        "    )\n",
        "    corrupt = tp.TokenizationReport(\n",
        "        text=\"corrupt\",\n",
        "        token_ids=[1, 99, 3, 4],\n",
        "        token_strs=[\"a\", \"X\", \"c\", \"d\"],\n",
        "        seq_len=4,\n",
        "        decoded_roundtrip=\"aXcd\",\n",
        "    )\n",
        "    comp = tp.compare_clean_corrupt(clean, corrupt)\n",
        "    assert comp.one_token_diff is True\n",
        "    assert ed.changed_token_position(comp) == 1\n",
        "\n",
        "\n",
        "def test_default_hypothesis_mentions_changed_position():\n",
        "    h = ed.default_hypothesis(7)\n",
        "    assert \"position 7\" in h\n",
        "\n",
        "\n",
        "def test_ensure_leading_space_adds_space_when_missing():\n",
        "    assert ed.ensure_leading_space(\"Paris\") == \" Paris\"\n",
        "    assert ed.ensure_leading_space(\" Paris\") == \" Paris\"\n",
        "\n",
        "\n",
        "def test_single_token_id_raises_for_multi_token_string_with_dummy_tokenizer():\n",
        "    class DummyBPE:\n",
        "        def __call__(self, s: str):\n",
        "            # Return (1, T) tensor\n",
        "            if s == \" def\":\n",
        "                return torch.tensor([[10]], dtype=torch.long)\n",
        "            if s == \" function\":\n",
        "                return torch.tensor([[20]], dtype=torch.long)\n",
        "            if s == \" JavaScript\":\n",
        "                return torch.tensor([[1, 2]], dtype=torch.long)  # multi-token\n",
        "            return torch.tensor([[999]], dtype=torch.long)\n",
        "\n",
        "    bpe = DummyBPE()\n",
        "    assert ed.single_token_id(bpe, \" def\") == 10\n",
        "    assert ed.single_token_id(bpe, \" function\") == 20\n",
        "    with pytest.raises(ValueError):\n",
        "        _ = ed.single_token_id(bpe, \" JavaScript\")\n",
        "\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_pick_first_valid_experiment_runs_or_skips():\n",
        "    \"\"\"\n",
        "    This validates the *real* Section 4 pipeline using BPETokenizer.\n",
        "    It may skip if tokenizer initialization/download fails OR if none of the candidates satisfy\n",
        "    the strict one-token-diff constraint in this environment.\n",
        "    \"\"\"\n",
        "    from mingpt.bpe import BPETokenizer\n",
        "\n",
        "    try:\n",
        "        bpe = BPETokenizer()\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping Section 4 BPETokenizer test due to tokenizer init/download error: {e}\")\n",
        "\n",
        "    try:\n",
        "        valid = ed.pick_first_valid_experiment(bpe)\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping because no candidate spec validated under strict constraints: {e}\")\n",
        "\n",
        "    assert valid.comparison.one_token_diff is True\n",
        "    assert isinstance(valid.changed_position, int)\n",
        "    assert isinstance(valid.token_a_id, int)\n",
        "    assert isinstance(valid.token_b_id, int)\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Slow model-weight test (optional)\n",
        "# --------------------------\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_slow_from_pretrained_gpt2_loads_and_runs():\n",
        "    \"\"\"\n",
        "    Slow test: downloads and loads GPT-2 weights.\n",
        "    If network/cache issues happen in Colab, we skip rather than fail hard.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    try:\n",
        "        model = GPT.from_pretrained(\"gpt2\")\n",
        "    except Exception as e:\n",
        "        pytest.skip(f\"Skipping from_pretrained test due to load/download error: {e}\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, 50257, (1, 8), dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "\n",
        "    assert logits.shape == (1, 8, 50257)\n",
        "    assert loss is None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Pair summary ===\n",
            "Clean tokens:   10\n",
            "Corrupt tokens: 10\n",
            "Same length?    True\n",
            "Diff count:     1\n",
            "Diff positions: [1]\n",
            "One-token diff? True\n",
            "\n",
            "Changed token position: 1\n",
            "\n",
            "=== Clean prompt ===\n",
            "In Python, the keyword to define a function is\n",
            "\n",
            "=== Corrupted prompt ===\n",
            "In JavaScript, the keyword to define a function is\n",
            "\n",
            "=== Token-by-token (clean) ===\n",
            "00 |   818 | 'In'\n",
            "01 | 11361 | ' Python'\n",
            "02 |    11 | ','\n",
            "03 |   262 | ' the'\n",
            "04 | 21179 | ' keyword'\n",
            "05 |   284 | ' to'\n",
            "06 |  8160 | ' define'\n",
            "07 |   257 | ' a'\n",
            "08 |  2163 | ' function'\n",
            "09 |   318 | ' is'\n",
            "\n",
            "=== Token-by-token (corrupt) ===\n",
            "00 |   818 | 'In'\n",
            "01 | 11933 | ' JavaScript'\n",
            "02 |    11 | ','\n",
            "03 |   262 | ' the'\n",
            "04 | 21179 | ' keyword'\n",
            "05 |   284 | ' to'\n",
            "06 |  8160 | ' define'\n",
            "07 |   257 | ' a'\n",
            "08 |  2163 | ' function'\n",
            "09 |   318 | ' is'\n",
            "\n",
            "=== Target tokens ===\n",
            "Token A (clean-consistent): ' def' -> id 825\n",
            "Token B (corrupted-consistent): ' function' -> id 2163\n",
            "\n",
            "=== Hypothesis ===\n",
            "Hypothesis: The changed token position (position 1) should matter most, so patching activations at this position across early-to-mid layers should strongly restore the clean-consistent continuation. Middle layers are expected to dominate because they often integrate and route the key conditioning fact/entity forward through the residual stream. Late layers may also show a secondary effect near the final token position because they directly refine the next-token logits.\n",
            "\n",
            "Wrote Section 4 Markdown to: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/section4.md\n"
          ]
        }
      ],
      "source": [
        "!python experiment_design_driver.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Pair summary ===\n",
            "Clean tokens:   10\n",
            "Corrupt tokens: 10\n",
            "Same length?    True\n",
            "Diff count:     1\n",
            "Diff positions: [1]\n",
            "One-token diff? True\n",
            "\n",
            "Changed token position: 1\n",
            "\n",
            "=== Clean prompt ===\n",
            "In Python, the keyword to define a function is\n",
            "\n",
            "=== Corrupted prompt ===\n",
            "In JavaScript, the keyword to define a function is\n",
            "\n",
            "=== Token-by-token (clean) ===\n",
            "00 |   818 | 'In'\n",
            "01 | 11361 | ' Python'\n",
            "02 |    11 | ','\n",
            "03 |   262 | ' the'\n",
            "04 | 21179 | ' keyword'\n",
            "05 |   284 | ' to'\n",
            "06 |  8160 | ' define'\n",
            "07 |   257 | ' a'\n",
            "08 |  2163 | ' function'\n",
            "09 |   318 | ' is'\n",
            "\n",
            "=== Token-by-token (corrupt) ===\n",
            "00 |   818 | 'In'\n",
            "01 | 11933 | ' JavaScript'\n",
            "02 |    11 | ','\n",
            "03 |   262 | ' the'\n",
            "04 | 21179 | ' keyword'\n",
            "05 |   284 | ' to'\n",
            "06 |  8160 | ' define'\n",
            "07 |   257 | ' a'\n",
            "08 |  2163 | ' function'\n",
            "09 |   318 | ' is'\n",
            "\n",
            "=== Target tokens ===\n",
            "Token A (clean-consistent): ' def' -> id 825\n",
            "Token B (corrupted-consistent): ' function' -> id 2163\n",
            "\n",
            "=== Hypothesis ===\n",
            "Hypothesis: The changed token position (position 1) should matter most, so patching activations at this position across early-to-mid layers should strongly restore the clean-consistent continuation. Middle layers are expected to dominate because they often integrate and route the key conditioning fact/entity forward through the residual stream. Late layers may also show a secondary effect near the final token position because they directly refine the next-token logits.\n",
            "\n",
            "Wrote Section 4 Markdown to: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/section4.md\n"
          ]
        }
      ],
      "source": [
        "!python experiment_design_driver.py \\\n",
        "  --clean \"In Python, the keyword to define a function is\" \\\n",
        "  --corrupt \"In JavaScript, the keyword to define a function is\" \\\n",
        "  --token_a \" def\" \\\n",
        "  --token_b \" function\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                          [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m15 passed\u001b[0m\u001b[32m in 13.23s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m pytest -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting apply_section5_patch.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile apply_section5_patch.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import pathlib\n",
        "import re\n",
        "\n",
        "import mingpt.model\n",
        "\n",
        "\n",
        "def ensure_typing_import(src: str) -> str:\n",
        "    \"\"\"\n",
        "    Ensure we have: from typing import Any, Dict, List, Optional\n",
        "    We insert it right after the torch imports if not present.\n",
        "    \"\"\"\n",
        "    need_line = \"from typing import Any, Dict, List, Optional\"\n",
        "    if need_line in src:\n",
        "        return src\n",
        "\n",
        "    # Try inserting after torch imports (stable in minGPT)\n",
        "    pattern = r\"(import torch\\s*\\nimport torch\\.nn as nn\\s*\\n)\"\n",
        "    m = re.search(pattern, src)\n",
        "    if not m:\n",
        "        # Fallback: insert after \"import math\"\n",
        "        pattern2 = r\"(import math\\s*\\n)\"\n",
        "        m2 = re.search(pattern2, src)\n",
        "        if not m2:\n",
        "            raise RuntimeError(\"Could not find a safe place to insert typing imports.\")\n",
        "        insert_at = m2.end(1)\n",
        "        return src[:insert_at] + \"\\n\" + need_line + \"\\n\" + src[insert_at:]\n",
        "\n",
        "    insert_at = m.end(1)\n",
        "    return src[:insert_at] + need_line + \"\\n\" + src[insert_at:]\n",
        "\n",
        "\n",
        "def insert_instrumentation_attributes(src: str) -> str:\n",
        "    \"\"\"\n",
        "    Insert instrumentation attributes into GPT.__init__ (only once).\n",
        "    We place them right after the parameter count print.\n",
        "    \"\"\"\n",
        "    if \"self.clean_activations\" in src:\n",
        "        return src\n",
        "\n",
        "    marker = 'print(\"number of parameters: %.2fM\" % (n_params/1e6,))'\n",
        "    if marker not in src:\n",
        "        raise RuntimeError(\"Could not find the parameter-count print marker in GPT.__init__.\")\n",
        "\n",
        "    inject = (\n",
        "        marker\n",
        "        + \"\\n\\n\"\n",
        "        + \"        # --- Mechanistic interpretability instrumentation (Section 5) ---\\n\"\n",
        "        + \"        # clean cache: list[layer][position] -> Tensor(d_model) for batch element 0\\n\"\n",
        "        + \"        self.clean_activations: Optional[List[List[torch.Tensor]]] = None\\n\"\n",
        "        + \"        self.clean_activation_meta: Optional[Dict[str, int]] = None\\n\"\n",
        "        + \"        # last recorded activations (debug/inspection; does NOT overwrite clean cache)\\n\"\n",
        "        + \"        self.last_activations: Optional[List[List[torch.Tensor]]] = None\\n\"\n",
        "    )\n",
        "    return src.replace(marker, inject)\n",
        "\n",
        "\n",
        "def insert_clear_method_if_missing(src: str) -> str:\n",
        "    \"\"\"\n",
        "    Add a small helper method to clear clean cache (only once).\n",
        "    We insert it right before forward() definition.\n",
        "    \"\"\"\n",
        "    if \"def clear_clean_activations\" in src:\n",
        "        return src\n",
        "\n",
        "    # Insert before the forward definition (original minGPT has `def forward(self, idx, targets=None):`)\n",
        "    anchor = \"    def forward(self, idx, targets=None):\"\n",
        "    if anchor not in src:\n",
        "        # Maybe forward already patched; insert before `def forward(` anyway\n",
        "        m = re.search(r\"\\n\\s*def forward\\(\", src)\n",
        "        if not m:\n",
        "            raise RuntimeError(\"Could not find forward() to insert clear_clean_activations() before.\")\n",
        "        insert_at = m.start()\n",
        "        helper = (\n",
        "            \"\\n\"\n",
        "            \"    def clear_clean_activations(self) -> None:\\n\"\n",
        "            \"        \\\"\\\"\\\"Clear the stored clean activation cache (Section 5).\\\"\\\"\\\"\\n\"\n",
        "            \"        self.clean_activations = None\\n\"\n",
        "            \"        self.clean_activation_meta = None\\n\"\n",
        "            \"\\n\"\n",
        "        )\n",
        "        return src[:insert_at] + helper + src[insert_at:]\n",
        "\n",
        "    helper = (\n",
        "        \"\\n\"\n",
        "        \"    def clear_clean_activations(self) -> None:\\n\"\n",
        "        \"        \\\"\\\"\\\"Clear the stored clean activation cache (Section 5).\\\"\\\"\\\"\\n\"\n",
        "        \"        self.clean_activations = None\\n\"\n",
        "        \"        self.clean_activation_meta = None\\n\"\n",
        "        \"\\n\"\n",
        "    )\n",
        "    return src.replace(anchor, helper + anchor)\n",
        "\n",
        "\n",
        "def replace_forward_with_instrumented(src: str) -> str:\n",
        "    \"\"\"\n",
        "    Replace GPT.forward with an instrumented version that can record activations:\n",
        "      - after each transformer block\n",
        "      - for each token position\n",
        "    Stored as: list[layer][position] -> Tensor(d_model), for batch element 0\n",
        "    \"\"\"\n",
        "    new_forward = r'''\n",
        "    def forward(\n",
        "        self,\n",
        "        idx,\n",
        "        targets=None,\n",
        "        *,\n",
        "        record_activations: bool = False,\n",
        "        cache_activations: bool = False,\n",
        "        overwrite_cache: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Forward pass with optional activation recording (Section 5).\n",
        "\n",
        "        Activation definition (standardized for this assignment):\n",
        "          - residual stream output AFTER each transformer block\n",
        "          - recorded for each token position\n",
        "          - stored for batch element 0 only\n",
        "          - deep-copied via detach().clone()\n",
        "\n",
        "        Storage:\n",
        "          - self.clean_activations: persistent \"clean run cache\" (read later for patching)\n",
        "          - self.last_activations: last recorded activations (debug/inspection)\n",
        "        \"\"\"\n",
        "        # If we're caching, we must record\n",
        "        record_activations = bool(record_activations or cache_activations)\n",
        "\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)  # (1, t)\n",
        "\n",
        "        # Clear last_activations to avoid stale reads\n",
        "        self.last_activations = None\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx)  # (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos)  # (1, t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "\n",
        "        acts = None\n",
        "        if record_activations:\n",
        "            acts = []  # list[layer][pos] -> Tensor(d_model)\n",
        "\n",
        "        for layer_idx, block in enumerate(self.transformer.h):\n",
        "            x = block(x)\n",
        "\n",
        "            if record_activations:\n",
        "                # store ONLY batch element 0\n",
        "                layer_acts = []\n",
        "                for p in range(t):\n",
        "                    # defensive copy: detach + clone\n",
        "                    layer_acts.append(x[0, p, :].detach().clone())\n",
        "                acts.append(layer_acts)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        # finalize activation storage\n",
        "        if record_activations:\n",
        "            self.last_activations = acts\n",
        "\n",
        "        if cache_activations:\n",
        "            if (self.clean_activations is not None) and (not overwrite_cache):\n",
        "                raise RuntimeError(\n",
        "                    \"Clean activation cache already exists. \"\n",
        "                    \"Pass overwrite_cache=True (or call model.clear_clean_activations()) \"\n",
        "                    \"to replace it for a new clean prompt.\"\n",
        "                )\n",
        "\n",
        "            self.clean_activations = acts\n",
        "            self.clean_activation_meta = {\n",
        "                \"seq_len\": int(t),\n",
        "                \"n_layer\": int(len(self.transformer.h)),\n",
        "                \"d_model\": int(logits.shape[-1] if False else x.shape[-1]),  # x is (b,t,d_model) here pre-ln_f? ln_f preserves size\n",
        "            }\n",
        "\n",
        "        return logits, loss\n",
        "'''.strip(\"\\n\")\n",
        "\n",
        "    # Replace ONLY the forward() definition inside GPT, stopping before generate()\n",
        "    # We match from `def forward(self, idx, targets=None):` up to just before `@torch.no_grad()` of generate.\n",
        "    pattern = r\"\\n\\s*def forward\\(self, idx, targets=None\\):\\n(?:.|\\n)*?(?=\\n\\s*@torch\\.no_grad\\(\\)\\n\\s*def generate)\"\n",
        "    if not re.search(pattern, src):\n",
        "        # If forward signature already changed, match more generally:\n",
        "        pattern2 = r\"\\n\\s*def forward\\([^\\)]*\\):\\n(?:.|\\n)*?(?=\\n\\s*@torch\\.no_grad\\(\\)\\n\\s*def generate)\"\n",
        "        if not re.search(pattern2, src):\n",
        "            raise RuntimeError(\"Could not find GPT.forward block to replace (before generate).\")\n",
        "        src, n = re.subn(pattern2, \"\\n\" + new_forward + \"\\n\", src, count=1)\n",
        "        if n != 1:\n",
        "            raise RuntimeError(\"Unexpected number of replacements for forward().\")\n",
        "        return src\n",
        "\n",
        "    src, n = re.subn(pattern, \"\\n\" + new_forward + \"\\n\", src, count=1)\n",
        "    if n != 1:\n",
        "        raise RuntimeError(\"Unexpected number of replacements for forward().\")\n",
        "    return src\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    src = path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "    src = ensure_typing_import(src)\n",
        "    src = insert_instrumentation_attributes(src)\n",
        "    src = insert_clear_method_if_missing(src)\n",
        "    src = replace_forward_with_instrumented(src)\n",
        "\n",
        "    path.write_text(src, encoding=\"utf-8\")\n",
        "    print(f\"âœ… Section 5 patch applied to: {path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Section 5 patch applied to: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/src/mingpt/mingpt/model.py\n"
          ]
        }
      ],
      "source": [
        "!python apply_section5_patch.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 0.14M\n",
            "layers cached: 3\n",
            "positions cached (layer 0): 10\n",
            "one activation shape: (48,)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from mingpt.model import GPT\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "set_seed(123)\n",
        "\n",
        "cfg = GPT.get_default_config()\n",
        "cfg.model_type = \"gpt-nano\"\n",
        "cfg.vocab_size = 1000\n",
        "cfg.block_size = 64\n",
        "\n",
        "m = GPT(cfg).eval()\n",
        "idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits, _ = m(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "print(\"layers cached:\", len(m.clean_activations))\n",
        "print(\"positions cached (layer 0):\", len(m.clean_activations[0]))\n",
        "print(\"one activation shape:\", tuple(m.clean_activations[0][0].shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_section_5.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section_5.py\n",
        "import pathlib\n",
        "import sys\n",
        "\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "# If you happen to be in a Colab-like layout, keep this harmless path add.\n",
        "COLAB_MINGPT_PATH = pathlib.Path(\"/content/src/mingpt\")\n",
        "if COLAB_MINGPT_PATH.exists():\n",
        "    sys.path.append(str(COLAB_MINGPT_PATH))\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "# --------------------------\n",
        "# Section 2 tests (repo orientation)\n",
        "# --------------------------\n",
        "\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ForwardLandmarks:\n",
        "    has_tok_emb: bool\n",
        "    has_pos_emb: bool\n",
        "    has_blocks_loop: bool\n",
        "    has_ln_f: bool\n",
        "    has_lm_head: bool\n",
        "\n",
        "\n",
        "def get_paths() -> Dict[str, str]:\n",
        "    pkg_path = pathlib.Path(mingpt.__file__).resolve()\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return {\n",
        "        \"mingpt.__file__\": str(pkg_path),\n",
        "        \"mingpt.model.__file__\": str(model_path),\n",
        "    }\n",
        "\n",
        "\n",
        "def read_model_source() -> str:\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return model_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "def attn_bias_fix_present(model_source: str) -> bool:\n",
        "    return 'len([k for k in sd if not k.endswith(\".attn.bias\")])' in model_source\n",
        "\n",
        "\n",
        "def forward_source() -> str:\n",
        "    return inspect.getsource(GPT.forward)\n",
        "\n",
        "\n",
        "def find_forward_landmarks(src: str) -> ForwardLandmarks:\n",
        "    has_tok_emb = \"tok_emb\" in src and \"wte\" in src\n",
        "    has_pos_emb = \"pos_emb\" in src and \"wpe\" in src\n",
        "    has_blocks_loop = (\"for block in self.transformer.h\" in src) or (\"for layer_idx, block in enumerate(self.transformer.h\" in src)\n",
        "    has_ln_f = \"ln_f\" in src\n",
        "    has_lm_head = \"lm_head\" in src and \"logits\" in src\n",
        "    return ForwardLandmarks(\n",
        "        has_tok_emb=has_tok_emb,\n",
        "        has_pos_emb=has_pos_emb,\n",
        "        has_blocks_loop=has_blocks_loop,\n",
        "        has_ln_f=has_ln_f,\n",
        "        has_lm_head=has_lm_head,\n",
        "    )\n",
        "\n",
        "\n",
        "def test_mingpt_importable_and_paths_exist():\n",
        "    paths = get_paths()\n",
        "    assert \"mingpt.__file__\" in paths and \"mingpt.model.__file__\" in paths\n",
        "\n",
        "    pkg_path = pathlib.Path(paths[\"mingpt.__file__\"])\n",
        "    model_path = pathlib.Path(paths[\"mingpt.model.__file__\"])\n",
        "    assert pkg_path.exists(), f\"mingpt package file not found: {pkg_path}\"\n",
        "    assert model_path.exists(), f\"mingpt.model file not found: {model_path}\"\n",
        "\n",
        "\n",
        "def test_attn_bias_fix_present_or_applied():\n",
        "    src = read_model_source()\n",
        "    assert attn_bias_fix_present(src), (\n",
        "        \"Required fix not found in mingpt/model.py. \"\n",
        "        \"Expected assert to ignore keys ending with .attn.bias.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def test_forward_pipeline_landmarks_present():\n",
        "    fwd_src = forward_source()\n",
        "    lm = find_forward_landmarks(fwd_src)\n",
        "    assert lm.has_tok_emb, \"Expected token embedding (wte/tok_emb) usage in forward.\"\n",
        "    assert lm.has_pos_emb, \"Expected positional embedding (wpe/pos_emb) usage in forward.\"\n",
        "    assert lm.has_blocks_loop, \"Expected loop over transformer blocks in forward.\"\n",
        "    assert lm.has_ln_f, \"Expected final layer norm ln_f in forward.\"\n",
        "    assert lm.has_lm_head, \"Expected lm_head/logits in forward.\"\n",
        "\n",
        "\n",
        "def test_fast_forward_and_generate_from_scratch():\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg)\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "    assert logits.shape == (1, 10, cfg.vocab_size)\n",
        "    assert loss is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=5, do_sample=False)\n",
        "    assert out.shape[1] == 15\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Section 5 tests (activation recording / clean cache)\n",
        "# --------------------------\n",
        "\n",
        "def _make_tiny_gpt():\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg).eval()\n",
        "    return model, cfg\n",
        "\n",
        "\n",
        "def test_section5_cache_structure_and_shapes():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 12\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    assert model.clean_activations is not None\n",
        "    assert isinstance(model.clean_activations, list)\n",
        "    assert len(model.clean_activations) == len(model.transformer.h)  # n_layer\n",
        "    assert len(model.clean_activations[0]) == T\n",
        "\n",
        "    # each [layer][pos] must be (d_model,)\n",
        "    d_model = model.transformer.wte.weight.shape[1]\n",
        "    a00 = model.clean_activations[0][0]\n",
        "    assert tuple(a00.shape) == (d_model,)\n",
        "    assert a00.requires_grad is False\n",
        "\n",
        "\n",
        "def test_section5_cache_uses_detach_clone_not_views():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 6\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    # If we had stored views into the same underlying tensor, data_ptr() would often match.\n",
        "    a0 = model.clean_activations[0][0]\n",
        "    a1 = model.clean_activations[0][1]\n",
        "    assert a0.data_ptr() != a1.data_ptr(), \"Expected clone()d per-position tensors with distinct storage.\"\n",
        "\n",
        "\n",
        "def test_section5_logits_identical_with_and_without_recording():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 10\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits1, _ = model(idx)  # normal\n",
        "        logits2, _ = model(idx, record_activations=True, cache_activations=False)  # recording only\n",
        "\n",
        "    assert torch.allclose(logits1, logits2), \"Activation recording must not change logits.\"\n",
        "\n",
        "\n",
        "def test_section5_clean_cache_not_overwritten_unless_requested():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    idx1 = torch.randint(0, cfg.vocab_size, (1, 8), dtype=torch.long)\n",
        "    idx2 = torch.randint(0, cfg.vocab_size, (1, 8), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx1, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    # snapshot values\n",
        "    snap = [[t.clone() for t in layer] for layer in model.clean_activations]\n",
        "\n",
        "    # normal forward must not change cache\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx2)\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(len(snap[L])):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p])\n",
        "\n",
        "    # recording-only must not change clean cache\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx2, record_activations=True, cache_activations=False)\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(len(snap[L])):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p])\n",
        "\n",
        "    # caching again WITHOUT overwrite must raise\n",
        "    with pytest.raises(RuntimeError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(idx2, cache_activations=True, overwrite_cache=False)\n",
        "\n",
        "\n",
        "def test_section5_batch_behavior_records_only_first_element():\n",
        "    model1, cfg = _make_tiny_gpt()\n",
        "    model2, _ = _make_tiny_gpt()\n",
        "    model2.load_state_dict(model1.state_dict())  # identical weights\n",
        "\n",
        "    T = 9\n",
        "    idx_batch = torch.randint(0, cfg.vocab_size, (2, T), dtype=torch.long)\n",
        "    idx_first = idx_batch[:1, :]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model1(idx_batch, cache_activations=True, overwrite_cache=True)\n",
        "        _ = model2(idx_first, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    # caches must match up to floating-point tolerance\n",
        "    for L in range(len(model1.clean_activations)):\n",
        "        for p in range(T):\n",
        "            assert torch.allclose(\n",
        "                model1.clean_activations[L][p],\n",
        "                model2.clean_activations[L][p],\n",
        "                rtol=1e-5,\n",
        "                atol=1e-6,\n",
        "            )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m9 passed\u001b[0m\u001b[32m in 3.03s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m pytest -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 5 EXTRA COMPROBATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 0.14M\n",
            "Cached layers: 3\n",
            "Seq len in cache (layer 0): 12\n",
            "One activation shape: (48,)\n",
            "Activation requires_grad: False\n",
            "Logits identical (cache ON vs OFF)?: True\n",
            "Cache unchanged after normal forward?: True\n",
            "Re-cache without overwrite correctly raised RuntimeError:\n",
            "  Clean activation cache already exists. Pass overwrite_cache=True (or call model.clear_clean_activations()) to replace it for a new clean pro ...\n",
            "\n",
            "âœ… Section 5 smoke test PASSED under your cache-protection semantics.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from mingpt.model import GPT\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "set_seed(3407)\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# ---- Build a tiny model (fast, no downloads) ----\n",
        "cfg = GPT.get_default_config()\n",
        "cfg.model_type = \"gpt-nano\"\n",
        "cfg.vocab_size = 1000\n",
        "cfg.block_size = 64\n",
        "\n",
        "model = GPT(cfg)\n",
        "model.eval()\n",
        "\n",
        "B, T = 1, 12\n",
        "idx = torch.randint(0, cfg.vocab_size, (B, T), dtype=torch.long)\n",
        "\n",
        "# ---- 1) Cache activations (CLEAN RUN) ----\n",
        "logits_cache, _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "print(\"Cached layers:\", len(model.clean_activations))\n",
        "print(\"Seq len in cache (layer 0):\", len(model.clean_activations[0]))\n",
        "print(\"One activation shape:\", tuple(model.clean_activations[0][0].shape))\n",
        "print(\"Activation requires_grad:\", model.clean_activations[0][0].requires_grad)\n",
        "\n",
        "# Structural checks\n",
        "assert len(model.clean_activations) == cfg.n_layer\n",
        "assert all(len(layer) == T for layer in model.clean_activations)\n",
        "assert model.clean_activations[0][0].shape == (cfg.n_embd,)\n",
        "assert model.clean_activations[0][0].requires_grad is False\n",
        "\n",
        "# ---- 2) Caching must NOT change logits ----\n",
        "logits_no_cache, _ = model(idx, cache_activations=False)\n",
        "same = torch.allclose(logits_cache, logits_no_cache, rtol=1e-5, atol=1e-6)\n",
        "print(\"Logits identical (cache ON vs OFF)?:\", same)\n",
        "assert same\n",
        "\n",
        "# ---- 3A) Normal forward runs must NOT modify the existing clean cache ----\n",
        "fp_before = (\n",
        "    model.clean_activations[0][0].clone(),\n",
        "    model.clean_activations[-1][-1].clone(),\n",
        ")\n",
        "\n",
        "idx2 = torch.randint(0, cfg.vocab_size, (B, T), dtype=torch.long)\n",
        "_ = model(idx2, cache_activations=False)   # <-- key change: do NOT try to cache again\n",
        "\n",
        "fp_after = (\n",
        "    model.clean_activations[0][0].clone(),\n",
        "    model.clean_activations[-1][-1].clone(),\n",
        ")\n",
        "\n",
        "unchanged = torch.allclose(fp_before[0], fp_after[0]) and torch.allclose(fp_before[1], fp_after[1])\n",
        "print(\"Cache unchanged after normal forward?:\", unchanged)\n",
        "assert unchanged\n",
        "\n",
        "# ---- 3B) Attempting to re-cache without overwrite MUST raise (your current behavior) ----\n",
        "raised = False\n",
        "try:\n",
        "    _ = model(idx2, cache_activations=True, overwrite_cache=False)\n",
        "except RuntimeError as e:\n",
        "    raised = True\n",
        "    print(\"Re-cache without overwrite correctly raised RuntimeError:\")\n",
        "    print(\" \", str(e)[:140], \"...\")\n",
        "assert raised\n",
        "\n",
        "print(\"\\nâœ… Section 5 smoke test PASSED under your cache-protection semantics.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 124.44M\n",
            "gpt2 layers cached: 12\n",
            "seq_len cached: 11\n",
            "d_model shape: (768,)\n",
            "last logits shape: (1, 50257)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "model = GPT.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "bpe = BPETokenizer()\n",
        "text = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "idx = bpe(text)  # (1, T)\n",
        "\n",
        "logits, _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "print(\"gpt2 layers cached:\", len(model.clean_activations))         # EXPECT: 12\n",
        "print(\"seq_len cached:\", len(model.clean_activations[0]))          # EXPECT: T\n",
        "print(\"d_model shape:\", tuple(model.clean_activations[0][0].shape))# EXPECT: (768,)\n",
        "print(\"last logits shape:\", tuple(logits[:, -1, :].shape))         # EXPECT: (1, 50257)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting apply_section6_patch.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile apply_section6_patch.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import pathlib\n",
        "import re\n",
        "\n",
        "import mingpt.model\n",
        "\n",
        "\n",
        "def ensure_typing_import(src: str) -> str:\n",
        "    \"\"\"\n",
        "    Ensure we have: from typing import Any, Dict, List, Optional\n",
        "    Insert it in a stable spot if missing.\n",
        "    \"\"\"\n",
        "    need_line = \"from typing import Any, Dict, List, Optional\"\n",
        "    if need_line in src:\n",
        "        return src\n",
        "\n",
        "    # Prefer inserting after torch imports\n",
        "    pattern = r\"(import torch\\s*\\nimport torch\\.nn as nn\\s*\\n)\"\n",
        "    m = re.search(pattern, src)\n",
        "    if m:\n",
        "        insert_at = m.end(1)\n",
        "        return src[:insert_at] + need_line + \"\\n\" + src[insert_at:]\n",
        "\n",
        "    # Fallback: after import math\n",
        "    pattern2 = r\"(import math\\s*\\n)\"\n",
        "    m2 = re.search(pattern2, src)\n",
        "    if not m2:\n",
        "        raise RuntimeError(\"Could not find a safe place to insert typing imports.\")\n",
        "    insert_at = m2.end(1)\n",
        "    return src[:insert_at] + \"\\n\" + need_line + \"\\n\" + src[insert_at:]\n",
        "\n",
        "\n",
        "def insert_last_logits_attribute(src: str) -> str:\n",
        "    \"\"\"\n",
        "    Add: self.last_logits: Optional[torch.Tensor] = None\n",
        "    Prefer inserting next to Section 5 instrumentation attributes if present.\n",
        "    Idempotent.\n",
        "    \"\"\"\n",
        "    if \"self.last_logits\" in src:\n",
        "        return src\n",
        "\n",
        "    # If Section 5 instrumentation exists, insert after last_activations\n",
        "    pattern = r\"(self\\.last_activations:\\s*Optional\\[List\\[List\\[torch\\.Tensor\\]\\]\\]\\s*=\\s*None\\s*\\n)\"\n",
        "    m = re.search(pattern, src)\n",
        "    if m:\n",
        "        inject = (\n",
        "            m.group(1)\n",
        "            + \"        # last-token logits (Section 6): logits at final prompt position (next-token distribution)\\n\"\n",
        "            + \"        self.last_logits: Optional[torch.Tensor] = None\\n\"\n",
        "        )\n",
        "        return src[:m.start(1)] + inject + src[m.end(1):]\n",
        "\n",
        "    # Fallback: insert after parameter-count print in __init__\n",
        "    marker = 'print(\"number of parameters: %.2fM\" % (n_params/1e6,))'\n",
        "    if marker not in src:\n",
        "        raise RuntimeError(\"Could not find a safe marker in GPT.__init__ to insert last_logits attribute.\")\n",
        "\n",
        "    inject = (\n",
        "        marker\n",
        "        + \"\\n\\n\"\n",
        "        + \"        # --- Mechanistic interpretability instrumentation (Section 6) ---\\n\"\n",
        "        + \"        # logits at final prompt position: shape (B, vocab_size)\\n\"\n",
        "        + \"        self.last_logits: Optional[torch.Tensor] = None\\n\"\n",
        "    )\n",
        "    return src.replace(marker, inject)\n",
        "\n",
        "\n",
        "def insert_last_logits_assignment_in_forward(src: str) -> str:\n",
        "    \"\"\"\n",
        "    After logits = self.lm_head(x), insert:\n",
        "        self.last_logits = logits[:, -1, :].detach().clone()\n",
        "    Idempotent.\n",
        "    \"\"\"\n",
        "    if re.search(r\"self\\.last_logits\\s*=\\s*logits\\[:,\\s*-1,\\s*:\\]\\.detach\\(\\)\\.clone\\(\\)\", src):\n",
        "        return src\n",
        "\n",
        "    # Find logits computation line inside forward\n",
        "    pattern = r\"(\\n(\\s*)logits\\s*=\\s*self\\.lm_head\\(x\\)\\s*\\n)\"\n",
        "    m = re.search(pattern, src)\n",
        "    if not m:\n",
        "        raise RuntimeError(\"Could not find the line `logits = self.lm_head(x)` in GPT.forward.\")\n",
        "\n",
        "    full_match = m.group(1)\n",
        "    indent = m.group(2)\n",
        "\n",
        "    insertion = (\n",
        "        full_match\n",
        "        + f\"{indent}# --- Section 6: store last-position logits (next-token distribution after the prompt) ---\\n\"\n",
        "        + f\"{indent}# Shape: (B, vocab_size). We detach+clone to avoid accidental mutation across runs.\\n\"\n",
        "        + f\"{indent}self.last_logits = logits[:, -1, :].detach().clone()\\n\"\n",
        "    )\n",
        "\n",
        "    return src[:m.start(1)] + insertion + src[m.end(1):]\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    src = path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "    src = ensure_typing_import(src)\n",
        "    src = insert_last_logits_attribute(src)\n",
        "    src = insert_last_logits_assignment_in_forward(src)\n",
        "\n",
        "    path.write_text(src, encoding=\"utf-8\")\n",
        "    print(f\"âœ… Section 6 patch applied to: {path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Section 6 patch applied to: /home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/src/mingpt/mingpt/model.py\n"
          ]
        }
      ],
      "source": [
        "!python apply_section6_patch.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting last_logits_driver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile last_logits_driver.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "\n",
        "def single_token_id(bpe: BPETokenizer, token_str: str) -> int:\n",
        "    ids = bpe(token_str)[0].tolist()\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{repr(token_str)} is not a single BPE token. Got {len(ids)} ids: {ids}\")\n",
        "    return int(ids[0])\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main() -> None:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    bpe = BPETokenizer()\n",
        "\n",
        "    clean = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "    idx = bpe(clean).to(device)  # (1, T)\n",
        "\n",
        "    logits, _ = model(idx)  # forward ONCE\n",
        "    assert model.last_logits is not None, \"model.last_logits was not set!\"\n",
        "    print(\"logits shape:\", tuple(logits.shape))\n",
        "    print(\"last_logits shape:\", tuple(model.last_logits.shape))  # (1, vocab)\n",
        "\n",
        "    # Top-k next tokens from last_logits\n",
        "    k = 10\n",
        "    last = model.last_logits[0]  # (vocab,)\n",
        "    probs = F.softmax(last, dim=-1)\n",
        "    top_p, top_i = torch.topk(probs, k)\n",
        "\n",
        "    print(\"\\n=== Top-k next tokens (from model.last_logits) ===\")\n",
        "    for rank in range(k):\n",
        "        tid = int(top_i[rank])\n",
        "        tok = bpe.decode(torch.tensor([tid]))\n",
        "        print(f\"{rank+1:02d}. id={tid:5d} tok={repr(tok):>12} prob={float(top_p[rank]):.4f}\")\n",
        "\n",
        "    # Metric: logit(TokenB) - logit(TokenA)\n",
        "    token_a = \" Jones\"\n",
        "    token_b = \" Smith\"\n",
        "    id_a = single_token_id(bpe, token_a)\n",
        "    id_b = single_token_id(bpe, token_b)\n",
        "\n",
        "    logit_a = float(model.last_logits[0, id_a])\n",
        "    logit_b = float(model.last_logits[0, id_b])\n",
        "    score = logit_b - logit_a\n",
        "\n",
        "    print(\"\\n=== Logit-diff metric ===\")\n",
        "    print(f\"Token A: {repr(token_a)} id={id_a} logit={logit_a:.4f}\")\n",
        "    print(f\"Token B: {repr(token_b)} id={id_b} logit={logit_b:.4f}\")\n",
        "    print(f\"score = logit(B) - logit(A) = {score:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "number of parameters: 124.44M\n",
            "logits shape: (1, 11, 50257)\n",
            "last_logits shape: (1, 50257)\n",
            "\n",
            "=== Top-k next tokens (from model.last_logits) ===\n",
            "01. id=  373 tok=      ' was' prob=0.1634\n",
            "02. id= 5437 tok=    ' Jones' prob=0.1396\n",
            "03. id=  338 tok=        \"'s\" prob=0.0806\n",
            "04. id=  550 tok=      ' had' prob=0.0491\n",
            "05. id=  318 tok=       ' is' prob=0.0229\n",
            "06. id=  290 tok=      ' and' prob=0.0227\n",
            "07. id=   11 tok=         ',' prob=0.0222\n",
            "08. id=  531 tok=     ' said' prob=0.0134\n",
            "09. id=  468 tok=      ' has' prob=0.0120\n",
            "10. id=  635 tok=     ' also' prob=0.0117\n",
            "\n",
            "=== Logit-diff metric ===\n",
            "Token A: ' Jones' id=5437 logit=-79.6386\n",
            "Token B: ' Smith' id=4176 logit=-83.7627\n",
            "score = logit(B) - logit(A) = -4.1241\n"
          ]
        }
      ],
      "source": [
        "!python last_logits_driver.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_section_6.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section_6.py\n",
        "import inspect\n",
        "import pathlib\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict\n",
        "\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "# If you happen to be in a Colab-like layout, keep this harmless path add.\n",
        "COLAB_MINGPT_PATH = pathlib.Path(\"/content/src/mingpt\")\n",
        "if COLAB_MINGPT_PATH.exists():\n",
        "    sys.path.append(str(COLAB_MINGPT_PATH))\n",
        "\n",
        "import mingpt\n",
        "import mingpt.model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Section 2: repo orientation sanity\n",
        "# --------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ForwardLandmarks:\n",
        "    has_tok_emb: bool\n",
        "    has_pos_emb: bool\n",
        "    has_blocks_loop: bool\n",
        "    has_ln_f: bool\n",
        "    has_lm_head: bool\n",
        "\n",
        "\n",
        "def get_paths() -> Dict[str, str]:\n",
        "    pkg_path = pathlib.Path(mingpt.__file__).resolve()\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return {\n",
        "        \"mingpt.__file__\": str(pkg_path),\n",
        "        \"mingpt.model.__file__\": str(model_path),\n",
        "    }\n",
        "\n",
        "\n",
        "def read_model_source() -> str:\n",
        "    model_path = pathlib.Path(mingpt.model.__file__).resolve()\n",
        "    return model_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "def attn_bias_fix_present(model_source: str) -> bool:\n",
        "    return 'len([k for k in sd if not k.endswith(\".attn.bias\")])' in model_source\n",
        "\n",
        "\n",
        "def forward_source() -> str:\n",
        "    return inspect.getsource(GPT.forward)\n",
        "\n",
        "\n",
        "def find_forward_landmarks(src: str) -> ForwardLandmarks:\n",
        "    has_tok_emb = (\"tok_emb\" in src) and (\"wte\" in src)\n",
        "    has_pos_emb = (\"pos_emb\" in src) and (\"wpe\" in src)\n",
        "    has_blocks_loop = (\"for block in self.transformer.h\" in src) or (\"enumerate(self.transformer.h\" in src)\n",
        "    has_ln_f = \"ln_f\" in src\n",
        "    has_lm_head = (\"lm_head\" in src) and (\"logits\" in src)\n",
        "    return ForwardLandmarks(\n",
        "        has_tok_emb=has_tok_emb,\n",
        "        has_pos_emb=has_pos_emb,\n",
        "        has_blocks_loop=has_blocks_loop,\n",
        "        has_ln_f=has_ln_f,\n",
        "        has_lm_head=has_lm_head,\n",
        "    )\n",
        "\n",
        "\n",
        "def test_mingpt_importable_and_paths_exist():\n",
        "    paths = get_paths()\n",
        "    assert \"mingpt.__file__\" in paths and \"mingpt.model.__file__\" in paths\n",
        "\n",
        "    pkg_path = pathlib.Path(paths[\"mingpt.__file__\"])\n",
        "    model_path = pathlib.Path(paths[\"mingpt.model.__file__\"])\n",
        "    assert pkg_path.exists(), f\"mingpt package file not found: {pkg_path}\"\n",
        "    assert model_path.exists(), f\"mingpt.model file not found: {model_path}\"\n",
        "\n",
        "\n",
        "def test_attn_bias_fix_present_or_applied():\n",
        "    src = read_model_source()\n",
        "    assert attn_bias_fix_present(src), (\n",
        "        \"Required fix not found in mingpt/model.py. \"\n",
        "        \"Expected assert to ignore keys ending with .attn.bias.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def test_forward_pipeline_landmarks_present():\n",
        "    fwd_src = forward_source()\n",
        "    lm = find_forward_landmarks(fwd_src)\n",
        "    assert lm.has_tok_emb, \"Expected token embedding (wte/tok_emb) usage in forward.\"\n",
        "    assert lm.has_pos_emb, \"Expected positional embedding (wpe/pos_emb) usage in forward.\"\n",
        "    assert lm.has_blocks_loop, \"Expected loop over transformer blocks in forward.\"\n",
        "    assert lm.has_ln_f, \"Expected final layer norm ln_f in forward.\"\n",
        "    assert lm.has_lm_head, \"Expected lm_head/logits in forward.\"\n",
        "\n",
        "\n",
        "def test_fast_forward_and_generate_from_scratch():\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg).eval()\n",
        "\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, 10), dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = model(idx)\n",
        "    assert logits.shape == (1, 10, cfg.vocab_size)\n",
        "    assert loss is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=5, do_sample=False)\n",
        "    assert out.shape[1] == 15\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Shared helper for Section 5/6 tests\n",
        "# --------------------------\n",
        "\n",
        "def _make_tiny_gpt():\n",
        "    cfg = GPT.get_default_config()\n",
        "    cfg.model_type = \"gpt-nano\"\n",
        "    cfg.vocab_size = 1000\n",
        "    cfg.block_size = 64\n",
        "    model = GPT(cfg).eval()\n",
        "    return model, cfg\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Section 5: activation recording / clean cache\n",
        "# --------------------------\n",
        "\n",
        "def test_section5_cache_structure_and_shapes():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 12\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    assert model.clean_activations is not None\n",
        "    assert isinstance(model.clean_activations, list)\n",
        "    assert len(model.clean_activations) == len(model.transformer.h)  # n_layer\n",
        "    assert len(model.clean_activations[0]) == T\n",
        "\n",
        "    d_model = model.transformer.wte.weight.shape[1]\n",
        "    a00 = model.clean_activations[0][0]\n",
        "    assert tuple(a00.shape) == (d_model,)\n",
        "    assert a00.requires_grad is False\n",
        "\n",
        "\n",
        "def test_section5_cache_uses_detach_clone_not_views():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 6\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    a0 = model.clean_activations[0][0]\n",
        "    a1 = model.clean_activations[0][1]\n",
        "    assert a0.data_ptr() != a1.data_ptr(), \"Expected clone()d per-position tensors with distinct storage.\"\n",
        "\n",
        "\n",
        "def test_section5_logits_identical_with_and_without_recording():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 10\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits1, _ = model(idx)  # normal\n",
        "        logits2, _ = model(idx, record_activations=True, cache_activations=False)  # recording only\n",
        "\n",
        "    assert torch.allclose(logits1, logits2), \"Activation recording must not change logits.\"\n",
        "\n",
        "\n",
        "def test_section5_clean_cache_not_overwritten_unless_requested():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    idx1 = torch.randint(0, cfg.vocab_size, (1, 8), dtype=torch.long)\n",
        "    idx2 = torch.randint(0, cfg.vocab_size, (1, 8), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx1, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    snap = [[t.clone() for t in layer] for layer in model.clean_activations]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx2)\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(len(snap[L])):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(idx2, record_activations=True, cache_activations=False)\n",
        "    for L in range(len(snap)):\n",
        "        for p in range(len(snap[L])):\n",
        "            assert torch.equal(model.clean_activations[L][p], snap[L][p])\n",
        "\n",
        "    with pytest.raises(RuntimeError):\n",
        "        with torch.no_grad():\n",
        "            _ = model(idx2, cache_activations=True, overwrite_cache=False)\n",
        "\n",
        "\n",
        "def test_section5_batch_behavior_records_only_first_element():\n",
        "    model1, cfg = _make_tiny_gpt()\n",
        "    model2, _ = _make_tiny_gpt()\n",
        "    model2.load_state_dict(model1.state_dict())  # identical weights\n",
        "\n",
        "    T = 9\n",
        "    idx_batch = torch.randint(0, cfg.vocab_size, (2, T), dtype=torch.long)\n",
        "    idx_first = idx_batch[:1, :]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model1(idx_batch, cache_activations=True, overwrite_cache=True)\n",
        "        _ = model2(idx_first, cache_activations=True, overwrite_cache=True)\n",
        "\n",
        "    for L in range(len(model1.clean_activations)):\n",
        "        for p in range(T):\n",
        "            assert torch.allclose(\n",
        "                model1.clean_activations[L][p],\n",
        "                model2.clean_activations[L][p],\n",
        "                rtol=1e-5,\n",
        "                atol=1e-6,\n",
        "            )\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Section 6: last-token logits extraction (NEW)\n",
        "# --------------------------\n",
        "\n",
        "def test_section6_last_logits_exists_and_matches_last_position_logits():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 11\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(idx)\n",
        "\n",
        "    assert hasattr(model, \"last_logits\"), \"Expected GPT to expose model.last_logits\"\n",
        "    assert model.last_logits is not None, \"model.last_logits was not set by forward()\"\n",
        "    assert tuple(model.last_logits.shape) == (1, cfg.vocab_size)\n",
        "\n",
        "    expected = logits[:, -1, :]\n",
        "    assert torch.allclose(model.last_logits, expected), \"model.last_logits must equal logits[:, -1, :] for the same run\"\n",
        "\n",
        "\n",
        "def test_section6_last_logits_is_detached_and_cloned():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 7\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(idx)\n",
        "\n",
        "    view = logits[:, -1, :]  # view into logits storage\n",
        "    assert model.last_logits.requires_grad is False\n",
        "    # clone must not share underlying storage with the view\n",
        "    assert model.last_logits.data_ptr() != view.data_ptr(), \"Expected last_logits to be a clone(), not a view\"\n",
        "\n",
        "\n",
        "def test_section6_last_logits_computed_even_when_recording_or_caching():\n",
        "    model, cfg = _make_tiny_gpt()\n",
        "    T = 9\n",
        "    idx = torch.randint(0, cfg.vocab_size, (1, T), dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits_a, _ = model(idx, record_activations=True, cache_activations=False)\n",
        "    assert model.last_logits is not None\n",
        "    assert torch.allclose(model.last_logits, logits_a[:, -1, :])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits_b, _ = model(idx, cache_activations=True, overwrite_cache=True)\n",
        "    assert model.last_logits is not None\n",
        "    assert torch.allclose(model.last_logits, logits_b[:, -1, :])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                             [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m12 passed\u001b[0m\u001b[32m in 2.86s\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m pytest -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EXTRA COMPROBATIONS FOR SECTION 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bledyx/UA/master-ia/TPLN/code/lvl1/lvl2/tpln-practice2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 124.44M\n",
            "number of parameters: 124.44M\n",
            "score(clean)  = -4.124076843261719\n",
            "score(corrupt)= 5.656242370605469\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "def single_token_id(bpe, s):\n",
        "    ids = bpe(s)[0].tolist()\n",
        "    assert len(ids) == 1, (s, ids)\n",
        "    return int(ids[0])\n",
        "\n",
        "@torch.no_grad()\n",
        "def score_for(prompt, token_a=\" Jones\", token_b=\" Smith\"):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = GPT.from_pretrained(\"gpt2\").to(device).eval()\n",
        "    bpe = BPETokenizer()\n",
        "    idx = bpe(prompt).to(device)\n",
        "    _ = model(idx)\n",
        "    id_a = single_token_id(bpe, token_a)\n",
        "    id_b = single_token_id(bpe, token_b)\n",
        "    return float(model.last_logits[0, id_b] - model.last_logits[0, id_a])\n",
        "\n",
        "clean = \"Michelle Jones was a top-notch student. Michelle\"\n",
        "corrupt = \"Michelle Smith was a top-notch student. Michelle\"\n",
        "\n",
        "s_clean = score_for(clean)\n",
        "s_corrupt = score_for(corrupt)\n",
        "\n",
        "print(\"score(clean)  =\", s_clean)\n",
        "print(\"score(corrupt)=\", s_corrupt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "048b2b328d174740a7f4eb64fd2f8b7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c789b9007024556915b3f9e3cabe35e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1209e810b3fc41808b444108a87afca1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16ca502911134178a532cda1cf139ca4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20ea57fda15f42d18d60048c165cb019": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0449079fc7a48a3a941c34c7cf2fdc9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9a67d3d3a8744fd881b8a2d5359e78ab",
            "value": "model.safetensors: 100%"
          }
        },
        "260906d3854c4ff78c2cda042bae0a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_048b2b328d174740a7f4eb64fd2f8b7f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c033d8e19c084b90bd8a88a3e5bdb5b8",
            "value": " 124/124 [00:00&lt;00:00, 3.38kB/s]"
          }
        },
        "37af4bb362b94e87b7c128e6966534f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ba0334c76344e0a824177d3572e6dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16ca502911134178a532cda1cf139ca4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ca37e879895f4707bb2e39d9229c65dd",
            "value": "generation_config.json: 100%"
          }
        },
        "3ce2905b2f8e41adbc9eb724aeb2f62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44854839f29f4de08577daf5c4db4c63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52f6eb03404a425aabda1a87b86afe15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cbfa6b49da84ee781492abacea18884": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eec0e26428c4034867633d70b4b3915": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "648a553be0bf49cba777f0eaaac741b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c789b9007024556915b3f9e3cabe35e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f6620c81dc4e4285a332cc40b7655caf",
            "value": " 665/665 [00:00&lt;00:00, 14.7kB/s]"
          }
        },
        "74d877d849d1498fbf87968a79b436d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d95517340925450d9544a147d0064f8f",
              "IPY_MODEL_795a4f3580c5469f86d0bcb96edc1f13",
              "IPY_MODEL_648a553be0bf49cba777f0eaaac741b2"
            ],
            "layout": "IPY_MODEL_37af4bb362b94e87b7c128e6966534f9"
          }
        },
        "784c12d3beb34f25a3c3387217b9a80a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20ea57fda15f42d18d60048c165cb019",
              "IPY_MODEL_8f25e6d89124473688b788bf69c1bfba",
              "IPY_MODEL_bc9fe7054cc849e0ae9a0d69393de9d2"
            ],
            "layout": "IPY_MODEL_b858fc0c1caa4872b91ec7638d6dbf68"
          }
        },
        "795a4f3580c5469f86d0bcb96edc1f13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cbfa6b49da84ee781492abacea18884",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9bcaec9f9c5467aa6e35316d3296302",
            "value": 665
          }
        },
        "80c335cb65a5408dae9ecd4ec9eda4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ba0334c76344e0a824177d3572e6dd7",
              "IPY_MODEL_eff3788657d8436e96a328e73cab52cc",
              "IPY_MODEL_260906d3854c4ff78c2cda042bae0a2e"
            ],
            "layout": "IPY_MODEL_aa21a3379eb44eb28a52028cc5a0590b"
          }
        },
        "8f25e6d89124473688b788bf69c1bfba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a08a849c10e24dffb645cfe75c4bb66a",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efb3d94f64314da6a0e9b3e4d1e123e4",
            "value": 548105171
          }
        },
        "9a67d3d3a8744fd881b8a2d5359e78ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a08a849c10e24dffb645cfe75c4bb66a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9d6ce4b456f4b6b872a832ffe858e06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa21a3379eb44eb28a52028cc5a0590b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b858fc0c1caa4872b91ec7638d6dbf68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc9fe7054cc849e0ae9a0d69393de9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9d6ce4b456f4b6b872a832ffe858e06",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_52f6eb03404a425aabda1a87b86afe15",
            "value": " 548M/548M [00:04&lt;00:00, 122MB/s]"
          }
        },
        "c033d8e19c084b90bd8a88a3e5bdb5b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca37e879895f4707bb2e39d9229c65dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0449079fc7a48a3a941c34c7cf2fdc9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d95517340925450d9544a147d0064f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1209e810b3fc41808b444108a87afca1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3ce2905b2f8e41adbc9eb724aeb2f62a",
            "value": "config.json: 100%"
          }
        },
        "efb3d94f64314da6a0e9b3e4d1e123e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eff3788657d8436e96a328e73cab52cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44854839f29f4de08577daf5c4db4c63",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5eec0e26428c4034867633d70b4b3915",
            "value": 124
          }
        },
        "f6620c81dc4e4285a332cc40b7655caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9bcaec9f9c5467aa6e35316d3296302": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
